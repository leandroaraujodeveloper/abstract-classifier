abstract,field,search_word,title
"  Clustering is a fundamental primitive in unsupervised learning which gives
rise to a rich class of computationally-challenging inference tasks. In this
work, we focus on the canonical task of clustering $d$-dimensional Gaussian
mixtures with unknown (and possibly degenerate) covariance. Recent works (Ghosh
et al.\ '20; Mao, Wein '21; Davis, Diaz, Wang '21) have established lower
bounds against the class of low-degree polynomial methods and the
sum-of-squares (SoS) hierarchy for recovering certain hidden structures planted
in Gaussian clustering instances. Prior work on many similar inference tasks
portends that such lower bounds strongly suggest the presence of an inherent
statistical-to-computational gap for clustering, that is, a parameter regime
where the clustering task is \textit{statistically} possible but no
\textit{polynomial-time} algorithm succeeds.
", Machine Learning,unsupervised learning,Lattice-Based Methods Surpass Sum-of-Squares in Clustering
"  Employing unmanned aerial vehicles (UAVs) has attracted growing interests and
emerged as the state-of-the-art technology for data collection in
Internet-of-Things (IoT) networks. In this paper, with the objective of
minimizing the total energy consumption of the UAV-IoT system, we formulate the
problem of jointly designing the UAV's trajectory and selecting cluster heads
in the IoT network as a constrained combinatorial optimization problem which is
classified as NP-hard and challenging to solve. We propose a novel deep
reinforcement learning (DRL) with a sequential model strategy that can
effectively learn the policy represented by a sequence-to-sequence neural
network for the UAV's trajectory design in an unsupervised manner. Through
extensive simulations, the obtained results show that the proposed DRL method
can find the UAV's trajectory that requires much less energy consumption when
compared to other baseline algorithms and achieves close-to-optimal
performance. In addition, simulation results show that the trained model by our
proposed DRL algorithm has an excellent generalization ability to larger
problem sizes without the need to retrain the model.

    ", Systems and Control,unsupervised learning,Joint Cluster Head Selection and Trajectory Planning in UAV-Aided IoT Networks by Reinforcement Learning with Sequential Model
"  The capability of accurate prediction of protein functions and properties is
essential in the biotechnology industry, e.g. drug development and artificial
protein synthesis, etc. The main challenges of protein function prediction are
the large label space and the lack of labeled training data. Our method
leverages unsupervised sequence embedding and the success of deep convolutional
neural network to overcome these challenges. In contrast, most of the existing
methods delete the rare protein functions to reduce the label space.
Furthermore, some existing methods require additional bio-information (e.g.,
the 3-dimensional structure of the proteins) which is difficult to be
determined in biochemical experiments. Our proposed method significantly
outperforms the other methods on the publicly available benchmark using only
protein sequences as input. This allows the process of identifying protein
functions to be sped up.

    ", Quantitative Methods,unsupervised learning,Leveraging Sequence Embedding and Convolutional Neural Network for Protein Function Prediction
"  Categorical attributes are those that can take a discrete set of values,
e.g., colours. This work is about compressing vectors over categorical
attributes to low-dimension discrete vectors. The current hash-based methods
compressing vectors over categorical attributes to low-dimension discrete
vectors do not provide any guarantee on the Hamming distances between the
compressed representations. Here we present FSketch to create sketches for
sparse categorical data and an estimator to estimate the pairwise Hamming
distances among the uncompressed data only from their sketches. We claim that
these sketches can be used in the usual data mining tasks in place of the
original data without compromising the quality of the task. For that, we ensure
that the sketches also are categorical, sparse, and the Hamming distance
estimates are reasonably precise. Both the sketch construction and the Hamming
distance estimation algorithms require just a single-pass; furthermore, changes
to a data point can be incorporated into its sketch in an efficient manner. The
compressibility depends upon how sparse the data is and is independent of the
original dimension -- making our algorithm attractive for many real-life
scenarios. Our claims are backed by rigorous theoretical analysis of the
properties of FSketch and supplemented by extensive comparative evaluations
with related algorithms on some real-world datasets. We show that FSketch is
significantly faster, and the accuracy obtained by using its sketches are among
the top for the standard unsupervised tasks of RMSE, clustering and similarity
search.

    ", Machine Learning,unsupervised learning,Dimensionality Reduction for Categorical Data
"  Domain adaptation is crucial to adapt a learned model to new scenarios, such
as domain shifts or changing data distributions. Current approaches usually
require a large amount of labeled or unlabeled data from the shifted domain.
This can be a hurdle in fields which require continuous dynamic adaptation or
suffer from scarcity of data, e.g. autonomous driving in challenging weather
conditions. To address this problem of continuous adaptation to distribution
shifts, we propose Dynamic Unsupervised Adaptation (DUA). We modify the feature
representations of the model by continuously adapting the statistics of the
batch normalization layers. We show that by accessing only a tiny fraction of
unlabeled data from the shifted domain and adapting sequentially, a strong
performance gain can be achieved. With even less than 1% of unlabeled data from
the target domain, DUA already achieves competitive results to strong
baselines. In addition, the computational overhead is minimal in contrast to
previous approaches. Our approach is simple, yet effective and can be applied
to any architecture which uses batch normalization as one of its components. We
show the utility of DUA by evaluating it on a variety of domain adaptation
datasets and tasks including object recognition, digit recognition and object
detection.

    ", Computer Vision and Pattern Recognition,unsupervised learning,The Norm Must Go On: Dynamic Unsupervised Domain Adaptation by Normalization

"  Solving high-dimensional partial differential equations is a recurrent
challenge in economics, science and engineering. In recent years, a great
number of computational approaches have been developed, most of them relying on
a combination of Monte Carlo sampling and deep learning based approximation.
For elliptic and parabolic problems, existing methods can broadly be classified
into those resting on reformulations in terms of $\textit{backward stochastic
differential equations}$ (BSDEs) and those aiming to minimize a regression-type
$L^2$-error ($\textit{physics-informed neural networks}$, PINNs). In this
paper, we review the literature and suggest a methodology based on the novel
$\textit{diffusion loss}$ that interpolates between BSDEs and PINNs. Our
contribution opens the door towards a unified understanding of numerical
approaches for high-dimensional PDEs, as well as for implementations that
combine the strengths of BSDEs and PINNs. We also provide generalizations to
eigenvalue problems and perform extensive numerical studies, including
calculations of the ground state for nonlinear Schr√∂dinger operators and
committor functions relevant in molecular dynamics.

    ", Numerical Analysis,recurrent neural networks,Interpolating between BSDEs and PINNs -- deep learning for elliptic and parabolic boundary value problems
"  The ATLAS experiment at CERN measures energy of proton-proton (p-p)
collisions with a repetition frequency of 40 MHz at the Large Hadron Collider
(LHC). The readout electronics of liquid-argon (LAr) calorimeters are being
prepared for high luminosity-LHC (HL-LHC) operation as part of the phase-II
upgrade, anticipating a pileup of up to 200 simultaneous p-p interactions. The
increase of the number of p-p interactions implies that calorimeter signals of
up to 25 consecutive collisions overlap, making energy reconstruction more
challenging.
", Instrumentation and Detectors,recurrent neural networks,Machine Learning for Real-Time Processing of ATLAS Liquid Argon Calorimeter Signals with FPGAs
"  Timely handgun detection is a crucial problem to improve public safety;
nevertheless, the effectiveness of many surveillance systems still depends of
finite human attention. Much of the previous research on handgun detection is
based on static image detectors, leaving aside valuable temporal information
that could be used to improve object detection in videos. To improve the
performance of surveillance systems, a real-time temporal handgun detection
system should be built. Using Temporal Yolov5, an architecture based on
Quasi-Recurrent Neural Networks, temporal information is extracted from video
to improve the results of handgun detection. Moreover, two publicly available
datasets are proposed, labeled with hands, guns, and phones. One containing
2199 static images to train static detectors, and another with 5960 frames of
videos to train temporal modules. Additionally, we explore two temporal data
augmentation techniques based on Mosaic and Mixup. The resulting systems are
three temporal architectures: one focused in reducing inference with a
mAP$_{50:95}$ of 55.9, another in having a good balance between inference and
accuracy with a mAP$_{50:95}$ of 59, and a last one specialized in accuracy
with a mAP$_{50:95}$ of 60.2. Temporal Yolov5 achieves real-time detection in
the small and medium architectures. Moreover, it takes advantage of temporal
features contained in videos to perform better than Yolov5 in our temporal
dataset, making TYolov5 suitable for real-world applications. The source code
is publicly available at ", Computer Vision and Pattern Recognition,recurrent neural networks,TYolov5: A Temporal Yolov5 Detector Based on Quasi-Recurrent Neural Networks for Real-Time Handgun Detection in Video
"  Climate change is posing new challenges to crop-related concerns including
food insecurity, supply stability and economic planning. As one of the central
challenges, crop yield prediction has become a pressing task in the machine
learning field. Despite its importance, the prediction task is exceptionally
complicated since crop yields depend on various factors such as weather, land
surface, soil quality as well as their interactions. In recent years, machine
learning models have been successfully applied in this domain. However, these
models either restrict their tasks to a relatively small region, or only study
over a single or few years, which makes them hard to generalize spatially and
temporally. In this paper, we introduce a novel graph-based recurrent neural
network for crop yield prediction, to incorporate both geographical and
temporal knowledge in the model, and further boost predictive power. Our method
is trained, validated, and tested on over 2000 counties from 41 states in the
US mainland, covering years from 1981 to 2019. As far as we know, this is the
first machine learning method that embeds geographical knowledge in crop yield
prediction and predicts the crop yields at county level nationwide. We also
laid a solid foundation for the comparison with other machine learning
baselines by applying well-known linear models, tree-based models, deep
learning methods and comparing their performance. Experiments show that our
proposed method consistently outperforms the existing state-of-the-art methods
on various metrics, validating the effectiveness of geospatial and temporal
information.

    ", Machine Learning,recurrent neural networks,A GNN-RNN Approach for Harnessing Geospatial and Temporal Information: Application to Crop Yield Prediction
"  Recurrent Neural Networks (RNNs) were recently successfully used to model the
way neural activity drives task-related behavior in animals, operating under
the implicit assumption that the obtained solutions are universal. Observations
in both neuroscience and machine learning challenge this assumption. Animals
can approach a given task with a variety of strategies, and training machine
learning algorithms introduces the phenomenon of underspecification. These
observations imply that every task is associated with a space of solutions. To
date, the structure of this space is not understood, limiting the approach of
comparing RNNs with neural data. Here, we characterize the space of solutions
associated with various tasks. We first study a simple two-neuron network on a
task that leads to multiple solutions. We trace the nature of the final
solution back to the network's initial connectivity and identify discrete
dynamical regimes that underlie this diversity. We then examine three
neuroscience-inspired tasks: Delayed and interval discrimination, and Time
reproduction. For each task, we find a rich set of solutions. Variability can
be found directly in the neural activity of the networks, and additionally by
testing the trained networks' ability to extrapolate, as a perturbation to a
system often reveals hidden structure. Furthermore, we relate extrapolation
patterns to specific dynamical objects and effective algorithms found by the
networks. We introduce a tool to derive the reduced dynamics of networks by
generating a compact directed graph describing the essence of the dynamics with
regards to behavioral inputs and outputs. Using this representation, we can
partition the solutions to each task into a handful of types and partially
predict them from neural features. Our results shed light on the concept of the
space of solutions and its uses in Machine learning and in Neuroscience.

    ", Neurons and Cognition,recurrent neural networks,Charting and navigating the space of solutions for recurrent neural networks

"  Clustering is a fundamental primitive in unsupervised learning which gives
rise to a rich class of computationally-challenging inference tasks. In this
work, we focus on the canonical task of clustering $d$-dimensional Gaussian
mixtures with unknown (and possibly degenerate) covariance. Recent works (Ghosh
et al.\ '20; Mao, Wein '21; Davis, Diaz, Wang '21) have established lower
bounds against the class of low-degree polynomial methods and the
sum-of-squares (SoS) hierarchy for recovering certain hidden structures planted
in Gaussian clustering instances. Prior work on many similar inference tasks
portends that such lower bounds strongly suggest the presence of an inherent
statistical-to-computational gap for clustering, that is, a parameter regime
where the clustering task is \textit{statistically} possible but no
\textit{polynomial-time} algorithm succeeds.
", Machine Learning,clustering,Lattice-Based Methods Surpass Sum-of-Squares in Clustering
"  Accurate estimation of daily rainfall return levels associated with large
return periods is needed for a number of hydrological planning purposes,
including protective infrastructure, dams, and retention basins. This is
especially relevant at small spatial scales. The ERA-5 reanalysis product
provides seasonal daily precipitation over Europe on a 0.25 x 0.25 grid (about
27 x 27 km). This translates more than 20,000 land grid points and leads to
models with a large number of parameters when estimating return levels. To
bypass this abundance of parameters, we build on the regional frequency
analysis (RFA), a well-known strategy in statistical hydrology. This approach
consists in identifying homogeneous regions, by gathering locations with
similar distributions of extremes up to a normalizing factor and developing
sparse regional models. In particular, we propose a step-by-step blueprint that
leverages a recently developed and fast clustering algorithm to infer return
level estimates over large spatial domains. This enables us to produce maps of
return level estimates of ERA-5 reanalysis daily precipitation over continental
Europe for various return periods and seasons. We discuss limitations and
practical challenges and also provide a git hub repository. We show that a
relatively parsimonious model with only a spatially varying scale parameter can
compete well against statistical models of higher complexity.

    ", Applications,clustering,High return level estimates of daily ERA-5 precipitation in Europe estimated using regionalised extreme value distributions
"  The APOGEE Open Cluster Chemical Abundances and Mapping (OCCAM) survey is
used to probe the chemical evolution of the s-process element cerium in the
Galactic disk. Cerium abundances were derived from measurements of Ce II lines
in the APOGEE spectra using the Brussels Automatic Code for Characterizing High
Accuracy Spectra (BACCHUS) in 218 stars belonging to 42 open clusters. Our
results indicate that, in general, for Ages $<$ 4 Gyr, younger open clusters
have higher [Ce/Fe] and [Ce/$\alpha$-element] ratios than older clusters. In
addition, metallicity segregates open clusters in the [Ce/X]-Age plane (where X
can be H, Fe, and the $\alpha$-elements O, Mg, Si, or Ca). These
metallicity-dependant relations result in [Ce/Fe] and [Ce/$\alpha$] ratios with
age that are not universal clocks. Radial gradients of [Ce/H] and [Ce/Fe]
ratios in open clusters, binned by age, were derived for the first time, with
d[Ce/H]dR$_{GC}$ being negative, while d[Ce/Fe]/dR$_{GC}$ is positive. [Ce/H]
and [Ce/Fe] gradients are approximately constant over time, with the [Ce/Fe]
gradient becoming slightly steeper, changing by $\sim$+0.009
dex-kpc$^{-1}$-Gyr$^{-1}$. Both the [Ce/H] and [Ce/Fe] gradients are shifted to
lower values of [Ce/H] and [Ce/Fe] for older open clusters. The chemical
pattern of Ce in open clusters across the Galactic disk is discussed within the
context of s-process yields from AGB stars, $\sim$Gyr time delays in Ce
enrichment of the interstellar medium, and the strong dependence of Ce
nucleosynthesis on the metallicity of its AGB stellar sources.

    ", Astrophysics of Galaxies,clustering,Exploring the s-process history in the Galactic disk: Cerium abundances and gradients in Open Clusters from the OCCAM/APOGEE sample
"  The $k$-nearest neighbor graph (KNNG) on high-dimensional data is a data
structure widely used in many applications such as similarity search, dimension
reduction and clustering. Due to its increasing popularity, several methods
under the same framework have been proposed in the past decade. This framework
contains two steps, i.e. building an initial KNNG (denoted as \texttt{INIT})
and then refining it by neighborhood propagation (denoted as \texttt{NBPG}).
However, there remain several questions to be answered. First, it lacks a
comprehensive experimental comparison among representative solutions in the
literature. Second, some recently proposed indexing structures, e.g., SW and
HNSW, have not been used or tested for building an initial KNNG. Third, the
relationship between the data property and the effectiveness of \texttt{NBPG}
is still not clear. To address these issues, we comprehensively compare the
representative approaches on real-world high-dimensional data sets to provide
practical and insightful suggestions for users. As the first attempt, we take
SW and HNSW as the alternatives of \texttt{INIT} in our experiments. Moreover,
we investigate the effectiveness of \texttt{NBPG} and find the strong
correlation between the huness phenomenon and the performance of \texttt{NBPG}.

    ", Data Structures and Algorithms,clustering,Revisiting $k$-Nearest Neighbor Graph Construction on High-Dimensional Data : Experiments and Analyses
"  3D Morphable Models (3DMMs) are generative models for face shape and
appearance. However, the shape parameters of traditional 3DMMs satisfy the
multivariate Gaussian distribution while the identity embeddings satisfy the
hypersphere distribution, and this conflict makes it challenging for face
reconstruction models to preserve the faithfulness and the shape consistency
simultaneously. To address this issue, we propose the Sphere Face Model(SFM), a
novel 3DMM for monocular face reconstruction, which can preserve both shape
fidelity and identity consistency. The core of our SFM is the basis matrix
which can be used to reconstruct 3D face shapes, and the basic matrix is
learned by adopting a two-stage training approach where 3D and 2D training data
are used in the first and second stages, respectively. To resolve the
distribution mismatch, we design a novel loss to make the shape parameters have
a hyperspherical latent space. Extensive experiments show that SFM has high
representation ability and shape parameter space's clustering performance.
Moreover, it produces fidelity face shapes, and the shapes are consistent in
challenging conditions in monocular face reconstruction.

    ", Computer Vision and Pattern Recognition,clustering,Sphere Face Model:A 3D Morphable Model with Hypersphere Manifold Latent Space
"  Humans and animals explore their environment and acquire useful skills even
in the absence of clear goals, exhibiting intrinsic motivation. The study of
intrinsic motivation in artificial agents is concerned with the following
question: what is a good general-purpose objective for an agent? We study this
question in dynamic partially-observed environments, and argue that a compact
and general learning objective is to minimize the entropy of the agent's state
visitation estimated using a latent state-space model. This objective induces
an agent to both gather information about its environment, corresponding to
reducing uncertainty, and to gain control over its environment, corresponding
to reducing the unpredictability of future world states. We instantiate this
approach as a deep reinforcement learning agent equipped with a deep
variational Bayes filter. We find that our agent learns to discover, represent,
and exercise control of dynamic objects in a variety of partially-observed
environments sensed with visual observations without extrinsic reward.

    ", Machine Learning,reinforcement learning,Information is Power: Intrinsic Control via Information Capture
"  Recent works in Reinforcement Learning (RL) combine model-free (Mf)-RL
algorithms with model-based (Mb)-RL approaches to get the best from both:
asymptotic performance of Mf-RL and high sample-efficiency of Mb-RL. Inspired
by these works, we propose a hierarchical framework that integrates online
learning for the Mb-trajectory optimization with off-policy methods for the
Mf-RL. In particular, two loops are proposed, where the Dynamic Mirror Descent
based Model Predictive Control (DMD-MPC) is used as the inner loop Mb-RL to
obtain an optimal sequence of actions. These actions are in turn used to
significantly accelerate the outer loop Mf-RL. We show that our formulation is
generic for a broad class of MPC-based policies and objectives, and includes
some of the well-known Mb-Mf approaches. We finally introduce a new algorithm:
Mirror-Descent Model Predictive RL (M-DeMoRL), which uses Cross-Entropy Method
(CEM) with elite fractions for the inner loop. Our experiments show faster
convergence of the proposed hierarchical approach on benchmark MuJoCo tasks. We
also demonstrate hardware training for trajectory tracking in a 2R leg and
hardware transfer for robust walking in a quadruped. We show that the
inner-loop Mb-RL significantly decreases the number of training iterations
required in the real system, thereby validating the proposed approach.

    ", Robotics,reinforcement learning,Dynamic Mirror Descent based Model Predictive Control for Accelerating Robot Learning
"  Deep reinforcement learning (RL) agents are becoming increasingly proficient
in a range of complex control tasks. However, the agent's behavior is usually
difficult to interpret due to the introduction of black-box function, making it
difficult to acquire the trust of users. Although there have been some
interesting interpretation methods for vision-based RL, most of them cannot
uncover temporal causal information, raising questions about their reliability.
To address this problem, we present a temporal-spatial causal interpretation
(TSCI) model to understand the agent's long-term behavior, which is essential
for sequential decision-making. TSCI model builds on the formulation of
temporal causality, which reflects the temporal causal relations between
sequential observations and decisions of RL agent. Then a separate causal
discovery network is employed to identify temporal-spatial causal features,
which are constrained to satisfy the temporal causality. TSCI model is
applicable to recurrent agents and can be used to discover causal features with
high efficiency once trained. The empirical results show that TSCI model can
produce high-resolution and sharp attention masks to highlight task-relevant
temporal-spatial information that constitutes most evidence about how
vision-based RL agents make sequential decisions. In addition, we further
demonstrate that our method is able to provide valuable causal interpretations
for vision-based RL agents from the temporal perspective.

    ", Computer Vision and Pattern Recognition,reinforcement learning,Temporal-Spatial Causal Interpretations for Vision-Based Reinforcement Learning
"  We introduce the dynamic grasp synthesis task: given an object with a known
6D pose and a grasp reference, our goal is to generate motions that move the
object to a target 6D pose. This is challenging, because it requires reasoning
about the complex articulation of the human hand and the intricate physical
interaction with the object. We propose a novel method that frames this problem
in the reinforcement learning framework and leverages a physics simulation,
both to learn and to evaluate such dynamic interactions. A hierarchical
approach decomposes the task into low-level grasping and high-level motion
synthesis. It can be used to generate novel hand sequences that approach,
grasp, and move an object to a desired location, while retaining
human-likeness. We show that our approach leads to stable grasps and generates
a wide range of motions. Furthermore, even imperfect labels can be corrected by
our method to generate dynamic interaction sequences. Video is available at
", Robotics,reinforcement learning,D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions
"  Temporal abstraction in reinforcement learning (RL), offers the promise of
improving generalization and knowledge transfer in complex environments, by
propagating information more efficiently over time. Although option learning
was initially formulated in a way that allows updating many options
simultaneously, using off-policy, intra-option learning (Sutton, Precup &
Singh, 1999), many of the recent hierarchical reinforcement learning approaches
only update a single option at a time: the option currently executing. We
revisit and extend intra-option learning in the context of deep reinforcement
learning, in order to enable updating all options consistent with current
primitive action choices, without introducing any additional estimates. Our
method can therefore be naturally adopted in most hierarchical RL frameworks.
When we combine our approach with the option-critic algorithm for option
discovery, we obtain significant improvements in performance and
data-efficiency across a wide variety of domains.

    ", Machine Learning,reinforcement learning,Flexible Option Learning
"  Hierarchical reinforcement learning (HRL) holds great potential for
sample-efficient learning on challenging long-horizon tasks. In particular,
letting a higher level assign subgoals to a lower level has been shown to
enable fast learning on difficult problems. However, such subgoal-based methods
have been designed with static reinforcement learning environments in mind and
consequently struggle with dynamic elements beyond the immediate control of the
agent even though they are ubiquitous in real-world problems. In this paper, we
introduce Hierarchical reinforcement learning with Timed Subgoals (HiTS), an
HRL algorithm that enables the agent to adapt its timing to a dynamic
environment by not only specifying what goal state is to be reached but also
when. We discuss how communicating with a lower level in terms of such timed
subgoals results in a more stable learning problem for the higher level. Our
experiments on a range of standard benchmarks and three new challenging dynamic
reinforcement learning environments show that our method is capable of
sample-efficient learning where an existing state-of-the-art subgoal-based HRL
method fails to learn stable solutions.

    ", Machine Learning,reinforcement learning,Hierarchical Reinforcement Learning with Timed Subgoals

"  With an increase of dataset availability, the potential for learning from a
variety of data sources has increased. One particular method to improve
learning from multiple data sources is to embed the data source during
training. This allows the model to learn generalizable features as well as
distinguishing features between datasets. However, these dataset embeddings
have mostly been used before contextualized transformer-based embeddings were
introduced in the field of Natural Language Processing. In this work, we
compare two methods to embed datasets in a transformer-based multilingual
dependency parser, and perform an extensive evaluation. We show that: 1)
embedding the dataset is still beneficial with these models 2) performance
increases are highest when embedding the dataset at the encoder level 3)
unsurprisingly, we confirm that performance increases are highest for small
datasets and datasets with a low baseline score. 4) we show that training on
the combination of all datasets performs similarly to designing smaller
clusters based on language-relatedness.

    ", Computation and Language,natural language processing,"Parsing with Pretrained Language Models, Multiple Datasets, and Dataset Embeddings"
"  Deep learning has recently made remarkable progress in natural language
processing. Yet, the resulting algorithms remain far from competing with the
language abilities of the human brain. Predictive coding theory offers a
potential explanation to this discrepancy: while deep language algorithms are
optimized to predict adjacent words, the human brain would be tuned to make
long-range and hierarchical predictions. To test this hypothesis, we analyze
the fMRI brain signals of 304 subjects each listening to 70min of short
stories. After confirming that the activations of deep language algorithms
linearly map onto those of the brain, we show that enhancing these models with
long-range forecast representations improves their brain-mapping. The results
further reveal a hierarchy of predictions in the brain, whereby the
fronto-parietal cortices forecast more abstract and more distant
representations than the temporal cortices. Overall, this study strengthens
predictive coding theory and suggests a critical role of long-range and
hierarchical predictions in natural language processing.

    ", Neurons and Cognition,natural language processing,Long-range and hierarchical language predictions in brains and algorithms
"  In this work, we propose a new and general framework to defend against
backdoor attacks, inspired by the fact that attack triggers usually follow a
\textsc{specific} type of attacking pattern, and therefore, poisoned training
examples have greater impacts on each other during training. We introduce the
notion of the {\it influence graph}, which consists of nodes and edges
respectively representative of individual training points and associated
pair-wise influences. The influence between a pair of training points
represents the impact of removing one training point on the prediction of
another, approximated by the influence function \citep{koh2017understanding}.
Malicious training points are extracted by finding the maximum average
sub-graph subject to a particular size. Extensive experiments on computer
vision and natural language processing tasks demonstrate the effectiveness and
generality of the proposed framework.

    ", Machine Learning,natural language processing,A General Framework for Defending Against Backdoor Attacks via Influence Graph
"  Saliency methods have been widely used to highlight important input features
in model predictions. Most existing methods use backpropagation on a modified
gradient function to generate saliency maps. Thus, noisy gradients can result
in unfaithful feature attributions. In this paper, we tackle this issue and
introduce a {\it saliency guided training}procedure for neural networks to
reduce noisy gradients used in predictions while retaining the predictive
performance of the model. Our saliency guided training procedure iteratively
masks features with small and potentially noisy gradients while maximizing the
similarity of model outputs for both masked and unmasked inputs. We apply the
saliency guided training procedure to various synthetic and real data sets from
computer vision, natural language processing, and time series across diverse
neural architectures, including Recurrent Neural Networks, Convolutional
Networks, and Transformers. Through qualitative and quantitative evaluations,
we show that saliency guided training procedure significantly improves model
interpretability across various domains while preserving its predictive
performance.

    ", Computer Vision and Pattern Recognition,natural language processing,Improving Deep Learning Interpretability by Saliency Guided Training
"  In the past decade, we have witnessed the rise of deep learning to dominate
the field of artificial intelligence. Advances in artificial neural networks
alongside corresponding advances in hardware accelerators with large memory
capacity, together with the availability of large datasets enabled researchers
and practitioners alike to train and deploy sophisticated neural network models
that achieve state-of-the-art performance on tasks across several fields
spanning computer vision, natural language processing, and reinforcement
learning. However, as these neural networks become bigger, more complex, and
more widely used, fundamental problems with current deep learning models become
more apparent. State-of-the-art deep learning models are known to suffer from
issues that range from poor robustness, inability to adapt to novel task
settings, to requiring rigid and inflexible configuration assumptions. Ideas
from collective intelligence, in particular concepts from complex systems such
as self-organization, emergent behavior, swarm optimization, and cellular
systems tend to produce solutions that are robust, adaptable, and have less
rigid assumptions about the environment configuration. It is therefore natural
to see these ideas incorporated into newer deep learning methods. In this
review, we will provide a historical context of neural network research's
involvement with complex systems, and highlight several active areas in modern
deep learning research that incorporate the principles of collective
intelligence to advance its current capabilities. To facilitate a
bi-directional flow of ideas, we also discuss work that utilize modern deep
learning models to help advance complex systems research. We hope this review
can serve as a bridge between complex systems and deep learning communities to
facilitate the cross pollination of ideas and foster new collaborations across
disciplines.

    ", Neural and Evolutionary Computing,natural language processing,Collective Intelligence for Deep Learning: A Survey of Recent Developments
"  Pattern recognition based on a high-dimensional predictor is considered. A
classifier is defined which is based on a Transformer encoder. The rate of
convergence of the misclassification probability of the classifier towards the
optimal misclassification probability is analyzed. It is shown that this
classifier is able to circumvent the curse of dimensionality provided the
aposteriori probability satisfies a suitable hierarchical composition model.
Furthermore, the difference between Transformer classifiers analyzed
theoretically in this paper and Transformer classifiers used nowadays in
practice are illustrated by considering classification problems in natural
language processing.

    ", Statistics Theory,natural language processing,On the rate of convergence of a classifier based on a Transformer encoder

"  Nowadays, due to advanced digital imaging technologies and internet
accessibility to the public, the number of generated digital images has
increased dramatically. Thus, the need for automatic image enhancement
techniques is quite apparent. In recent years, deep learning has been used
effectively. Here, after introducing some recently developed works on image
enhancement, an image enhancement system based on convolutional neural networks
is presented. Our goal is to make an effective use of two available approaches,
convolutional neural network and bilateral grid. In our approach, we increase
the training data and the model dimensions and propose a variable rate during
the training process. The enhancement results produced by our proposed method,
while incorporating 5 different experts, show both quantitative and qualitative
improvements as compared to other available methods.

    ", Image and Video Processing,convolutional neural networks,Image Enhancement via Bilateral Learning
"  We study the problem of recommending items to occasional groups (a.k.a.
cold-start groups), where the occasional groups are formed ad-hoc and
", Information Retrieval,convolutional neural networks,Self-supervised Graph Learning for Occasional Group Recommendation
"  This project focuses on the self-supervised training of convolutional neural
networks (CNNs) and transformer networks for the task of image recognition. A
simple siamese network with different backbones is used in order to maximize
the similarity of two augmented transformed images from the same source image.
In this way, the backbone is able to learn visual information without
supervision. Finally, the method is evaluated on three image recognition
datasets.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,Ablation study of self-supervised learning for image classification
"  Recent advances in the design of neural network architectures, in particular
those specialized in modeling sequences, have provided significant improvements
in speech separation performance. In this work, we propose to use a
bio-inspired architecture called Fully Recurrent Convolutional Neural Network
(FRCNN) to solve the separation task. This model contains bottom-up, top-down
and lateral connections to fuse information processed at various time-scales
represented by \textit{stages}. In contrast to the traditional approach
updating stages in parallel, we propose to first update the stages one by one
in the bottom-up direction, then fuse information from adjacent stages
simultaneously and finally fuse information from all stages to the bottom stage
together. Experiments showed that this asynchronous updating scheme achieved
significantly better results with much fewer parameters than the traditional
synchronous updating scheme. In addition, the proposed model achieved good
balance between speech separation accuracy and computational efficiency as
compared to other state-of-the-art models on three benchmark datasets.

    ", Sound,convolutional neural networks,Speech Separation Using an Asynchronous Fully Recurrent Convolutional Neural Network
"  Predicting wildfire spread is critical for land management and disaster
preparedness. To this end, we present `Next Day Wildfire Spread,' a curated,
large-scale, multivariate data set of historical wildfires aggregating nearly a
decade of remote-sensing data across the United States. In contrast to existing
fire data sets based on Earth observation satellites, our data set combines 2D
fire data with multiple explanatory variables (e.g., topography, vegetation,
weather, drought index, population density) aligned over 2D regions, providing
a feature-rich data set for machine learning. To demonstrate the usefulness of
this data set, we implement a convolutional autoencoder that takes advantage of
the spatial information of this data to predict wildfire spread. We compare the
performance of the neural network with other machine learning models: logistic
regression and random forest. This data set can be used as a benchmark for
developing wildfire propagation models based on remote sensing data for a lead
time of one day.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,Next Day Wildfire Spread: A Machine Learning Data Set to Predict Wildfire Spreading from Remote-Sensing Data
"  Attention mechanism plays a more and more important role in point cloud
analysis and channel attention is one of the hotspots. With so much channel
information, it is difficult for neural networks to screen useful channel
information. Thus, an adaptive channel encoding mechanism is proposed to
capture channel relationships in this paper. It improves the quality of the
representation generated by the network by explicitly encoding the
interdependence between the channels of its features. Specifically, a
channel-wise convolution (Channel-Conv) is proposed to adaptively learn the
relationship between coordinates and features, so as to encode the channel.
Different from the popular attention weight schemes, the Channel-Conv proposed
in this paper realizes adaptability in convolution operation, rather than
simply assigning different weights for channels. Extensive experiments on
existing benchmarks verify our method achieves the state of the arts.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,Adaptive Channel Encoding for Point Cloud Analysis
"  The modeling, computational cost, and accuracy of traditional Spatio-temporal
networks are the three most concentrated research topics in video action
recognition. The traditional 2D convolution has a low computational cost, but
it cannot capture the time relationship; the convolutional neural networks
(CNNs) model based on 3D convolution can obtain good performance, but its
computational cost is high, and the amount of parameters is large. In this
paper, we propose a plug-and-play Spatio-temporal Shift Module (STSM), which is
a generic module that is both effective and high-performance. Specifically,
after STSM is inserted into other networks, the performance of the network can
be improved without increasing the number of calculations and parameters. In
particular, when the network is 2D CNNs, our STSM module allows the network to
learn efficient Spatio-temporal features. We conducted extensive evaluations of
the proposed module, conducted numerous experiments to study its effectiveness
in video action recognition, and achieved state-of-the-art results on the
kinetics-400 and Something-Something V2 datasets.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,STSM: Spatio-Temporal Shift Module for Efficient Action Recognition
"  Artificial intelligence (AI) based device identification improves the
security of the internet of things (IoT), and accelerates the authentication
process. However, existing approaches rely on the assumption that we can learn
all the classes from the training set, namely, closed-set classification. To
overcome the closed-set limitation, we propose a novel open set RF device
identification method to classify unseen classes in the testing set. First, we
design a specific convolution neural network (CNN) with a short-time Fourier
transforming (STFT) pre-processing module, which efficiently recognizes the
differences of feature maps learned from various RF device signals. Then to
generate a representation of known class bounds, we estimate the probability
map of the open-set via the OpenMax function. We conduct experiments on sampled
data and voice signal sets, considering various pre-processing schemes, network
structures, distance metrics, tail sizes, and openness degrees. The simulation
results show the superiority of the proposed method in terms of robustness and
accuracy.

    ", Signal Processing,convolutional neural networks,Deep Open Set Identification for RF Devices
"  The pair-contact process with diffusion (PCPD), a generalized model of the
ordinary pair-contact process (PCP) without diffusion, exhibits a continuous
absorbing phase transition. Unlike the PCP, whose nature of phase transition is
clearly classified into the directed percolation (DP) universality class, the
model of PCPD has been controversially discussed since its infancy. To our best
knowledge, there is so far no consensus on whether the phase transition of the
PCPD falls into the unknown university classes or else conveys a new kind of
non-equilibrium phase transition. In this paper, both unsupervised and
supervised learning are employed to study the PCPD with scrutiny. Firstly, two
unsupervised learning methods, principal component analysis (PCA) and
autoencoder, are taken. Our results show that both methods can cluster the
original configurations of the model and provide reasonable estimates of
thresholds. Therefore, no matter whether the non-equilibrium lattice model is a
random process of unitary (for instance the DP) or binary (for instance the
PCP), or whether it contains the diffusion motion of particles, unsupervised
leaning can capture the essential, hidden information. Beyond that, supervised
learning is also applied to learning the PCPD at different diffusion rates. We
proposed a more accurate numerical method to determine the spatial correlation
exponent $\nu_{\perp}$, which, to a large degree, avoids the uncertainty of
data collapses through naked eyes. Our extensive calculations reveal that
$\nu_{\perp}$ of PCPD depends continuously on the diffusion rate $D$, which
supports the viewpoint that the PCPD may lead to a new type of absorbing phase
transition.

    ", Statistical Mechanics,unsupervised learning,Machine learning of pair-contact process with diffusion
"  The pretrain-finetune paradigm is a classical pipeline in visual learning.
Recent progress on unsupervised pretraining methods shows superior transfer
performance to their supervised counterparts. This paper revisits this
phenomenon and sheds new light on understanding the transferability gap between
unsupervised and supervised pretraining from a multilayer perceptron (MLP)
perspective. While previous works focus on the effectiveness of MLP on
unsupervised image classification where pretraining and evaluation are
conducted on the same dataset, we reveal that the MLP projector is also the key
factor to better transferability of unsupervised pretraining methods than
supervised pretraining methods. Based on this observation, we attempt to close
the transferability gap between supervised and unsupervised pretraining by
adding an MLP projector before the classifier in supervised pretraining. Our
analysis indicates that the MLP projector can help retain intra-class variation
of visual features, decrease the feature distribution distance between
pretraining and evaluation datasets, and reduce feature redundancy. Extensive
experiments on public benchmarks demonstrate that the added MLP projector
significantly boosts the transferability of supervised pretraining, \eg
\textbf{+7.2\%} top-1 accuracy on the concept generalization task,
\textbf{+5.8\%} top-1 accuracy for linear evaluation on 12-domain
classification tasks, and \textbf{+0.8\%} AP on COCO object detection task,
making supervised pretraining comparable or even better than unsupervised
pretraining. Codes will be released upon acceptance.

    ", Computer Vision and Pattern Recognition,unsupervised learning,Revisiting the Transferability of Supervised Pretraining: an MLP Perspective
"  Deep language models have achieved remarkable success in the NLP domain. The
standard way to train a deep language model is to employ unsupervised learning
from scratch on a large unlabeled corpus. However, such large corpora are only
available for widely-adopted and high-resource languages and domains. This
study presents the first deep language model, DPRK-BERT, for the DPRK language.
We achieve this by compiling the first unlabeled corpus for the DPRK language
and fine-tuning a preexisting the ROK language model. We compare the proposed
model with existing approaches and show significant improvements on two DPRK
datasets. We also present a cross-lingual version of this model which yields
better generalization across the two Korean languages. Finally, we provide
various NLP tools related to the DPRK language that would foster future
research.

    ", Computation and Language,unsupervised learning,DPRK-BERT: The Supreme Language Model
"  Vaccine hesitancy and other COVID-19-related concerns and complaints in the
Philippines are evident on social media. It is important to identify these
different topics and sentiments in order to gauge public opinion, use the
insights to develop policies, and make necessary adjustments or actions to
improve public image and reputation of the administering agency and the
COVID-19 vaccines themselves. This paper proposes a semi-supervised machine
learning pipeline to perform topic modeling, sentiment analysis, and an
analysis of vaccine brand reputation to obtain an in-depth understanding of
national public opinion of Filipinos on Facebook. The methodology makes use of
a multilingual version of Bidirectional Encoder Representations from
Transformers or BERT for topic modeling, hierarchical clustering, five
different classifiers for sentiment analysis, and cosine similarity of BERT
topic embeddings for vaccine brand reputation analysis. Results suggest that
any type of COVID-19 misinformation is an emergent property of COVID-19 public
opinion, and that the detection of COVID-19 misinformation can be an
unsupervised task. Sentiment analysis aided by hierarchical clustering reveal
that 21 of the 25 topics extrapolated by topic modeling are negative topics.
Such negative comments spike in count whenever the Department of Health in the
Philippines posts about the COVID-19 situation in other countries.
Additionally, the high numbers of laugh reactions on the Facebook posts by the
same agency -- without any humorous content -- suggest that the reactors of
these posts tend to react the way they do, not because of what the posts are
about but because of who posted them.

    ", Computers and Society,unsupervised learning,"Topic Modeling, Clade-assisted Sentiment Analysis, and Vaccine Brand Reputation Analysis of COVID-19 Vaccine-related Facebook Comments in the Philippines"
"  Accurate cardiac computing, analysis and modeling from multi-modality images
are important for the diagnosis and treatment of cardiac disease. Late
gadolinium enhancement magnetic resonance imaging (LGE MRI) is a promising
technique to visualize and quantify myocardial infarction (MI) and atrial
scars. Automating quantification of MI and atrial scars can be challenging due
to the low image quality and complex enhancement patterns of LGE MRI. Moreover,
compared with the other sequences LGE MRIs with gold standard labels are
particularly limited, which represents another obstacle for developing novel
algorithms for automatic segmentation and quantification of LGE MRIs. This
chapter aims to summarize the state-of-the-art and our recent advanced
contributions on deep learning based multi-modality cardiac image analysis.
Firstly, we introduce two benchmark works for multi-sequence cardiac MRI based
myocardial and pathology segmentation. Secondly, two novel frameworks for left
atrial scar segmentation and quantification from LGE MRI were presented.
Thirdly, we present three unsupervised domain adaptation techniques for
cross-modality cardiac image segmentation.

    ", Image and Video Processing,unsupervised learning,Multi-Modality Cardiac Image Analysis with Deep Learning
"  As Automatic Speech Processing (ASR) systems are getting better, there is an
increasing interest of using the ASR output to do downstream Natural Language
Processing (NLP) tasks. However, there are few open source toolkits that can be
used to generate reproducible results on different Spoken Language
Understanding (SLU) benchmarks. Hence, there is a need to build an open source
standard that can be used to have a faster start into SLU research. We present
ESPnet-SLU, which is designed for quick development of spoken language
understanding in a single framework. ESPnet-SLU is a project inside end-to-end
speech processing toolkit, ESPnet, which is a widely used open-source standard
for various speech processing tasks like ASR, Text to Speech (TTS) and Speech
Translation (ST). We enhance the toolkit to provide implementations for various
SLU benchmarks that enable researchers to seamlessly mix-and-match different
ASR and NLU models. We also provide pretrained models with intensively tuned
hyper-parameters that can match or even outperform the current state-of-the-art
performances. The toolkit is publicly available at
", Computation and Language,natural language processing,ESPnet-SLU: Advancing Spoken Language Understanding through ESPnet
"  Data augmentation (DA) is a common solution to data scarcity and imbalance
problems, which is an area getting increasing attentions from the Natural
Language Processing (NLP) community. While various DA techniques have been used
in NLP research, little is known about the role of linguistic knowledge in DA
for NLP; in particular, whether more linguistic knowledge leads to a better DA
approach. To investigate that, we designed two adapted DA programs and applied
them to LCQMC (a Large-scale Chinese Question Matching Corpus) for a binary
Chinese question matching classification task. The two DA programs produce
augmented texts by five simple text editing operations, largely irrespective of
language generation rules, but one is enhanced with a n-gram language model to
make it fused with extra linguistic knowledge. We then trained four neural
network models and a pre-trained model on the LCQMC train sets of varying size
as well as the corresponding augmented trained sets produced by the two DA
programs. The test set performances of the five classification models show that
adding probabilistic linguistic knowledge as constrains does not make the base
DA program better, since there are no discernible performance differences
between the models trained on the two types of augmented train sets. Instead,
since the added linguistic knowledge decreases the diversity of the augmented
texts, the trained models generalizability is hampered. Moreover, models
trained on both types of the augmented trained sets were found to be
outperformed by those directly trained on the associated un-augmented train
sets, due to the inability of the underlying text editing operations to make
paraphrastic augmented texts. We concluded that the validity and diversity of
the augmented texts are two important factors for a DA approach or technique to
be effective and proposed a possible paradigm shift for text augmentation.

    ", Computation and Language,natural language processing,Linguistic Knowledge in Data Augmentation for Natural Language Processing: An Example on Chinese Question Matching
"  Computational Humour (CH) has attracted the interest of Natural Language
Processing and Computational Linguistics communities. Creating datasets for
automatic measurement of humour quotient is difficult due to multiple possible
interpretations of the content. In this work, we create a multi-modal
humour-annotated dataset ($\sim$40 hours) using stand-up comedy clips. We
devise a novel scoring mechanism to annotate the training data with a humour
quotient score using the audience's laughter. The normalized duration (laughter
duration divided by the clip duration) of laughter in each clip is used to
compute this humour coefficient score on a five-point scale (0-4). This method
of scoring is validated by comparing with manually annotated scores, wherein a
quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a
model that provides a ""funniness"" score, on a five-point scale, given the audio
and its corresponding text. We compare various neural language models for the
task of humour-rating and achieve an accuracy of $0.813$ in terms of Quadratic
Weighted Kappa (QWK). Our ""Open Mic"" dataset is released for further research
along with the code.

    ", Computation and Language,natural language processing,"""So You Think You're Funny?"": Rating the Humour Quotient in Standup Comedy"
"  Transformers have achieved remarkable performance in a myriad of fields
including natural language processing and computer vision. However, when it
comes to the graph mining area, where graph neural network (GNN) has been the
dominant paradigm, transformers haven't achieved competitive performance,
especially on the node classification task. Existing graph transformer models
typically adopt fully-connected attention mechanism on the whole input graph
and thus suffer from severe scalability issues and are intractable to train in
data insufficient cases. To alleviate these issues, we propose a novel
Gophormer model which applies transformers on ego-graphs instead of
full-graphs. Specifically, Node2Seq module is proposed to sample ego-graphs as
the input of transformers, which alleviates the challenge of scalability and
serves as an effective data augmentation technique to boost model performance.
Moreover, different from the feature-based attention strategy in vanilla
transformers, we propose a proximity-enhanced attention mechanism to capture
the fine-grained structural bias. In order to handle the uncertainty introduced
by the ego-graph sampling, we further propose a consistency regularization and
a multi-sample inference strategy for stabilized training and testing,
respectively. Extensive experiments on six benchmark datasets are conducted to
demonstrate the superiority of Gophormer over existing graph transformers and
popular GNNs, revealing the promising future of graph transformers.

    ", Machine Learning,natural language processing,Gophormer: Ego-Graph Transformer for Node Classification
"  Deep reinforcement learning is an effective tool to learn robot control
policies from scratch. However, these methods are notorious for the enormous
amount of required training data which is prohibitively expensive to collect on
real robots. A highly popular alternative is to learn from simulations,
allowing to generate the data much faster, safer, and cheaper. Since all
simulators are mere models of reality, there are inevitable differences between
the simulated and the real data, often referenced as the 'reality gap'. To
bridge this gap, many approaches learn one policy from a distribution over
simulators. In this paper, we propose to combine reinforcement learning from
randomized physics simulations with policy distillation. Our algorithm, called
Distilled Domain Randomization (DiDoR), distills so-called teacher policies,
which are experts on domains that have been sampled initially, into a student
policy that is later deployed. This way, DiDoR learns controllers which
transfer directly from simulation to reality, i.e., without requiring data from
the target domain. We compare DiDoR against three baselines in three sim-to-sim
as well as two sim-to-real experiments. Our results show that the target domain
performance of policies trained with DiDoR is en par or better than the
baselines'. Moreover, our approach neither increases the required memory
capacity nor the time to compute an action, which may well be a point of
failure for successfully deploying the learned controller.

    ", Machine Learning,reinforcement learning,Distilled Domain Randomization
"  We propose a simple architecture for deep reinforcement learning by embedding
inputs into a learned Fourier basis and show that it improves the sample
efficiency of both state-based and image-based RL. We perform infinite-width
analysis of our architecture using the Neural Tangent Kernel and theoretically
show that tuning the initial variance of the Fourier basis is equivalent to
functional regularization of the learned deep network. That is, these learned
Fourier features allow for adjusting the degree to which networks underfit or
overfit different frequencies in the training data, and hence provide a
controlled mechanism to improve the stability and performance of RL
optimization. Empirically, this allows us to prioritize learning low-frequency
functions and speed up learning by reducing networks' susceptibility to noise
in the optimization process, such as during Bellman updates. Experiments on
standard state-based and image-based RL benchmarks show clear benefits of our
architecture over the baselines. Website at
", Machine Learning,reinforcement learning,Functional Regularization for Reinforcement Learning via Learned Fourier Features
"  Federated learning (FL) is a distributed machine learning technology for
next-generation AI systems that allows a number of workers, i.e., edge devices,
collaboratively learn a shared global model while keeping their data locally to
prevent privacy leakage. Enabling FL over wireless multi-hop networks can
democratize AI and make it accessible in a cost-effective manner. However, the
noisy bandwidth-limited multi-hop wireless connections can lead to delayed and
nomadic model updates, which significantly slows down the FL convergence speed.
To address such challenges, this paper aims to accelerate FL convergence over
wireless edge by optimizing the multi-hop federated networking performance. In
particular, the FL convergence optimization problem is formulated as a Markov
decision process (MDP). To solve such MDP, multi-agent reinforcement learning
(MA-RL) algorithms along with domain-specific action space refining schemes are
developed, which online learn the delay-minimum forwarding paths to minimize
the model exchange latency between the edge devices (i.e., workers) and the
remote server. To validate the proposed solutions, FedEdge is developed and
implemented, which is the first experimental framework in the literature for FL
over multi-hop wireless edge computing networks. FedEdge allows us to fast
prototype, deploy, and evaluate novel FL algorithms along with RL-based system
optimization methods in real wireless devices. Moreover, a physical
experimental testbed is implemented by customizing the widely adopted Linux
wireless routers and ML computing nodes.Finally, our experimentation results on
the testbed show that the proposed network-accelerated FL system can
practically and significantly improve FL convergence speed, compared to the FL
system empowered by the production-grade commercially available wireless
networking protocol, BATMAN-Adv.

    ", Networking and Internet Architecture,reinforcement learning,EdgeML: Towards Network-Accelerated Federated Learning over Wireless Edge
"  In this study, we present a meta-learning model to adapt the predictions of
the network's capacity between viewers who participate in a live video
streaming event. We propose the MELANIE model, where an event is formulated as
a Markov Decision Process, performing meta-learning on reinforcement learning
tasks. By considering a new event as a task, we design an actor-critic learning
scheme to compute the optimal policy on estimating the viewers' high-bandwidth
connections. To ensure fast adaptation to new connections or changes among
viewers during an event, we implement a prioritized replay memory buffer based
on the Kullback-Leibler divergence of the reward/throughput of the viewers'
connections. Moreover, we adopt a model-agnostic meta-learning framework to
generate a global model from past events. As viewers scarcely participate in
several events, the challenge resides on how to account for the low structural
similarity of different events. To combat this issue, we design a graph
signature buffer to calculate the structural similarities of several streaming
events and adjust the training of the global model accordingly. We evaluate the
proposed model on the link weight prediction task on three real-world datasets
of live video streaming events. Our experiments demonstrate the effectiveness
of our proposed model, with an average relative gain of 25% against
state-of-the-art strategies. For reproduction purposes, our evaluation datasets
and implementation are publicly available at
", Networking and Internet Architecture,reinforcement learning,Meta-Reinforcement Learning via Buffering Graph Signatures for Live Video Streaming Events
"  This paper proposes a deep reinforcement learning-based video streaming
scheme for mobility-aware vehicular networks, e.g., vehicles on the highway. We
consider infrastructure-assisted and mmWave-based scenarios in which the macro
base station (MBS) cannot directly provide the streaming service to vehicles
due to the short range of mmWave beams so that small mmWave base stations
(mBSs) along the road deliver the desired videos to users. For a smoother
streaming service, the MBS proactively pushes video chunks to mBSs. This is
done to support vehicles that are currently covered and/or will be by each mBS.
We formulate the dynamic video delivery scheme that adaptively determines 1)
which content, 2) what quality and 3) how many chunks to be proactively
delivered from the MBS to mBSs using Markov decision process (MDP). Since it is
difficult for the MBS to track all the channel conditions and the network
states have extensive dimensions, we adopt the deep deterministic policy
gradient (DDPG) algorithm for the DRL-based video delivery scheme. This paper
finally shows that the DRL agent learns a streaming policy that pursues high
average quality while limiting packet drops, avoiding playback stalls, reducing
quality fluctuations and saving backhaul usage.

    ", Networking and Internet Architecture,reinforcement learning,Quality-Aware Deep Reinforcement Learning for Streaming in Infrastructure-Assisted Connected Vehicles
"  In this paper, we present a comparative study on the robustness of two
different online streaming speech recognition models: Monotonic Chunkwise
Attention (MoChA) and Recurrent Neural Network-Transducer (RNN-T). We explore
three recently proposed data augmentation techniques, namely, multi-conditioned
training using an acoustic simulator, Vocal Tract Length Perturbation (VTLP)
for speaker variability, and SpecAugment. Experimental results show that
unidirectional models are in general more sensitive to noisy examples in the
training set. It is observed that the final performance of the model depends on
the proportion of training examples processed by data augmentation techniques.
MoChA models generally perform better than RNN-T models. However, we observe
that training of MoChA models seems to be more sensitive to various factors
such as the characteristics of training sets and the incorporation of
additional augmentations techniques. On the other hand, RNN-T models perform
better than MoChA models in terms of latency, inference time, and the stability
of training. Additionally, RNN-T models are generally more robust against noise
and reverberation. All these advantages make RNN-T models a better choice for
streaming on-device speech recognition compared to MoChA models.

    ", Audio and Speech Processing,recurrent neural networks,A comparison of streaming models and data augmentation methods for robust speech recognition
"  Cosmological analyses of samples of photometrically-identified Type Ia
supernovae (SNe Ia) depend on understanding the effects of 'contamination' from
core-collapse and peculiar SN Ia events. We employ a rigorous analysis on
state-of-the-art simulations of photometrically identified SN Ia samples and
determine cosmological biases due to such 'non-Ia' contamination in the Dark
Energy Survey (DES) 5-year SN sample. As part of the analysis, we test on our
DES simulations the performance of SuperNNova, a photometric SN classifier
based on recurrent neural networks. Depending on the choice of non-Ia SN models
in both the simulated data sample and training sample, contamination ranges
from 0.8-3.5 %, with the efficiency of the classification from 97.7-99.5 %.
Using the Bayesian Estimation Applied to Multiple Species (BEAMS) framework and
its extension 'BEAMS with Bias Correction' (BBC), we produce a redshift-binned
Hubble diagram marginalised over contamination and corrected for selection
effects and we use it to constrain the dark energy equation-of-state, $w$.
Assuming a flat universe with Gaussian $\Omega_M$ prior of $0.311\pm0.010$, we
show that biases on $w$ are $<0.008$ when using SuperNNova and accounting for a
wide range of non-Ia SN models in the simulations. Systematic uncertainties
associated with contamination are estimated to be at most $\sigma_{w,
\mathrm{syst}}=0.004$. This compares to an expected statistical uncertainty of
$\sigma_{w,\mathrm{stat}}=0.039$ for the DES-SN sample, thus showing that
contamination is not a limiting uncertainty in our analysis. We also measure
biases due to contamination on $w_0$ and $w_a$ (assuming a flat universe), and
find these to be $<$0.009 in $w_0$ and $<$0.108 in $w_a$, hence 5 to 10 times
smaller than the statistical uncertainties expected from the DES-SN sample.

    ", Cosmology and Nongalactic Astrophysics,recurrent neural networks,The Dark Energy Survey Supernova Program: Cosmological biases from supernova photometric classification
"  Testing self-driving cars in different areas requires surrounding cars with
accordingly different driving styles such as aggressive or conservative styles.
A method of numerically measuring and differentiating human driving styles to
create a virtual driver with a certain driving style is in demand. However,
most methods for measuring human driving styles require thresholds or labels to
classify the driving styles, and some require additional questionnaires for
drivers about their driving attitude. These limitations are not suitable for
creating a large virtual testing environment. Driving models (DMs) simulate
human driving styles. Calibrating a DM makes the simulated driving behavior
closer to human-driving behavior, and enable the simulation of human-driving
cars. Conventional DM-calibrating methods do not take into account that the
parameters in a DM vary while driving. These ""fixed"" calibrating methods cannot
reflect an actual interactive driving scenario. In this paper, we propose a
DM-calibration method for measuring human driving styles to reproduce real
car-following behavior more accurately. The method includes 1) an objective
entropy weight method for measuring and clustering human driving styles, and 2)
online adaption of DM parameters based on deep learning by combining Bayesian
optimization (BO) and a gated recurrent unit neural network. We conducted
experiments to evaluate the proposed method, and the results indicate that it
can be easily used to measure human driver styles. The experiments also showed
that we can calibrate a corresponding DM in a virtual testing environment with
up to 26% more accuracy than with fixed calibration methods.

    ", Machine Learning,recurrent neural networks,Online Adaptation of Parameters using GRU-based Neural Network with BO for Accurate Driving Model
"  Hand pose estimation (HPE) can be used for a variety of human-computer
interaction applications such as gesture-based control for physical or
virtual/augmented reality devices. Recent works have shown that videos or
multi-view images carry rich information regarding the hand, allowing for the
development of more robust HPE systems. In this paper, we present the
Multi-View Video-Based 3D Hand (MuViHand) dataset, consisting of multi-view
videos of the hand along with ground-truth 3D pose labels. Our dataset includes
more than 402,000 synthetic hand images available in 4,560 videos. The videos
have been simultaneously captured from six different angles with complex
backgrounds and random levels of dynamic lighting. The data has been captured
from 10 distinct animated subjects using 12 cameras in a semi-circle topology
where six tracking cameras only focus on the hand and the other six fixed
cameras capture the entire body. Next, we implement MuViHandNet, a neural
pipeline consisting of image encoders for obtaining visual embeddings of the
hand, recurrent learners to learn both temporal and angular sequential
information, and graph networks with U-Net architectures to estimate the final
3D pose information. We perform extensive experiments and show the challenging
nature of this new dataset as well as the effectiveness of our proposed method.
Ablation studies show the added value of each component in MuViHandNet, as well
as the benefit of having temporal and sequential information in the dataset.

    ", Computer Vision and Pattern Recognition,recurrent neural networks,Multi-View Video-Based 3D Hand Pose Estimation
"  Currently, due to the advantages of light weight, simple deployment,
multi-environment support, short startup time, scalability, and easy migration,
container technology has been widely used in both cloud and edge/fog computing,
and addresses the problem of device heterogeneity in different computing
environments. On this basis, as one of the most popular container orchestration
and management systems, Kubernetes almost dominates the cloud environment.
However, since it is primarily designed for centralized resource management
scenarios where computing resources are sufficient, the system is unstable in
edge environments due to hardware limitations. Therefore, in order to realize
container orchestration in the cloud and edge/fog hybrid computing environment,
we propose a feasible approach to build a hybrid clustering based on K3s, which
solves the problem that virtual instances in different environments cannot be
connected due to IP addresses. We also propose three design patterns for
deploying the FogBus2 framework into hybrid environments, including 1) Host
Network Mode, 2) Proxy Server, and 3) Environment Variable.

    "," Distributed, Parallel, and Cluster Computing",clustering,Container Orchestration Techniques in Cloud and Edge/Fog Computing Environments
"  The AcOH${}^+$ molecular ion is identified as a prospective system to search
for $\mathcal{CP}$-violation effects. According to our study AcOH${}^+$ belongs
to the class of laser-coolable polyatomic molecular cations implying the large
coherence time in the experiments to study symmetry violating effects of
fundamental interactions. We perform both nuclear and high level relativistic
coupled cluster electronic structure calculations to express experimentally
measurable $\mathcal{T}$,$\mathcal{P}$-violating energy shift in terms of
fundamental quantities such as the nuclear magnetic quadrupole moment (MQM),
electron electric dipole moment ($e$EDM) and dimensionless scalar-pseudoscalar
nuclear-electron interaction constant. We further express nuclear MQM in terms
of the strength constants of $\mathcal{CP}$-violating nuclear forces: quantum
chromodynamics vacuum angle $\bar{\theta}$ and quark chromo-EDMs. The
equilibrium geometry of AcOH${}^+$ in the ground and the four lowest excited
electronic states was found to be linear. The calculated Franck-Condon factors
and transition dipole moments indicate that the laser cooling using optical
cycle involving the first excited state is possible for the trapped AcOH${}^+$
ions with the Doppler limit estimated to be~$\sim 4$~nK. The lifetime of the
(0,1$^1$,0) excited vibrational state considered as a working one for MQM and
$e$EDM search experiments is estimated to be $\sim 0.4$ sec.

    ", Atomic Physics,clustering,Laser-coolable AcOH$^+$ ion for $\mathcal{CP}$-violation searches
"  ISCEA (Infrared Satellite for Cosmic Evolution Astrophysics) is a small
astrophysics mission whose Science Goal is to discover how galaxies evolved in
the cosmic web of dark matter at cosmic noon. Its Science Objective is to
determine the history of star formation and its quenching in galaxies as a
function of local density and stellar mass when the Universe was 3-5 Gyrs old
(1.2<z<2.1). ISCEA is designed to test the Science Hypothesis that during the
period of cosmic noon, at 1.7 < z < 2.1, environmental quenching is the
dominant quenching mechanism for typical galaxies not only in clusters and
groups, but also in the extended cosmic web surrounding these structures. ISCEA
meets its Science Objective by making a 10% shot noise measurement of star
formation rate down to 6 solar masses per year using H-alpha out to a radius >
10 Mpc in each of 50 protocluster (cluster and cosmic web) fields at 1.2 < z <
2.1. ISCEA measures the star formation quenching factor in those fields, and
galaxy kinematics with a precision < 50 km/s to deduce the 3D spatial
distribution in each field. ISCEA will transform our understanding of galaxy
evolution at cosmic noon.
", Astrophysics of Galaxies,clustering,Illuminating Galaxy Evolution at Cosmic Noon with ISCEA: the Infrared Satellite for Cosmic Evolution Astrophysics
"  The $\mathrm{SU}(4)$ Heisenberg model on the honeycomb lattice is expected to
host a quantum spin-orbital liquid at low temperature with an astonishing
candidate material, $\alpha$-ZrCl$_3$. We employed the canonical thermal pure
quantum state method to investigate the finite-temperature phase of this model.
Exploiting the full symmetry of $\mathrm{SU}(4)$, the calculation up to a
24-site cluster, which is equivalent to 48 sites in the spin-1/2 language, is
possible. This state-of-the-art computation with large-scale parallelization
enables us to capture the thermodynamic properties of the $\mathrm{SU}(4)$
Heisenberg model on the honeycomb lattice. In particular, the specific heat
shows a characteristic peak-and-shoulder structure, which should be related to
the nature of the low-temperature quantum spin-orbital liquid phase. We also
discuss what can be concluded from the assumption that the ground state is
gapped and symmetric in view of the generalized Lieb-Schultz-Mattis theorem.

    ", Strongly Correlated Electrons,clustering,Thermodynamic signature of the $\mathrm{SU}(4)$ spin-orbital liquid and symmetry fractionalization from the Lieb-Schultz-Mattis theorem
"  The CaFe Project involves the study of the properties of the low ionization
emission lines (LILs) pertaining to the broad-line region (BLR) in active
galaxies. These emission lines, especially the singly-ionized iron (Fe II) in
the optical and the corresponding singly-ionized calcium (Ca II) in the
near-infrared (NIR) are found to show a strong correlation in their emission
strengths, i.e., with respect to the broad H$\beta$ emission line, the latter
also belonging to the same category of LILs. We outline the progress made in
the past years that has developed our understanding of the location and the
efficient production of these emission lines. We have yet to realize the full
potential of Ca II emission and its connection to the black hole and the BLR
parameters which can be useful in - (1) the classification of Type-1 active
galactic nuclei (AGNs) in the context of the main sequence of quasars, (2) to
realize an updated radius-luminosity relation wherein the inclusion of the
strength of this emission line with respect to H$\beta$ can be an effective
tracer of the accretion rate of the AGN, and, (3) the close connection of Ca II
to Fe II can allow us to use the ratio of the two species to quantify the
chemical evolution in these active galaxies across cosmic time. In this paper,
we use our current sample and utilize a non-linear dimensionality reduction
technique - t-distributed Stochastic Neighbour Embedding (tSNE), to understand
the clustering in our dataset based on direct observables.

    ", Astrophysics of Galaxies,clustering,Optical Fe II and near-infrared Ca II emission in active galaxies
"  We present CpT: Convolutional point Transformer - a novel deep learning
architecture for dealing with the unstructured nature of 3D point cloud data.
CpT is an improvement over existing attention-based Convolutions Neural
Networks as well as previous 3D point cloud processing transformers. It
achieves this feat due to its effectiveness in creating a novel and robust
attention-based point set embedding through a convolutional projection layer
crafted for processing dynamically local point set neighbourhoods. The
resultant point set embedding is robust to the permutations of the input
points. Our novel CpT block builds over local neighbourhoods of points obtained
via a dynamic graph computation at each layer of the networks' structure. It is
fully differentiable and can be stacked just like convolutional layers to learn
global properties of the points. We evaluate our model on standard benchmark
datasets such as ModelNet40, ShapeNet Part Segmentation, and the S3DIS 3D
indoor scene semantic segmentation dataset to show that our model can serve as
an effective backbone for various point cloud processing tasks when compared to
the existing state-of-the-art approaches.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,CpT: Convolutional Point Transformer for 3D Point Cloud Processing
"  Incorporating group symmetry directly into the learning process has proved to
be an effective guideline for model design. By producing features that are
guaranteed to transform covariantly to the group actions on the inputs,
group-equivariant convolutional neural networks (G-CNNs) achieve significantly
improved generalization performance in learning tasks with intrinsic symmetry.
General theory and practical implementation of G-CNNs have been studied for
planar images under either rotation or scaling transformation, but only
individually. We present, in this paper, a roto-scale-translation equivariant
CNN (RST-CNN), that is guaranteed to achieve equivariance jointly over these
three groups via coupled group convolutions. Moreover, as symmetry
transformations in reality are rarely perfect and typically subject to input
deformation, we provide a stability analysis of the equivariance of
representation to input distortion, which motivates the truncated expansion of
the convolutional filters under (pre-fixed) low-frequency spatial modes. The
resulting model provably achieves deformation-robust RST equivariance, i.e.,
the RST symmetry is still ""approximately"" preserved when the transformation is
""contaminated"" by a nuisance data deformation, a property that is especially
important for out-of-distribution generalization. Numerical experiments on
MNIST, Fashion-MNIST, and STL-10 demonstrate that the proposed model yields
remarkable gains over prior arts, especially in the small data regime where
both rotation and scaling variations are present within the data.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,Deformation Robust Roto-Scale-Translation Equivariant CNNs
"  Wet weather makes water film over the road and that film causes lower
friction between tire and road surface. When a vehicle passes the low-friction
road, the accident can occur up to 35% higher frequency than a normal condition
road. In order to prevent accidents as above, identifying the road condition in
real-time is essential. Thus, we propose a convolutional auto-encoder-based
anomaly detection model for taking both less computational resources and
achieving higher anomaly detection performance. The proposed model adopts a
non-compression method rather than a conventional bottleneck structured
auto-encoder. As a result, the computational cost of the neural network is
reduced up to 1 over 25 compared to the conventional models and the anomaly
detection performance is improved by up to 7.72%. Thus, we conclude the
proposed model as a cutting-edge algorithm for real-time anomaly detection.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,Efficient Non-Compression Auto-Encoder for Driving Noise-based Road Surface Anomaly Detection
"  Recent improvements in convolutional neural network (CNN)-based single image
super-resolution (SISR) methods rely heavily on fabricating network
architectures, rather than finding a suitable training algorithm other than
simply minimizing the regression loss. Adapting knowledge distillation (KD) can
open a way for bringing further improvement for SISR, and it is also beneficial
in terms of model efficiency. KD is a model compression method that improves
the performance of Deep Neural Networks (DNNs) without using additional
parameters for testing. It is getting the limelight recently for its competence
at providing a better capacity-performance tradeoff. In this paper, we propose
a novel feature distillation (FD) method which is suitable for SISR. We show
the limitations of the existing FitNet-based FD method that it suffers in the
SISR task, and propose to modify the existing FD algorithm to focus on local
feature information. In addition, we propose a teacher-student-difference-based
soft feature attention method that selectively focuses on specific pixel
locations to extract feature information. We call our method local-selective
feature distillation (LSFD) and verify that our method outperforms conventional
FD methods in SISR problems.

    ", Image and Video Processing,convolutional neural networks,Local-Selective Feature Distillation for Single Image Super-Resolution
"  Musicologists use various labels to classify similar music styles under a
shared title. But, non-specialists may categorize music differently. That could
be through finding patterns in harmony, instruments, and form of the music.
People usually identify a music genre solely by listening, but now computers
and Artificial Intelligence (AI) can automate this process. The work on
applying AI in the classification of types of music has been growing recently,
but there is no evidence of such research on the Kurdish music genres. In this
research, we developed a dataset that contains 880 samples from eight different
Kurdish music genres. We evaluated two machine learning approaches, a Deep
Neural Network (DNN) and a Convolutional Neural Network (CNN), to recognize the
genres. The results showed that the CNN model outperformed the DNN by achieving
92% versus 90% accuracy.

    ", Sound,convolutional neural networks,Comparing the Accuracy of Deep Neural Networks (DNN) and Convolutional Neural Network (CNN) in Music Genre Recognition (MGR): Experiments on Kurdish Music
"  We study the training of Vision Transformers for semi-supervised image
classification. Transformers have recently demonstrated impressive performance
on a multitude of supervised learning tasks. Surprisingly, we find Vision
Transformers perform poorly on a semi-supervised ImageNet setting. In contrast,
Convolutional Neural Networks (CNNs) achieve superior results in small labeled
data regime. Further investigation reveals that the reason is CNNs have strong
spatial inductive bias. Inspired by this observation, we introduce a joint
semi-supervised learning framework, Semiformer, which contains a Transformer
branch, a Convolutional branch and a carefully designed fusion module for
knowledge sharing between the branches. The Convolutional branch is trained on
the limited supervised data and generates pseudo labels to supervise the
training of the transformer branch on unlabeled data. Extensive experiments on
ImageNet demonstrate that Semiformer achieves 75.5\% top-1 accuracy,
outperforming the state-of-the-art. In addition, we show Semiformer is a
general framework which is compatible with most modern Transformer and
Convolutional neural architectures.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,Semi-Supervised Vision Transformers
"  There are limited works showing the efficacy of unsupervised
Out-of-Distribution (OOD) methods on complex medical data. Here, we present
preliminary findings of our unsupervised OOD detection algorithm, SimCLR-LOF,
as well as a recent state of the art approach (SSD), applied on medical images.
SimCLR-LOF learns semantically meaningful features using SimCLR and uses LOF
for scoring if a test sample is OOD. We evaluated on the multi-source
International Skin Imaging Collaboration (ISIC) 2019 dataset, and show results
that are competitive with SSD as well as with recent supervised approaches
applied on the same data.

    ", Image and Video Processing,unsupervised learning,Unsupervised Approaches for Out-Of-Distribution Dermoscopic Lesion Detection
"  Graph representation learning methods generate numerical vector
representations for the nodes in a network, thereby enabling their use in
standard machine learning models. These methods aim to preserve relational
information, such that nodes that are similar in the graph are found close to
one another in the representation space. Similarity can be based largely on one
of two notions: connectivity or structural role. In tasks where node structural
role is important, connectivity based methods show poor performance. Recent
work has begun to focus on scalability of learning methods to massive graphs of
millions to billions of nodes and edges. Many unsupervised node representation
learning algorithms are incapable of scaling to large graphs, and are unable to
generate node representations for unseen nodes. In this work, we propose
Inferential SIR-GN, a model which is pre-trained on random graphs, then
computes node representations rapidly, including for very large networks. We
demonstrate that the model is able to capture node's structural role
information, and show excellent performance at node and graph classification
tasks, on unseen networks. Additionally, we observe the scalability of
Inferential SIR-GN is comparable to the fastest current approaches for massive
graphs.

    ", Machine Learning,unsupervised learning,Inferential SIR-GN: Scalable Graph Representation Learning
"  We demonstrate that Domain Invariant Feature Learning (DIFL) can improve the
out-of-domain generalizability of a deep learning Tuberculosis screening
algorithm. It is well known that state of the art deep learning algorithms
often have difficulty generalizing to unseen data distributions due to ""domain
shift"". In the context of medical imaging, this could lead to unintended biases
such as the inability to generalize from one patient population to another. We
analyze the performance of a ResNet-50 classifier for the purposes of
Tuberculosis screening using the four most popular public datasets with
geographically diverse sources of imagery. We show that without domain
adaptation, ResNet-50 has difficulty in generalizing between imaging
distributions from a number of public Tuberculosis screening datasets with
imagery from geographically distributed regions. However, with the
incorporation of DIFL, the out-of-domain performance is greatly enhanced.
Analysis criteria includes a comparison of accuracy, sensitivity, specificity
and AUC over both the baseline, as well as the DIFL enhanced algorithms. We
conclude that DIFL improves generalizability of Tuberculosis screening while
maintaining acceptable accuracy over the source domain imagery when applied
across a variety of public datasets.

    ", Image and Video Processing,unsupervised learning,Mitigating domain shift in AI-based tuberculosis screening with unsupervised domain adaptation
"  Unsupervised dialogue structure learning is an important and meaningful task
in natural language processing. The extracted dialogue structure and process
can help analyze human dialogue, and play a vital role in the design and
evaluation of dialogue systems. The traditional dialogue system requires
experts to manually design the dialogue structure, which is very costly. But
through unsupervised dialogue structure learning, dialogue structure can be
automatically obtained, reducing the cost of developers constructing dialogue
process. The learned dialogue structure can be used to promote the dialogue
generation of the downstream task system, and improve the logic and consistency
of the dialogue robot's ", Computation and Language,unsupervised learning,DSBERT:Unsupervised Dialogue Structure learning with BERT
"  Unsupervised video person re-identification (reID) methods usually depend on
global-level features. And many supervised reID methods employed local-level
features and achieved significant performance improvements. However, applying
local-level features to unsupervised methods may introduce an unstable
performance. To improve the performance stability for unsupervised video reID,
this paper introduces a general scheme fusing part models and unsupervised
learning. In this scheme, the global-level feature is divided into equal
local-level feature. A local-aware module is employed to explore the poentials
of local-level feature for unsupervised learning. A global-aware module is
proposed to overcome the disadvantages of local-level features. Features from
these two modules are fused to form a robust feature representation for each
input image. This feature representation has the advantages of local-level
feature without suffering from its disadvantages. Comprehensive experiments are
conducted on three benchmarks, including PRID2011, iLIDS-VID, and
DukeMTMC-VideoReID, and the results demonstrate that the proposed approach
achieves state-of-the-art performance. Extensive ablation studies demonstrate
the effectiveness and robustness of proposed scheme, local-aware module and
global-aware module.

    ", Computer Vision and Pattern Recognition,unsupervised learning,Exploiting Robust Unsupervised Video Person Re-identification
"  Pretraining language models with next-token prediction on massive text
corpora has delivered phenomenal zero-shot, few-shot, transfer learning and
multi-tasking capabilities on both generative and discriminative language
tasks. Motivated by this success, we explore a Vector-quantized Image Modeling
(VIM) approach that involves pretraining a Transformer to predict rasterized
image tokens autoregressively. The discrete image tokens are encoded from a
learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple
improvements over vanilla VQGAN from architecture to codebook learning,
yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN
further improves vector-quantized image modeling tasks, including
unconditional, class-conditioned image generation and unsupervised
representation learning. When trained on ImageNet at 256x256 resolution, we
achieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of
4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and
17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised
pretraining, we further evaluate the pretrained Transformer by averaging
intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained
VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 72.2%
for a similar model size. ViM-L also outperforms iGPT-XL which is trained with
extra web image data and larger model size.

    ", Computer Vision and Pattern Recognition,unsupervised learning,Vector-quantized Image Modeling with Improved VQGAN
"  The micro-segmentation of customers in the finance sector is a non-trivial
task and has been an atypical omission from recent scientific literature. Where
traditional segmentation classifies customers based on coarse features such as
demographics, micro-segmentation depicts more nuanced differences between
individuals, bringing forth several advantages including the potential for
improved personalization in financial services. AI and representation learning
offer a unique opportunity to solve the problem of micro-segmentation. Although
ubiquitous in many industries, the proliferation of AI in sensitive industries
such as finance has become contingent on the imperatives of responsible AI. We
had previously solved the micro-segmentation problem by extracting temporal
features from the state space of a recurrent neural network (RNN). However, due
to the inherent opacity of RNNs our solution lacked an explanation - one of the
imperatives of responsible AI. In this study, we address this issue by
extracting an explanation for and providing an interpretation of our temporal
features. We investigate the state space of our RNN and through a linear
regression model reconstruct the trajectories in the state space with high
fidelity. We show that our linear regression coefficients have not only learned
the rules used to create the RNN's output data but have also learned the
relationships that were not directly evident in the raw data.

    ", Machine Learning,recurrent neural networks,Discovering Novel Customer Features with Recurrent Neural Networks for Personality Based Financial Services
"  Data assimilation (DA) is integrated with machine learning in order to
perform entirely data-driven online state estimation. To achieve this,
recurrent neural networks (RNNs) are implemented as surrogate models to replace
key components of the DA cycle in numerical weather prediction (NWP), including
the conventional numerical forecast model, the forecast error covariance
matrix, and the tangent linear and adjoint models. It is shown how these RNNs
can be initialized using DA methods to directly update the hidden/reservoir
state with observations of the target system. The results indicate that these
techniques can be applied to estimate the state of a system for the repeated
initialization of short-term forecasts, even in the absence of a traditional
numerical forecast model. Further, it is demonstrated how these integrated
RNN-DA methods can scale to higher dimensions by applying domain localization
and parallelization, providing a path for practical applications in NWP.

    ", Machine Learning,recurrent neural networks,Integrating Recurrent Neural Networks with Data Assimilation for Scalable Data-Driven State Estimation
"  Developing intelligent neuromorphic solutions remains a challenging
endeavour. It requires a solid conceptual understanding of the hardware's
fundamental building blocks. Beyond this, accessible and user-friendly
prototyping is crucial to speed up the design pipeline. We developed an open
source Loihi emulator based on the neural network simulator Brian that can
easily be incorporated into existing simulation workflows. We demonstrate
errorless Loihi emulation in software for a single neuron and for a recurrently
connected spiking neural network. On-chip learning is also reviewed and
implemented, with reasonable discrepancy due to stochastic rounding. This work
provides a coherent presentation of Loihi's computational unit and introduces a
new, easy-to-use Loihi prototyping package with the aim to help streamline
conceptualisation and deployment of new algorithms.

    ", Neural and Evolutionary Computing,recurrent neural networks,Brian2Loihi: An emulator for the neuromorphic chip Loihi using the spiking neural network simulator Brian
"  Tracking a turbulent plume to locate its source is a complex control problem
because it requires multi-sensory integration and must be robust to
intermittent odors, changing wind direction, and variable plume statistics.
This task is routinely performed by flying insects, often over long distances,
in pursuit of food or mates. Several aspects of this remarkable behavior have
been studied in detail in many experimental studies. Here, we take a
complementary in silico approach, using artificial agents trained with
reinforcement learning to develop an integrated understanding of the behaviors
and neural computations that support plume tracking. Specifically, we use deep
reinforcement learning (DRL) to train recurrent neural network (RNN) agents to
locate the source of simulated turbulent plumes. Interestingly, the agents'
emergent behaviors resemble those of flying insects, and the RNNs learn to
represent task-relevant variables, such as head direction and time since last
odor encounter. Our analyses suggest an intriguing experimentally testable
hypothesis for tracking plumes in changing wind direction -- that agents follow
local plume shape rather than the current wind direction. While reflexive
short-memory behaviors are sufficient for tracking plumes in constant wind,
longer timescales of memory are essential for tracking plumes that switch
direction. At the level of neural dynamics, the RNNs' population activity is
low-dimensional and organized into distinct dynamical structures, with some
correspondence to behavioral modules. Our in silico approach provides key
intuitions for turbulent plume tracking strategies and motivates future
targeted experimental and theoretical developments.

    ", Neurons and Cognition,recurrent neural networks,Emergent behavior and neural dynamics in artificial agents tracking turbulent plumes
"  Integration of renewable energy sources and emerging loads like electric
vehicles to smart grids brings more uncertainty to the distribution system
management. Demand Side Management (DSM) is one of the approaches to reduce the
uncertainty. Some applications like Nonintrusive Load Monitoring (NILM) can
support DSM, however they require accurate forecasting on high resolution data.
This is challenging when it comes to single loads like one residential
household due to its high volatility. In this paper, we review some of the
existing Deep Learning-based methods and present our solution using Time
Pooling Deep Recurrent Neural Network. The proposed method augments data using
time pooling strategy and can overcome overfitting problems and model
uncertainties of data more efficiently. Simulation and implementation results
show that our method outperforms the existing algorithms in terms of RMSE and
MAE metrics.

    ", Machine Learning,recurrent neural networks,Short-Term Load Forecasting Using Time Pooling Deep Recurrent Neural Network
"  The fourth industrial revolution (4IR) is likely to have a substantial impact
on the economy. Companies need to build up capabilities to implement new
technologies, and automation may make some occupations obsolete. However,
where, when, and how the change will happen remain to be determined. Robust
empirical indicators of technological progress linked to occupations can help
to illuminate this change. With this aim, we provide such an indicator based on
patent data. Using natural language processing, we calculate patent exposure
scores for more than 900 occupations, which represent the technological
progress related to them. To provide a lens on the impact of the 4IR, we
differentiate between traditional and 4IR patent exposure. Our method differs
from previous approaches in that it both accounts for the diversity of
task-level patent exposures within an occupation and reflects work activities
more accurately. We find that exposure to 4IR patents differs from traditional
patent exposure. Manual tasks, and accordingly occupations such as construction
and production, are exposed mainly to traditional (non-4IR) patents but have
low exposure to 4IR patents. The analysis suggests that 4IR technologies may
have a negative impact on job growth; this impact appears 10 to 20 years after
patent filing. Further, we compared the 4IR exposure to other automation and AI
exposure scores. Whereas many measures refer to theoretical automation
potential, our patent-based indicator reflects actual technology diffusion. Our
work not only allows analyses of the impact of 4IR technologies as a whole, but
also provides exposure scores for more than 300 technology fields, such as AI
and smart office technologies. Finally, the work provides a general mapping of
patents to tasks and occupations, which enables future researchers to construct
individual exposure measures.

    ", Computers and Society,natural language processing,Exposure of occupations to technologies of the fourth industrial revolution
"  Intent classification (IC) and slot filling (SF) are critical building blocks
in task-oriented dialogue systems. These two tasks are closely-related and can
flourish each other. Since only a few utterances can be utilized for
identifying fast-emerging new intents and slots, data scarcity issue often
occurs when implementing IC and SF. However, few IC/SF models perform well when
the number of training samples per class is quite small. In this paper, we
propose a novel explicit-joint and supervised-contrastive learning framework
for few-shot intent classification and slot filling. Its highlights are as
follows. (i) The model extracts intent and slot representations via
bidirectional interactions, and extends prototypical network to achieve
explicit-joint learning, which guarantees that IC and SF tasks can mutually
reinforce each other. (ii) The model integrates with supervised contrastive
learning, which ensures that samples from same class are pulled together and
samples from different classes are pushed apart. In addition, the model follows
a not common but practical way to construct the episode, which gets rid of the
traditional setting with fixed way and shot, and allows for unbalanced
datasets. Extensive experiments on three public datasets show that our model
can achieve promising performance.

    ", Computation and Language,natural language processing,An Explicit-Joint and Supervised-Contrastive Learning Framework for Few-Shot Intent Classification and Slot Filling
"  Graph neural networks (GNNs) integrate the comprehensive relation of graph
data and the representation learning capability of neural networks, which is
one of the most popular deep learning methods and achieves state-of-the-art
performance in many applications, such as natural language processing and
computer vision. In real-world scenarios, increasing the depth (i.e., the
number of layers) of GNNs is sometimes necessary to capture more latent
knowledge of the input data to mitigate the uncertainty caused by missing
values. However, involving more complex structures and more parameters will
decrease the performance of GNN models. One reason called oversmoothing is
recently introduced but the relevant research remains nascent. In general,
oversmoothing makes the final representations of nodes indiscriminative, thus
deteriorating the node classification and link prediction performance. In this
paper, we first survey the current de-oversmoothing methods and propose three
major metrics to evaluate a de-oversmoothing method, i.e., constant divergence
indicator, easy-to-determine divergence indicator, and model-agnostic strategy.
Then, we propose the Topology-guided Graph Contrastive Layer, named TGCL, which
is the first de-oversmoothing method maintaining all three mentioned metrics.
With the contrastive learning manner, we provide the theoretical analysis of
the effectiveness of the proposed TGCL. Last but not least, we design extensive
experiments to illustrate the empirical performance of TGCL comparing with
state-of-the-art baselines.

    ", Machine Learning,natural language processing,Tackling Oversmoothing of GNNs with Contrastive Learning
"  The growing use of social media has led to the development of several Machine
Learning (ML) and Natural Language Processing(NLP) tools to process the
unprecedented amount of social media content to make actionable decisions.
However, these MLand NLP algorithms have been widely shown to be vulnerable to
adversarial attacks. These vulnerabilities allow adversaries to launch a
diversified set of adversarial attacks on these algorithms in different
applications of social media text processing. In this paper, we provide a
comprehensive review of the main approaches for adversarial attacks and
defenses in the context of social media applications with a particular focus on
key challenges and future research directions. In detail, we cover literature
on six key applications, namely (i) rumors detection, (ii) satires detection,
(iii) clickbait & spams identification, (iv) hate speech detection,
(v)misinformation detection, and (vi) sentiment analysis. We then highlight the
concurrent and anticipated future research questions and provide
recommendations and directions for future work.

    ", Computation and Language,natural language processing,"Adversarial Attacks and Defenses for Social Network Text Processing Applications: Techniques, Challenges and Future Research Directions"
"  Learning to execute algorithms is a fundamental problem that has been widely
studied. Prior work~\cite{veli19neural} has shown that to enable systematic
generalisation on graph algorithms it is critical to have access to the
intermediate steps of the program/algorithm. In many reasoning tasks, where
algorithmic-style reasoning is important, we only have access to the input and
output examples. Thus, inspired by the success of pre-training on similar tasks
or data in Natural Language Processing (NLP) and Computer Vision, we set out to
study how we can transfer algorithmic reasoning knowledge. Specifically, we
investigate how we can use algorithms for which we have access to the execution
trace to learn to solve similar tasks for which we do not. We investigate two
major classes of graph algorithms, parallel algorithms such as breadth-first
search and Bellman-Ford and sequential greedy algorithms such as Prim and
Dijkstra. Due to the fundamental differences between algorithmic reasoning
knowledge and feature extractors such as used in Computer Vision or NLP, we
hypothesise that standard transfer techniques will not be sufficient to achieve
systematic generalisation. To investigate this empirically we create a dataset
including 9 algorithms and 3 different graph types. We validate this
empirically and show how instead multi-task learning can be used to achieve the
transfer of algorithmic reasoning knowledge.

    ", Machine Learning,natural language processing,How to transfer algorithmic reasoning knowledge to learn new algorithms?
"  Continuously learning new tasks using high-level ideas or knowledge is a key
capability of humans. In this paper, we propose Lifelong reinforcement learning
with Sequential linear temporal logic formulas and Reward Machines (LSRM),
which enables an agent to leverage previously learned knowledge to fasten
learning of logically specified tasks. For the sake of more flexible
specification of tasks, we first introduce Sequential Linear Temporal Logic
(SLTL), which is a supplement to the existing Linear Temporal Logic (LTL)
formal language. We then utilize Reward Machines (RM) to exploit structural
reward functions for tasks encoded with high-level events, and propose
automatic extension of RM and efficient knowledge transfer over tasks for
continuous learning in lifetime. Experimental results show that LSRM
outperforms the methods that learn the target tasks from scratch by taking
advantage of the task decomposition using SLTL and knowledge transfer over RM
during the lifelong learning process.

    ", Artificial Intelligence,reinforcement learning,Lifelong Reinforcement Learning with Temporal Logic Formulas and Reward Machines
"  Current state-of-the-art research for tackling the problem of malware
detection and classification is centered on the design, implementation and
deployment of systems powered by machine learning because of its ability to
generalize to never-before-seen malware families and polymorphic mutations.
However, it has been shown that machine learning models, in particular deep
neural networks, lack robustness against crafted inputs (adversarial examples).
In this work, we have investigated the vulnerability of a state-of-the-art
shallow convolutional neural network malware classifier against the dead code
insertion technique. We propose a general framework powered by a Double
Q-network to induce misclassification over malware families. The framework
trains an agent through a convolutional neural network to select the optimal
positions in a code sequence to insert dead code instructions so that the
machine learning classifier mislabels the resulting executable. The experiments
show that the proposed method significantly drops the classification accuracy
of the classifier to 56.53% while having an evasion rate of 100% for the
samples belonging to the Kelihos_ver3, Simda, and Kelihos_ver1 families. In
addition, the average number of instructions needed to mislabel malware in
comparison to a random agent decreased by 33%.

    ", Cryptography and Security,reinforcement learning,Enhancing the Insertion of NOP Instructions to Obfuscate Malware via Deep Reinforcement Learning
"  The study of generalisation in deep Reinforcement Learning (RL) aims to
produce RL algorithms whose policies generalise well to novel unseen situations
at deployment time, avoiding overfitting to their training environments.
Tackling this is vital if we are to deploy reinforcement learning algorithms in
real world scenarios, where the environment will be diverse, dynamic and
unpredictable. This survey is an overview of this nascent field. We provide a
unifying formalism and terminology for discussing different generalisation
problems, building upon previous works. We go on to categorise existing
benchmarks for generalisation, as well as current methods for tackling the
generalisation problem. Finally, we provide a critical discussion of the
current state of the field, including recommendations for future work. Among
other conclusions, we argue that taking a purely procedural content generation
approach to benchmark design is not conducive to progress in generalisation, we
suggest fast online adaptation and tackling RL-specific problems as some areas
for future work on methods for generalisation, and we recommend building
benchmarks in underexplored problem settings such as offline RL generalisation
and reward-function variation.

    ", Machine Learning,reinforcement learning,A Survey of Generalisation in Deep Reinforcement Learning
"  In 2021 the Johns Hopkins University Applied Physics Laboratory held an
internal challenge to develop artificially intelligent (AI) agents that could
excel at the collaborative card game Hanabi. Agents were evaluated on their
ability to play with human players whom the agents had never previously
encountered. This study details the development of the agent that won the
challenge by achieving a human-play average score of 16.5, outperforming the
current state-of-the-art for human-bot Hanabi scores. The winning agent's
development consisted of observing and accurately modeling the author's
decision making in Hanabi, then training with a behavioral clone of the author.
Notably, the agent discovered a human-complementary play style by first
mimicking human decision making, then exploring variations to the human-like
strategy that led to higher simulated human-bot scores. This work examines in
detail the design and implementation of this human compatible Hanabi teammate,
as well as the existence and implications of human-complementary strategies and
how they may be explored for more successful applications of AI in human
machine teams.

    ", Artificial Intelligence,reinforcement learning,Reinforcement Learning on Human Decision Models for Uniquely Collaborative AI Teammates
"  Operating in the real-world often requires agents to learn about a complex
environment and apply this understanding to achieve a breadth of goals. This
problem, known as goal-conditioned reinforcement learning (GCRL), becomes
especially challenging for long-horizon goals. Current methods have tackled
this problem by augmenting goal-conditioned policies with graph-based planning
algorithms. However, they struggle to scale to large, high-dimensional state
spaces and assume access to exploration mechanisms for efficiently collecting
training data. In this work, we introduce Successor Feature Landmarks (SFL), a
framework for exploring large, high-dimensional environments so as to obtain a
policy that is proficient for any goal. SFL leverages the ability of successor
features (SF) to capture transition dynamics, using it to drive exploration by
estimating state-novelty and to enable high-level planning by abstracting the
state-space as a non-parametric landmark-based graph. We further exploit SF to
directly compute a goal-conditioned policy for inter-landmark traversal, which
we use to execute plans to ""frontier"" landmarks at the edge of the explored
state space. We show in our experiments on MiniGrid and ViZDoom that SFL
enables efficient exploration of large, high-dimensional state spaces and
outperforms state-of-the-art baselines on long-horizon GCRL tasks.

    ", Machine Learning,reinforcement learning,Successor Feature Landmarks for Long-Horizon Goal-Conditioned Reinforcement Learning
"  Dynamic quadruped locomotion over challenging terrains with precise foot
placements is a hard problem for both optimal control methods and Reinforcement
Learning (RL). Non-linear solvers can produce coordinated constraint satisfying
motions, but often take too long to converge for online application. RL methods
can learn dynamic reactive controllers but require carefully tuned shaping
rewards to produce good gaits and can have trouble discovering precise
coordinated movements. Imitation learning circumvents this problem and has been
used with motion capture data to extract quadruped gaits for flat terrains.
However, it would be costly to acquire motion capture data for a very large
variety of terrains with height differences. In this work, we combine the
advantages of trajectory optimization and learning methods and show that
terrain adaptive controllers can be obtained by training policies to imitate
trajectories that have been planned over procedural terrains by a non-linear
solver. We show that the learned policies transfer to unseen terrains and can
be fine-tuned to dynamically traverse challenging terrains that require precise
foot placements and are very hard to solve with standard RL.

    ", Robotics,reinforcement learning,Learning Coordinated Terrain-Adaptive Locomotion by Imitating a Centroidal Dynamics Planner
"  Meta-learners and ensembles aim to combine a set of relevant yet diverse base
models to improve predictive performance. However, determining an appropriate
set of base models is challenging, especially in online environments where the
underlying distribution of data can change over time. In this paper, we present
a novel approach for estimating the conceptual similarity of base models, which
is calculated using the Principal Angles (PAs) between their underlying
subspaces. We propose two methods that use conceptual similarity as a metric to
obtain a relevant yet diverse subset of base models: (i) parameterised
threshold culling and (ii) parameterless conceptual clustering. We evaluate
these methods against thresholding using common ensemble pruning metrics,
namely predictive performance and Mutual Information (MI), in the context of
online Transfer Learning (TL), using both synthetic and real-world data. Our
results show that conceptual similarity thresholding has a reduced
computational overhead, and yet yields comparable predictive performance to
thresholding using predictive performance and MI. Furthermore, conceptual
clustering achieves similar predictive performances without requiring
parameterisation, and achieves this with lower computational overhead than
thresholding using predictive performance and MI when the number of base models
becomes large.

    ", Machine Learning,clustering,Conceptually Diverse Base Model Selection for Meta-Learners in Concept Drifting Data Streams
"  We address the problem of estimating the poses of multiple instances of the
source point cloud within a target point cloud. Existing solutions require
sampling a lot of hypotheses to detect possible instances and reject the
outliers, whose robustness and efficiency degrade notably when the number of
instances and outliers increase. We propose to directly group the set of noisy
correspondences into different clusters based on a distance invariance matrix.
The instances and outliers are automatically identified through clustering. Our
method is robust and fast. We evaluated our method on both synthetic and
real-world datasets. The results show that our approach can correctly register
up to 20 instances with an F1 score of 90.46% in the presence of 70% outliers,
which performs significantly better and at least 10x faster than existing
methods

    ", Computer Vision and Pattern Recognition,clustering,Multi-instance Point Cloud Registration by Efficient Correspondence Clustering
"  Context: The establishment of the Mining Software Repositories (MSR) data
showcase conference track has encouraged researchers to provide data sets as a
basis for further empirical studies. Objective: Examine the usage of data
papers published in the MSR proceedings in terms of use frequency, users, and
use purpose. Method: Data track papers were collected from the MSR data
showcase track and through the manual inspection of older MSR proceedings. The
use of data papers was established through manual citation searching followed
by reading the citing studies and dividing them into strong and weak citations.
Contrary to weak, strong citations truly use the data set of a data paper. Data
papers were then manually clustered based on their content, whereas their
strong citations were classified by hand according to the knowledge areas of
the Guide to the Software Engineering Body of Knowledge. A survey study on 108
authors and users of data papers provided further insights regarding motivation
and effort in data paper production, encouraging and discouraging factors in
data set use, and future desired direction regarding data papers. Results: We
found that 65% of the data papers have been used in other studies, with a
long-tail distribution in the number of strong citations. Weak citations to
data papers usually refer to them as an example. MSR data papers are cited in
total less than other MSR papers. A considerable number of the strong citations
stem from the teams that authored the data papers. Publications providing
Version Control System (VCS) primary and derived data are the most frequent
data papers and the most often strongly cited ones. Enhanced developer data
papers are the least common ones, and the second least frequently strongly
cited. Data paper authors tend to gather data in the context of other research.
[...]

    ", Software Engineering,clustering,Standing on Shoulders or Feet? An Extended Study on the Usage of the MSR Data Papers
"  In this article, we study the transport properties of superconcentrated
electrolytes using Molecular Dynamics simulations, which have been shown
experimentally to retard elemental dissolution in vanadium containing cathode
materials. Five compositions between one and seven molar lithium
bis(trifluoromethanesulfonyl)imide in 1,3-Dioxolane and 1,2-Dimethoxyethane
solvent mixture are studied using non-polarizable Optimized Potentials for
Liquid Simulations - All Atom force field. The simulated physico-chemical
properties such as ionic conductivity, self-diffusion coefficients, and density
are observed to match well with the results obtained through experiments.
Radial Distribution Function analysis reveals a strong co-ordination between
salt anions and vanadium cations as the electrolyte transitions from a
salt-in-solvent type to solvent-in-salt type electrolyte. A high anion content
in the first solvation shell of vanadium cations is observed for
solvent-in-salt type electrolytes, through ion-clustering calculations.
Solvation free energy calculations using Free Energy Perturbation method
indicate that the active material dissolution should be retarded by using
superconcentrated electrolytes. Ion-dynamics of the clusters reveal that
vanadium cation transport occurs against its concentration gradient due to
strong coulombic interactions with the salt anions in superconcentrated
electrolytes. The improvement in the cycleability of several vanadium
containing cathode materials provides a robust proof for the theoretical
framework described in this manuscript.

    ", Materials Science,clustering,"Effect of salt concentration on the solubility, ion-dynamics, and transport properties of dissolved vanadium ions in lithium-ion battery electrolytes: Generalized solubility limit approach (Part II)"
"  Spanners have been shown to be a powerful tool in graph algorithms. Many
spanner constructions use a certain type of clustering at their core, where
each cluster has small diameter and there are relatively few spanner edges
between clusters. In this paper, we provide a clustering algorithm that, given
$k\geq 2$, can be used to compute a spanner of stretch $2k-1$ and expected size
$O(n^{1+1/k})$ in $k$ rounds in the CONGEST model. This improves upon the state
of the art (by Elkin, and Neiman [TALG'19]) by making the bounds on both
running time and stretch independent of the random choices of the algorithm,
whereas they only hold with high probability in previous results. Spanners are
used in certain synchronizers, thus our improvement directly carries over to
such synchronizers. Furthermore, for keeping the \emph{total} number of
inter-cluster edges small in low diameter decompositions, our clustering
algorithm provides the following guarantees. Given $\beta\in (0,1]$, we compute
a low diameter decomposition with diameter bound $O\left(\frac{\log
n}{\beta}\right)$ such that each edge $e\in E$ is an inter-cluster edge with
probability at most $\beta\cdot w(e)$ in $O\left(\frac{\log n}{\beta}\right)$
rounds in the CONGEST model. Again, this improves upon the state of the art (by
Miller, Peng, and Xu [SPAA'13]) by making the bounds on both running time and
diameter independent of the random choices of the algorithm, whereas they only
hold with high probability in previous results.

    ", Data Structures and Algorithms,clustering,An Improved Random Shift Algorithm for Spanners and Low Diameter Decompositions
"  The goal of continuous control is to synthesize desired behaviors. In
reinforcement learning (RL)-driven approaches, this is often accomplished
through careful task reward engineering for efficient exploration and running
an off-the-shelf RL algorithm. While reward maximization is at the core of RL,
reward engineering is not the only -- sometimes nor the easiest -- way for
specifying complex behaviors. In this paper, we introduce \braxlines, a toolkit
for fast and interactive RL-driven behavior generation beyond simple reward
maximization that includes Composer, a programmatic API for generating
continuous control environments, and set of stable and well-tested baselines
for two families of algorithms -- mutual information maximization (MiMax) and
divergence minimization (DMin) -- supporting unsupervised skill learning and
distribution sketching as other modes of behavior specification. In addition,
we discuss how to standardize metrics for evaluating these algorithms, which
can no longer rely on simple reward maximization. Our implementations build on
a hardware-accelerated Brax simulator in Jax with minimal modifications,
enabling behavior synthesis within minutes of training. We hope Braxlines can
serve as an interactive toolkit for rapid creation and testing of environments
and behaviors, empowering explosions of future benchmark designs and new modes
of RL-driven behavior generation and their algorithmic research.

    ", Machine Learning,unsupervised learning,Braxlines: Fast and Interactive Toolkit for RL-driven Behavior Engineering beyond Reward Maximization
"  Many methods have been proposed over the years to tackle the task of facial
3D geometry and texture recovery from a single image. Such methods often fail
to provide high-fidelity texture without relying on 3D facial scans during
training. In contrast, the complementary task of 3D facial generation has not
received as much attention. As opposed to the 2D texture domain, where GANs
have proven to produce highly realistic facial images, the more challenging 3D
geometry domain has not yet caught up to the same levels of realism and
diversity.
", Computer Vision and Pattern Recognition,unsupervised learning,Unsupervised High-Fidelity Facial Texture Generation and Reconstruction
"  Unsupervised visual representation learning has gained much attention from
the computer vision community because of the recent achievement of contrastive
learning. Most of the existing contrastive learning frameworks adopt the
instance discrimination as the pretext task, which treating every single
instance as a different class. However, such method will inevitably cause class
collision problems, which hurts the quality of the learned representation.
Motivated by this observation, we introduced a weakly supervised contrastive
learning framework (WCL) to tackle this issue. Specifically, our proposed
framework is based on two projection heads, one of which will perform the
regular instance discrimination task. The other head will use a graph-based
method to explore similar samples and generate a weak label, then perform a
supervised contrastive learning task based on the weak label to pull the
similar images closer. We further introduced a K-Nearest Neighbor based
multi-crop strategy to expand the number of positive samples. Extensive
experimental results demonstrate WCL improves the quality of self-supervised
representations across different datasets. Notably, we get a new
state-of-the-art result for semi-supervised learning. With only 1\% and 10\%
labeled examples, WCL achieves 65\% and 72\% ImageNet Top-1 Accuracy using
ResNet50, which is even higher than SimCLRv2 with ResNet101.

    ", Computer Vision and Pattern Recognition,unsupervised learning,Weakly Supervised Contrastive Learning
"  Traditional methods for unsupervised learning of finite mixture models
require to evaluate the likelihood of all components of the mixture. This
becomes computationally prohibitive when the number of components is large, as
it is, for example, in the sum-product (transform) networks. Therefore, we
propose to apply a combination of the expectation maximization and the
Metropolis-Hastings algorithm to evaluate only a small number of,
stochastically sampled, components, thus substantially reducing the
computational cost. The Markov chain of component assignments is sequentially
generated across the algorithm's iterations, having a non-stationary target
distribution whose parameters vary via a gradient-descent scheme. We put
emphasis on generality of our method, equipping it with the ability to train
both shallow and deep mixture models which involve complex, and possibly
nonlinear, transformations. The performance of our method is illustrated in a
variety of synthetic and real-data contexts, considering deep models, such as
mixtures of normalizing flows and sum-product (transform) networks.

    ", Machine Learning,unsupervised learning,Fitting large mixture models using stochastic component selection
"  Unsupervised domain adaptation (UDA) aims to bridge the domain shift between
the labeled source domain and the unlabeled target domain. However, most
existing works perform the global-level feature alignment for semantic
segmentation, while the local consistency between the regions has been largely
neglected, and these methods are less robust to changing of outdoor
environments. Motivated by the above facts, we propose a novel and fully
end-to-end trainable approach, called regional contrastive consistency
regularization (RCCR) for domain adaptive semantic segmentation. Our core idea
is to pull the similar regional features extracted from the same location of
different images to be closer, and meanwhile push the features from the
different locations of the two images to be separated. We innovatively propose
momentum projector heads, where the teacher projector is the exponential moving
average of the student. Besides, we present a region-wise contrastive loss with
two sampling strategies to realize effective regional consistency. Finally, a
memory bank mechanism is designed to learn more robust and stable region-wise
features under varying environments. Extensive experiments on two common UDA
benchmarks, i.e., GTAV to Cityscapes and SYNTHIA to Cityscapes, demonstrate
that our approach outperforms the state-of-the-art methods.

    ", Computer Vision and Pattern Recognition,unsupervised learning,Domain Adaptive Semantic Segmentation with Regional Contrastive Consistency Regularization
"  Given a node-attributed graph, how can we efficiently represent it with few
numerical features that expressively reflect its topology and attribute
information? We propose A-DOGE, for Attributed DOS-based Graph Embedding, based
on density of states (DOS, a.k.a. spectral density) to tackle this problem.
A-DOGE is designed to fulfill a long desiderata of desirable characteristics.
Most notably, it capitalizes on efficient approximation algorithms for DOS,
that we extend to blend in node labels and attributes for the first time,
making it fast and scalable for large attributed graphs and graph databases.
Being based on the entire eigenspectrum of a graph, A-DOGE can capture
structural and attribute properties at multiple (""glocal"") scales. Moreover, it
is unsupervised (i.e. agnostic to any specific objective) and lends itself to
various interpretations, which makes it is suitable for exploratory graph
mining tasks. Finally, it processes each graph independent of others, making it
amenable for streaming settings as well as parallelization. Through extensive
experiments, we show the efficacy and efficiency of A-DOGE on exploratory graph
analysis and graph classification tasks, where it significantly outperforms
unsupervised baselines and achieves competitive performance with modern
supervised GNNs, while achieving the best trade-off between accuracy and
runtime.

    ", Machine Learning,unsupervised learning,Fast Attributed Graph Embedding via Density of States
"  In recent years the amounts of personal photos captured increased
significantly, giving rise to new challenges in multi-image understanding and
high-level image understanding. Event recognition in personal photo albums
presents one challenging scenario where life events are recognized from a
disordered collection of images, including both relevant and irrelevant images.
Event recognition in images also presents the challenge of high-level image
understanding, as opposed to low-level image object classification. In absence
of methods to analyze multiple inputs, previous methods adopted temporal
mechanisms, including various forms of recurrent neural networks. However,
their effective temporal window is local. In addition, they are not a natural
choice given the disordered characteristic of photo albums. We address this gap
with a tailor-made solution, combining the power of CNNs for image
representation and transformers for album representation to perform global
reasoning on image collection, offering a practical and efficient solution for
photo albums event recognition. Our solution reaches state-of-the-art results
on 3 prominent benchmarks, achieving above 90\% mAP on all datasets. We further
explore the related image-importance task in event recognition, demonstrating
how the learned attentions correlate with the human-annotated importance for
this subjective task, thus opening the door for new applications.

    ", Computer Vision and Pattern Recognition,recurrent neural networks,PETA: Photo Albums Event Recognition using Transformers Attention
"  We previously established a large lung sound database, HF_Lung_V2 (Lung_V2).
We trained convolutional-bidirectional gated recurrent unit (CNN-BiGRU)
networks for detecting inhalation, exhalation, continuous adventitious sound
(CAS) and discontinuous adventitious sound at the recording level on the basis
of Lung_V2. However, the performance of CAS detection was poor due to many
reasons, one of which is the highly diversified CAS patterns. To make the
original CNN-BiGRU model learn the CAS patterns more effectively and not cause
too much computing burden, three strategies involving minimal modifications of
the network architecture of the CNN layers were investigated: (1) making the
CNN layers a bit deeper by using the residual blocks, (2) making the CNN layers
a bit wider by increasing the number of CNN kernels, and (3) separating the
feature input into multiple paths (the model was denoted by Multi-path
CNN-BiGRU). The performance of CAS segment and event detection were evaluated.
Results showed that improvement in CAS detection was observed among all the
proposed architecture-modified models. The F1 score for CAS event detection of
the proposed models increased from 0.445 to 0.491-0.530, which was deemed
significant. However, the Multi-path CNN-BiGRU model outperformed the other
models in terms of the number of winning titles (five) in total nine evaluation
metrics. In addition, the Multi-path CNN-BiGRU model did not cause extra
computing burden (0.97-fold inference time) compared to the original CNN-BiGRU
model. Conclusively, the Multi-path CNN layers can efficiently improve the
effectiveness of feature extraction and subsequently result in better CAS
detection.

    ", Sound,recurrent neural networks,Multi-path Convolutional Neural Networks Efficiently Improve Feature Extraction in Continuous Adventitious Lung Sound Detection
"  Concept learning approaches based on refinement operators explore partially
ordered solution spaces to compute concepts, which are used as binary
classification models for individuals. However, the refinement trees spanned by
these approaches can easily grow to millions of nodes for complex learning
problems. This leads to refinement-based approaches often failing to detect
optimal concepts efficiently. In this paper, we propose a supervised machine
learning approach for learning concept lengths, which allows predicting the
length of the target concept and therefore facilitates the reduction of the
search space during concept learning. To achieve this goal, we compare four
neural architectures and evaluate them on four benchmark knowledge
graphs--Carcinogenesis, Mutagenesis, Semantic Bible, Family Benchmark. Our
evaluation results suggest that recurrent neural network architectures perform
best at concept length prediction with an F-measure of up to 92%. We show that
integrating our concept length predictor into the CELOE (Class Expression
Learner for Ontology Engineering) algorithm improves CELOE's runtime by a
factor of up to 13.4 without any significant changes to the quality of the
results it generates. For reproducibility, we provide our implementation in the
public GitHub repository at
", Machine Learning,recurrent neural networks,Prediction of concept lengths for fast concept learning in description logics
"  The metro ridership prediction has always received extensive attention from
governments and researchers. Recent works focus on designing complicated graph
convolutional recurrent network architectures to capture spatial and temporal
patterns. These works extract the information of spatial dimension well, but
the limitation of temporal dimension still exists. We extended Neural ODE
algorithms to the graph network and proposed the STR-GODEs network, which can
effectively learn spatial, temporal, and ridership correlations without the
limitation of dividing data into equal-sized intervals on the timeline. While
learning the spatial relations and the temporal correlations, we modify the
GODE-RNN cell to obtain the ridership feature and hidden states. Ridership
information and its hidden states are added to the GODESolve to reduce the
error accumulation caused by long time series in prediction. Extensive
experiments on two large-scale datasets demonstrate the efficacy and robustness
of our model.

    ", Machine Learning,recurrent neural networks,STR-GODEs: Spatial-Temporal-Ridership Graph ODEs for Metro Ridership Prediction
"  Video predictive understanding encompasses a wide range of efforts that are
concerned with the anticipation of the unobserved future from the current as
well as historical video observations. Action prediction is a major sub-area of
video predictive understanding and is the focus of this review. This sub-area
has two major subdivisions: early action recognition and future action
prediction. Early action recognition is concerned with recognizing an ongoing
action as soon as possible. Future action prediction is concerned with the
anticipation of actions that follow those previously observed. In either case,
the \textbf{\textit{causal}} relationship between the past, current, and
potential future information is the main focus. Various mathematical tools such
as Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian
recursive filtering are widely adopted jointly with computer vision techniques
for these two tasks. However, these approaches face challenges such as the
curse of dimensionality, poor generalization, and constraints from
domain-specific knowledge. Recently, structures that rely on deep convolutional
neural networks and recurrent neural networks have been extensively proposed
for improving the performance of existing vision tasks, in general, and action
prediction tasks, in particular. However, they have their own shortcomings, \eg
reliance on massive training data and lack of strong theoretical underpinnings.
In this survey, we start by introducing the major sub-areas of the broad area
of video predictive understanding, which recently have received intensive
attention and proven to have practical value. Next, a thorough review of
various early action recognition and future action prediction algorithms are
provided with suitably organized divisions. Finally, we conclude our discussion
with future research directions.

    ", Computer Vision and Pattern Recognition,recurrent neural networks,Review of Video Predictive Understanding: Early Action Recognition and Future Action Prediction
"  With the sweeping digitalization of societal, medical, industrial, and
scientific processes, sensing technologies are being deployed that produce
increasing volumes of time series data, thus fueling a plethora of new or
improved applications. In this setting, outlier detection is frequently
important, and while solutions based on neural networks exist, they leave room
for improvement in terms of both accuracy and efficiency. With the objective of
achieving such improvements, we propose a diversity-driven, convolutional
ensemble. To improve accuracy, the ensemble employs multiple basic outlier
detection models built on convolutional sequence-to-sequence autoencoders that
can capture temporal dependencies in time series. Further, a novel
diversity-driven training method maintains diversity among the basic models,
with the aim of improving the ensemble's accuracy. To improve efficiency, the
approach enables a high degree of parallelism during training. In addition, it
is able to transfer some model parameters from one basic model to another,
which reduces training time. We report on extensive experiments using
real-world multivariate time series that offer insight into the design choices
underlying the new approach and offer evidence that it is capable of improved
accuracy and efficiency. This is an extended version of ""Unsupervised Time
Series Outlier Detection with Diversity-Driven Convolutional Ensembles"", to
appear in PVLDB 2022.

    ", Machine Learning,convolutional neural networks,Unsupervised Time Series Outlier Detection with Diversity-Driven Convolutional Ensembles -- Extended Version
"  In this paper, we present a novel deep neural network architecture for joint
class-agnostic object segmentation and grasp detection for robotic picking
tasks using a parallel-plate gripper. We introduce depth-aware Coordinate
Convolution (CoordConv), a method to increase accuracy for point proposal based
object instance segmentation in complex scenes without adding any additional
network parameters or computation complexity. Depth-aware CoordConv uses depth
data to extract prior information about the location of an object to achieve
highly accurate object instance segmentation. These resulting segmentation
masks, combined with predicted grasp candidates, lead to a complete scene
description for grasping using a parallel-plate gripper. We evaluate the
accuracy of grasp detection and instance segmentation on challenging robotic
picking datasets, namely Sil√©ane and OCID_grasp, and show the benefit of
joint grasp detection and segmentation on a real-world robotic picking task.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,Depth-aware Object Segmentation and Grasp Detection for Robotic Picking Tasks
"  Recent work in deep learning focuses on solving physical systems in the
Ordinary Differential Equation or Partial Differential Equation. This current
work proposed a variant of Convolutional Neural Networks (CNNs) that can learn
the hidden dynamics of a physical system using ordinary differential equation
(ODEs) systems (ODEs) and Partial Differential Equation systems (PDEs). Instead
of considering the physical system such as image, time -series as a system of
multiple layers, this new technique can model a system in the form of
Differential Equation (DEs). The proposed method has been assessed by solving
several steady-state PDEs on irregular domains, including heat equations,
Navier-Stokes equations.

    ", Machine Learning,convolutional neural networks,Continuous Convolutional Neural Networks: Coupled Neural PDE and ODE
"  Heart rate and respiratory rate measurement is a vital step for diagnosing
many diseases. Non-contact camera based physiological measurement is more
accessible and convenient in Telehealth nowadays than contact instruments such
as fingertip oximeters since non-contact methods reduce risk of infection.
However, remote physiological signal measurement is challenging due to
environment illumination variations, head motion, facial expression, etc. It's
also desirable to have a unified network which could estimate both heart rate
and respiratory rate to reduce system complexity and latency. We propose a
convolutional neural network which leverages spatial attention and channel
attention, which we call it dual attention network (DAN) to jointly estimate
heart rate and respiratory rate with camera video as input. Extensive
experiments demonstrate that our proposed system significantly improves heart
rate and respiratory rate measurement accuracy.

    ", Image and Video Processing,convolutional neural networks,Dual Attention Network for Heart Rate and Respiratory Rate Estimation
"  Particle image velocimetry (PIV) is essential in experimental fluid dynamics.
In the current work, we propose a new velocity field estimation paradigm, which
achieves a synergetic combination of the deep learning method and the
traditional cross-correlation method. Specifically, the deep learning method is
used to optimize and correct a coarse velocity guess to achieve a
super-resolution calculation. And the cross-correlation method provides the
initial velocity field based on a coarse correlation with a large interrogation
window. As a reference, the coarse velocity guess helps with improving the
robustness of the proposed algorithm. This fully convolutional network with
embedded cross-correlation is named as CC-FCN. CC-FCN has two types of input
layers, one is for the particle images, and the other is for the initial
velocity field calculated using cross-correlation with a coarse resolution.
Firstly, two pyramidal modules extract features of particle images and initial
velocity field respectively. Then the fusion module appropriately fuses these
features. Finally, CC-FCN achieves the super-resolution calculation through a
series of deconvolution layers to obtain the single-pixel velocity field. As
the supervised learning strategy is considered, synthetic data sets including
ground-truth fluid motions are generated to train the network parameters.
Synthetic and real experimental PIV data sets are used to test the trained
neural network in terms of accuracy, precision, spatial resolution and
robustness. The test results show that these attributes of CC-FCN are further
improved compared with those of other tested PIV algorithms. The proposed model
could therefore provide competitive and robust estimations for PIV experiments.

    ", Fluid Dynamics,convolutional neural networks,A robust single-pixel particle image velocimetry based on fully convolutional networks with cross-correlation embedded
"  Although speech recognition has become a widespread technology, inferring
emotion from speech signals still remains a challenge. To address this problem,
this paper proposes a quaternion convolutional neural network (QCNN) based
speech emotion recognition (SER) model in which Mel-spectrogram features of
speech signals are encoded in an RGB quaternion domain. We show that our QCNN
based SER model outperforms other real-valued methods in the Ryerson
Audio-Visual Database of Emotional Speech and Song (RAVDESS, 8-classes)
dataset, achieving, to the best of our knowledge, state-of-the-art results. The
QCNN also achieves comparable results with the state-of-the-art methods in the
Interactive Emotional Dyadic Motion Capture (IEMOCAP 4-classes) and Berlin
EMO-DB (7-classes) datasets. Specifically, the model achieves an accuracy of
77.87\%, 70.46\%, and 88.78\% for the RAVDESS, IEMOCAP, and EMO-DB datasets,
respectively. In addition, our results show that the quaternion unit structure
is better able to encode internal dependencies to reduce its model size
significantly compared to other methods.

    ", Sound,convolutional neural networks,Speech Emotion Recognition Using Quaternion Convolutional Neural Networks
"  Reactive synthesis algorithms allow automatic construction of policies to
control an environment modeled as a Markov Decision Process (MDP) that are
optimal with respect to high-level temporal logic specifications assuming the
MDP model is known a priori. Reinforcement learning algorithms, in contrast,
are designed to learn an optimal policy when the transition probabilities of
the MDP are unknown, but require the user to associate local rewards with
transitions. The appeal of high-level temporal logic specifications has
motivated research to develop RL algorithms for synthesis of policies from
specifications. To understand the techniques, and nuanced variations in their
theoretical guarantees, in the growing body of resulting literature, we develop
a formal framework for defining transformations among RL tasks with different
forms of objectives. We define the notion of sampling-based reduction to relate
two MDPs whose transition probabilities can be learnt by sampling, followed by
formalization of preservation of optimal policies, convergence, and robustness.
We then use our framework to restate known results, establish new results to
fill in some gaps, and identify open problems.

    ", Formal Languages and Automata Theory,reinforcement learning,A Framework for Transforming Specifications in Reinforcement Learning
"  Emergency vehicles (EMVs) play a critical role in a city's response to
time-critical events such as medical emergencies and fire outbreaks. The
existing approaches to reduce EMV travel time employ route optimization and
traffic signal pre-emption without accounting for the coupling between route
these two subproblems. As a result, the planned route often becomes suboptimal.
In addition, these approaches also do not focus on minimizing disruption to the
overall traffic flow. To address these issues, we introduce EMVLight in this
paper. This is a decentralized reinforcement learning (RL) framework for
simultaneous dynamic routing and traffic signal control. EMVLight extends
Dijkstra's algorithm to efficiently update the optimal route for an EMV in
real-time as it travels through the traffic network. Consequently, the
decentralized RL agents learn network-level cooperative traffic signal phase
strategies that reduce EMV travel time and the average travel time of non-EMVs
in the network. We have carried out comprehensive experiments with synthetic
and real-world maps to demonstrate this benefit. Our results show that EMVLight
outperforms benchmark transportation engineering techniques as well as existing
RL-based traffic signal control methods.

    ", Artificial Intelligence,reinforcement learning,A Decentralized Reinforcement Learning Framework for Efficient Passage of Emergency Vehicles
"  We study automated intrusion prevention using reinforcement learning.
Following a novel approach, we formulate the problem of intrusion prevention as
an (optimal) multiple stopping problem. This formulation gives us insight into
the structure of optimal policies, which we show to have threshold properties.
For most practical cases, it is not feasible to obtain an optimal defender
policy using dynamic programming. We therefore develop a reinforcement learning
approach to approximate an optimal policy. Our method for learning and
validating policies includes two systems: a simulation system where defender
policies are incrementally learned and an emulation system where statistics are
produced that drive simulation runs and where learned policies are evaluated.
We show that our approach can produce effective defender policies for a
practical IT infrastructure of limited size. Inspection of the learned policies
confirms that they exhibit threshold properties.

    ", Machine Learning,reinforcement learning,Intrusion Prevention through Optimal Stopping
"  In the last decade, there have been significant advances in multi-agent
reinforcement learning (MARL) but there are still numerous challenges, such as
high sample complexity and slow convergence to stable policies, that need to be
overcome before wide-spread deployment is possible. However, many real-world
environments already, in practice, deploy sub-optimal or heuristic approaches
for generating policies. An interesting question which arises is how to best
use such approaches as advisors to help improve reinforcement learning in
multi-agent domains. In this paper, we provide a principled framework for
incorporating action recommendations from online sub-optimal advisors in
multi-agent settings. We describe the problem of ADvising Multiple Intelligent
Reinforcement Agents (ADMIRAL) in nonrestrictive general-sum stochastic game
environments and present two novel Q-learning based algorithms: ADMIRAL -
Decision Making (ADMIRAL-DM) and ADMIRAL - Advisor Evaluation (ADMIRAL-AE),
which allow us to improve learning by appropriately incorporating advice from
an advisor (ADMIRAL-DM), and evaluate the effectiveness of an advisor
(ADMIRAL-AE). We analyze the algorithms theoretically and provide fixed-point
guarantees regarding their learning in general-sum stochastic games.
Furthermore, extensive experiments illustrate that these algorithms: can be
used in a variety of environments, have performances that compare favourably to
other related baselines, can scale to large state-action spaces, and are robust
to poor advice from advisors.

    ", Artificial Intelligence,reinforcement learning,Multi-Agent Advisor Q-Learning
"  We focus on a simulation-based optimization problem of choosing the best
design from the feasible space. Although the simulation model can be queried
with finite samples, its internal processing rule cannot be utilized in the
optimization process. We formulate the sampling process as a policy searching
problem and give a solution from the perspective of Reinforcement Learning
(RL). Concretely, Actor-Critic (AC) framework is applied, where the Actor
serves as a surrogate model to predict the performance on unknown designs,
whereas the actor encodes the sampling policy to be optimized. We design the
updating rule and propose two algorithms for the cases where the feasible
spaces are continuous and discrete respectively. Some experiments are designed
to validate the effectiveness of proposed algorithms, including two toy
examples, which intuitively explain the algorithms, and two more complex tasks,
i.e., adversarial attack task and RL task, which validate the effectiveness in
large-scale problems. The results show that the proposed algorithms can
successfully deal with these problems. Especially note that in the RL task, our
methods give a new perspective to robot control by treating the task as a
simulation model and solving it by optimizing the policy generating process,
while existing works commonly optimize the policy itself directly.

    ", Machine Learning,reinforcement learning,An Actor-Critic Method for Simulation-Based Optimization
"  We discuss the problem of decentralized multi-agent reinforcement learning
(MARL) in this work. In our setting, the global state, action, and reward are
assumed to be fully observable, while the local policy is protected as privacy
by each agent, and thus cannot be shared with others. There is a communication
graph, among which the agents can exchange information with their neighbors.
The agents make individual decisions and cooperate to reach a higher
accumulated reward.
", Multiagent Systems,reinforcement learning,Decentralized Multi-Agent Reinforcement Learning: An Off-Policy Method
"  Natural language instructions for visual navigation often use scene
descriptions (e.g., ""bedroom"") and object references (e.g., ""green chairs"") to
provide a breadcrumb trail to a goal location. This work presents a
transformer-based vision-and-language navigation (VLN) agent that uses two
different visual encoders -- a scene classification network and an object
detector -- which produce features that match these two distinct types of
visual cues. In our method, scene features contribute high-level contextual
information that supports object-level processing. With this design, our model
is able to use vision-and-language pretraining (i.e., learning the alignment
between images and text from large-scale web data) to substantially improve
performance on the Room-to-Room (R2R) and Room-Across-Room (RxR) benchmarks.
Specifically, our approach leads to improvements of 1.8% absolute in SPL on R2R
and 3.7% absolute in SR on RxR. Our analysis reveals even larger gains for
navigation instructions that contain six or more object references, which
further suggests that our approach is better able to use object features and
align them to references in the instructions.

    ", Computer Vision and Pattern Recognition,natural language processing,SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation
"  As online false information continues to grow, automated fact-checking has
gained an increasing amount of attention in recent years. Researchers in the
field of Natural Language Processing (NLP) have contributed to the task by
building fact-checking datasets, devising automated fact-checking pipelines and
proposing NLP methods to further research in the development of different
components. This paper reviews relevant research on automated fact-checking
covering both the claim detection and claim validation components.

    ", Computation and Language,natural language processing,Automated Fact-Checking: A Survey
"  Clinical documentation can be transformed by Electronic Health Records, yet
the documentation process is still a tedious, time-consuming, and error-prone
process. Clinicians are faced with multi-faceted requirements and fragmented
interfaces for information exploration and documentation. These challenges are
only exacerbated in the Emergency Department -- clinicians often see 35
patients in one shift, during which they have to synthesize an often previously
unknown patient's medical records in order to reach a tailored diagnosis and
treatment plan. To better support this information synthesis, clinical
documentation tools must enable rapid contextual access to the patient's
medical record. MedKnowts is an integrated note-taking editor and information
retrieval system which unifies the documentation and search process and
provides concise synthesized concept-oriented slices of the patient's medical
record. MedKnowts automatically captures structured data while still allowing
users the flexibility of natural language. MedKnowts leverages this structure
to enable easier parsing of long notes, auto-populated text, and proactive
information retrieval, easing the documentation burden.

    ", Human-Computer Interaction,natural language processing,MedKnowts: Unified Documentation and Information Retrieval for Electronic Health Records
"  Generating fine-grained, realistic images from text has many applications in
the visual and semantic realm. Considering that, we propose Bangla Attentional
Generative Adversarial Network (AttnGAN) that allows intensified, multi-stage
processing for high-resolution Bangla text-to-image generation. Our model can
integrate the most specific details at different sub-regions of the image. We
distinctively concentrate on the relevant words in the natural language
description. This framework has achieved a better inception score on the CUB
dataset. For the first time, a fine-grained image is generated from Bangla text
using attentional GAN. Bangla has achieved 7th position among 100 most spoken
languages. This inspires us to explicitly focus on this language, which will
ensure the inevitable need of many people. Moreover, Bangla has a more complex
syntactic structure and less natural language processing resource that
validates our work more.

    ", Computer Vision and Pattern Recognition,natural language processing,Fine-Grained Image Generation from Bangla Text Description using Attentional Generative Adversarial Network
"  Visual question answering (VQA) has recently been introduced to remote
sensing to make information extraction from overhead imagery more accessible to
everyone. VQA considers a question (in natural language, therefore easy to
formulate) about an image and aims at providing an answer through a model based
on computer vision and natural language processing methods. As such, a VQA
model needs to jointly consider visual and textual features, which is
frequently done through a fusion step. In this work, we study three different
fusion methodologies in the context of VQA for remote sensing and analyse the
gains in accuracy with respect to the model complexity. Our findings indicate
that more complex fusion mechanisms yield an improved performance, yet that
seeking a trade-of between model complexity and performance is worthwhile in
practice.

    ", Computer Vision and Pattern Recognition,natural language processing,How to find a good image-text embedding for remote sensing visual question answering?
"  Estimation of semantic similarity is crucial for a variety of natural
language processing (NLP) tasks. In the absence of a general theory of semantic
information, many papers rely on human annotators as the source of ground truth
for semantic similarity estimation. This paper investigates the ambiguities
inherent in crowd-sourced semantic labeling. It shows that annotators that
treat semantic similarity as a binary category (two sentences are either
similar or not similar and there is no middle ground) play the most important
role in the labeling. The paper offers heuristics to filter out unreliable
annotators and stimulates further discussions on human perception of semantic
similarity.

    ", Computation and Language,natural language processing,Rethinking Crowd Sourcing for Semantic Similarity
"  The current state of adoption of well-structured electronic health records
and integration of digital methods for storing medical patient data in
structured formats can often considered as inferior compared to the use of
traditional, unstructured text based patient data documentation. Data mining in
the field of medical data analysis often needs to rely solely on processing of
unstructured data to retrieve relevant data. In natural language processing
(NLP), statistical models have been shown successful in various tasks like
part-of-speech tagging, relation extraction (RE) and named entity recognition
(NER). In this work, we present GERNERMED, the first open, neural NLP model for
NER tasks dedicated to detect medical entity types in German text data. Here,
we avoid the conflicting goals of protection of sensitive patient data from
training data extraction and the publication of the statistical model weights
by training our model on a custom dataset that was translated from publicly
available datasets in foreign language by a pretrained neural machine
translation model. The sample code and the statistical model is available at:
", Computation and Language,natural language processing,GERNERMED -- An Open German Medical NER Model
"  The evolution of high-performance computing is associated with the growth of
energy consumption. Performance of cluster computes (is increased via rising in
performance and the number of used processors, GPUs, and coprocessors. An
increment in the number of computing elements results in significant growth of
energy consumption. Power grids limits for supercomputer centers (SCC) are
driving the transition to more energy-efficient solutions. Often upgrade of
computing resources is done step-by-step, i.e. parts of older supercomputers
are removed from service and replaced with newer ones. A single SCC at any time
can operate several computing systems with different performance and power
consumption. That is why the problem of scheduling parallel programs execution
on SCC resources to optimize energy consumption and minimize the increase in
execution time (energy-efficient scheduling) is important. The goal of the
presented work was the development of a new energy-efficient algorithm for
scheduling computing resources at SCC. To reach the goal the authors analyzed
methods of scheduling computing resources in a shared facility, including
energy consumption minimizing methods. The study made it possible to formulate
the problem of energy-efficient scheduling for a set of CCs and propose an
algorithm for its solution. Experiments on NPB benchmarks allowed achieving
significant reduction in energy consumption with a minor increase of runtime.

    "," Distributed, Parallel, and Cluster Computing",clustering,An energy-efficient scheduling algorithm for shared facility supercomputer centers
"  Combined resistance noise and muon-spin relaxation ($\mu$SR) measurements of
the ferromagnetic semiconductor HgCr$_2$Se$_4$ suggest a degree of
magnetoelectric coupling and provide evidence for the existence of isolated
magnetic polarons. These form at elevated temperatures and undergo a
percolation transition with a drastic enhancement of the low-frequency
1/$f$-type charge fluctuations at the insulator-to-metal transition at $\sim 95
- 98$ K in the vicinity of the magnetic ordering temperature $T_C \sim 105 -
107$ K. Upon approaching the percolation threshold from above, the strikingly
unusual dynamics of a distinct two-level fluctuator superimposed on the $1/f$
noise can be described by a slowing down of the dynamics of a nanoscale
magnetic cluster, a magnetic polaron, when taking into account an effective
radius of the polaron depending on the spin correlation length. Coinciding
temperature scales found in $\mu$SR and noise measurements suggest changes in
the magnetic dynamics over a wide range of frequencies and are consistent with
the existence of large polarized and domain-wall-like regions at low
temperatures, that result from the freezing of spin dynamics at the magnetic
polaron percolation transition.

    ", Strongly Correlated Electrons,clustering,Probing the magnetic polaron state in the ferromagnetic semiconductor HgCr$_2$Se$_4$ with resistance fluctuation and muon-spin spectroscopy measurements
"  Supermassive black holes dominate the gravitational potential in galactic
nuclei. In these dense environments, stars follow nearly Keplerian orbits and
see their orbital planes relax through the potential fluctuations generated by
the stellar cluster itself. For typical astrophysical galactic nuclei, the most
likely outcome of this vector resonant relaxation (VRR) is that the orbital
planes of the most massive stars spontaneously self-align within a narrow disc.
We present a maximum entropy method to systematically determine this long-term
distribution of orientations and use it for a wide range of stellar orbital
parameters and initial conditions. The heaviest stellar objects are found to
live within a thin equatorial disk. The thickness of this disk depends on the
stars' initial mass function, and on the geometry of the initial cluster. This
work highlights a possible (indirect) novel method to constrain the
distribution of intermediate mass black holes in galactic nuclei.

    ", Astrophysics of Galaxies,clustering,Orbital alignment and mass segregation in galactic nuclei via vector resonant relaxation
"  Parkinson disease (PD)'s speech recognition is an effective way for its
diagnosis, which has become a hot and difficult research area in recent years.
As we know, there are large corpuses (segments) within one subject. However,
too large segments will increase the complexity of the classification model.
Besides, the clinicians interested in finding diagnostic speech markers that
reflect the pathology of the whole subject. Since the optimal relevant features
of each speech sample segment are different, it is difficult to find the
uniform diagnostic speech markers. Therefore, it is necessary to reconstruct
the existing large segments within one subject into few segments even one
segment within one subject, which can facilitate the extraction of relevant
speech features to characterize diagnostic markers for the whole subject. To
address this problem, an enveloped deep speech sample learning algorithm for
Parkinson's subjects based on multilayer fuzzy c-mean (MlFCM) clustering and
interlayer consistency preservation is proposed in this paper. The algorithm
can be used to achieve intra-subject sample reconstruction for Parkinson's
disease (PD) to obtain a small number of high-quality prototype sample
segments. At the end of the paper, several representative PD speech datasets
are selected and compared with the state-of-the-art related methods,
respectively. The experimental results show that the proposed algorithm is
effective signifcantly.

    ", Sound,clustering,Subject Enveloped Deep Sample Fuzzy Ensemble Learning Algorithm of Parkinson's Speech Data
"  The ability to analyse and differentiate network protocol traffic is crucial
for network resource management to provide differentiated services by Telcos.
Automated Protocol Analysis (APA) is crucial to significantly improve
efficiency and reduce reliance on human experts. There are numerous automated
state-of-the-art unsupervised methods for clustering unknown protocols in APA.
However, many such methods have not been sufficiently explored using diverse
test datasets. Thus failing to demonstrate their robustness to generalise. This
study proposed a comprehensive framework to evaluate various combinations of
feature extraction and clustering methods in APA. It also proposed a novel
approach to automate selection of dataset dependent model parameters for
feature extraction, resulting in improved performance. Promising results of a
novel field-based tokenisation approach also led to our proposal of a novel
automated hybrid approach for feature extraction and clustering of unknown
protocols in APA. Our proposed hybrid approach performed the best in 7 out of 9
of the diverse test datasets, thus displaying the robustness to generalise
across diverse unknown protocols. It also outperformed the unsupervised
clustering technique in state-of-the-art open-source APA tool, NETZOB in all
test datasets.

    ", Machine Learning,clustering,Exploring Unsupervised Learning Methods for Automated Protocol Analysis
"  Modern datasets often contain large subsets of correlated features and
nuisance features, which are not or loosely related to the main underlying
structures of the data. Nuisance features can be identified using the Laplacian
score criterion, which evaluates the importance of a given feature via its
consistency with the Graph Laplacians' leading eigenvectors. We demonstrate
that in the presence of large numbers of nuisance features, the Laplacian must
be computed on the subset of selected features rather than on the complete
feature set. To do this, we propose a fully differentiable approach for
unsupervised feature selection, utilizing the Laplacian score criterion to
avoid the selection of nuisance features. We employ an autoencoder architecture
to cope with correlated features, trained to reconstruct the data from the
subset of selected features. Building on the recently proposed concrete layer
that allows controlling for the number of selected features via architectural
design, simplifying the optimization process. Experimenting on several
real-world datasets, we demonstrate that our proposed approach outperforms
similar approaches designed to avoid only correlated or nuisance features, but
not both. Several state-of-the-art clustering results are reported.

    ", Machine Learning,unsupervised learning,Deep Unsupervised Feature Selection by Discarding Nuisance and Correlated Features
"  Millimeter-wave cellular communication requires beamforming procedures that
enable alignment of the transmitter and receiver beams as the user equipment
(UE) moves. For efficient beam tracking it is advantageous to classify users
according to their traffic and mobility patterns. Research to date has
demonstrated efficient ways of machine learning based UE classification.
Although different machine learning approaches have shown success, most of them
are based on physical layer attributes of the received signal. This, however,
imposes additional complexity and requires access to those lower layer signals.
In this paper, we show that traditional supervised and even unsupervised
machine learning methods can successfully be applied on higher layer channel
measurement reports in order to perform UE classification, thereby reducing the
complexity of the classification process.

    ", Information Theory,unsupervised learning,Learning-Based UE Classification in Millimeter-Wave Cellular Systems With Mobility
"  Person re-identification (Re-ID) has been a significant research topic in the
past decade due to its real-world applications and research significance. While
supervised person Re-ID methods achieve superior performance over unsupervised
counterparts, they can not scale to large unlabelled datasets and new domains
due to the prohibitive labelling cost. Therefore, unsupervised person Re-ID has
drawn increasing attention for its potential to address the scalability issue
in person Re-ID. Unsupervised person Re-ID is challenging primarily due to
lacking identity labels to supervise person feature learning. The corresponding
solutions are diverse and complex, with various merits and limitations.
Therefore, comprehensive surveys on this topic are essential to summarise
challenges and solutions to foster future research. Existing person Re-ID
surveys have focused on supervised methods from classifications and
applications but lack detailed discussion on how the person Re-ID solutions
address the underlying challenges. This survey review recent works on
unsupervised person Re-ID from the perspective of challenges and solutions.
Specifically, we provide an in-depth analysis of highly influential methods
considering the four significant challenges in unsupervised person Re-ID: 1)
lacking ground-truth identity labels to supervise person feature learning; 2)
learning discriminative person features with pseudo-supervision; 3) learning
cross-camera invariant person feature, and 4) the domain shift between
datasets. We summarise and analyse evaluation results and provide insights on
the effectiveness of the solutions. Finally, we discuss open issues and suggest
some promising future research directions.

    ", Computer Vision and Pattern Recognition,unsupervised learning,Unsupervised Person Re-Identification: A Systematic Survey of Challenges and Solutions
"  Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to a different unlabeled target domain. Most existing
UDA methods focus on learning domain-invariant feature representation, either
from the domain level or category level, using convolution neural networks
(CNNs)-based frameworks. One fundamental problem for the category level based
UDA is the production of pseudo labels for samples in target domain, which are
usually too noisy for accurate domain alignment, inevitably compromising the
UDA performance. With the success of Transformer in various tasks, we find that
the cross-attention in Transformer is robust to the noisy input pairs for
better feature alignment, thus in this paper Transformer is adopted for the
challenging UDA task. Specifically, to generate accurate input pairs, we design
a two-way center-aware labeling algorithm to produce pseudo labels for target
samples. Along with the pseudo labels, a weight-sharing triple-branch
transformer framework is proposed to apply self-attention and cross-attention
for source/target feature learning and source-target domain alignment,
respectively. Such design explicitly enforces the framework to learn
discriminative domain-specific and domain-invariant representations
simultaneously. The proposed method is dubbed CDTrans (cross-domain
transformer), and it provides one of the first attempts to solve UDA tasks with
a pure transformer solution. Extensive experiments show that our proposed
method achieves the best performance on Office-Home, VisDA-2017, and DomainNet
datasets.

    ", Computer Vision and Pattern Recognition,unsupervised learning,CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation
"  One of the outstanding analytical problems in X-ray single particle imaging
(SPI) is the classification of structural heterogeneity, which is especially
difficult given the low signal-to-noise ratios of individual patterns and that
even identical objects can yield patterns that vary greatly when orientation is
taken into consideration. We propose two methods which explicitly account for
this orientation-induced variation and can robustly determine the structural
landscape of a sample ensemble. The first, termed common-line principal
component analysis (PCA) provides a rough classification which is essentially
parameter-free and can be run automatically on any SPI dataset. The second
method, utilizing variation auto-encoders (VAEs) can generate 3D structures of
the objects at any point in the structural landscape. We implement both these
methods in combination with the noise-tolerant expand-maximize-compress (EMC)
algorithm and demonstrate its utility by applying it to an experimental dataset
from gold nanoparticles with only a few thousand photons per pattern and
recover both discrete structural classes as well as continuous deformations.
These developments diverge from previous approaches of extracting reproducible
subsets of patterns from a dataset and open up the possibility to move beyond
studying homogeneous sample sets and study open questions on topics such as
nanocrystal growth and dynamics as well as phase transitions which have not
been externally triggered.

    ", Image and Video Processing,unsupervised learning,Unsupervised learning approaches to characterize heterogeneous samples using X-ray single particle imaging
"  The paper describes our proposed methodology for the seven basic expression
classification track of Affective Behavior Analysis in-the-wild (ABAW)
Competition 2021. In this task, facial expression recognition (FER) methods aim
to classify the correct expression category from a diverse background, but
there are several challenges. First, to adapt the model to in-the-wild
scenarios, we use the knowledge from pre-trained large-scale face recognition
data. Second, we propose an ensemble model with a convolution neural network
(CNN), a CNN-recurrent neural network (CNN-RNN), and a CNN-Transformer
(CNN-Transformer), to incorporate both spatial and temporal information. Our
ensemble model achieved F1 as 0.4133, accuracy as 0.6216 and final metric as
0.4821 on the validation set.

    ", Computer Vision and Pattern Recognition,recurrent neural networks,Spatial and Temporal Networks for Facial Expression Recognition in the Wild Videos
"  Controlling the model to generate texts of different categories is a
challenging task that is getting more and more attention. Recently, generative
adversarial net (GAN) has shown promising results in category text generation.
However, the texts generated by GANs usually suffer from the problems of mode
collapse and training instability. To avoid the above problems, we propose a
novel model named category-aware variational recurrent neural network
(CatVRNN), which is inspired by multi-task learning. In our model, generation
and classification are trained simultaneously, aiming at generating texts of
different categories. Moreover, the use of multi-task learning can improve the
quality of generated texts, when the classification task is appropriate. And we
propose a function to initialize the hidden state of CatVRNN to force model to
generate texts of a specific category. Experimental results on three datasets
demonstrate that our model can do better than several state-of-the-art text
generation methods based GAN in the category accuracy and quality of generated
texts.

    ", Computation and Language,recurrent neural networks,CatVRNN: Generating Category Texts via Multi-task Learning
"  This paper presents an attempt to employ the mask language modeling approach
of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of
polyphonic piano MIDI files for tackling a number of symbolic-domain
discriminative music understanding tasks. These include two note-level
classification tasks, i.e., melody extraction and velocity prediction, as well
as two sequence-level classification tasks, i.e., composer classification and
emotion classification. We find that, given a pre-trained Transformer, our
models outperform recurrent neural network based baselines with less than 10
epochs of fine-tuning. Ablation studies show that the pre-training remains
effective even if none of the MIDI data of the downstream tasks are seen at the
pre-training stage, and that freezing the self-attention layers of the
Transformer at the fine-tuning stage slightly degrades performance. All the
five datasets employed in this work are publicly available, as well as
checkpoints of our pre-trained and fine-tuned models. As such, our research can
be taken as a benchmark for symbolic-domain music understanding.

    ", Sound,recurrent neural networks,MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding
"  Most of the existing deep learning-based sequential recommendation approaches
utilize the recurrent neural network architecture or self-attention to model
the sequential patterns and temporal influence among a user's historical
behavior and learn the user's preference at a specific time. However, these
methods have two main drawbacks. First, they focus on modeling users' dynamic
states from a user-centric perspective and always neglect the dynamics of items
over time. Second, most of them deal with only the first-order user-item
interactions and do not consider the high-order connectivity between users and
items, which has recently been proved helpful for the sequential
recommendation. To address the above problems, in this article, we attempt to
model user-item interactions by a bipartite graph structure and propose a new
recommendation approach based on a Position-enhanced and Time-aware Graph
Convolutional Network (PTGCN) for the sequential recommendation. PTGCN models
the sequential patterns and temporal dynamics between user-item interactions by
defining a position-enhanced and time-aware graph convolution operation and
learning the dynamic representations of users and items simultaneously on the
bipartite graph with a self-attention aggregator. Also, it realizes the
high-order connectivity between users and items by stacking multi-layer graph
convolutions. To demonstrate the effectiveness of PTGCN, we carried out a
comprehensive evaluation of PTGCN on three real-world datasets of different
sizes compared with a few competitive baselines. Experimental results indicate
that PTGCN outperforms several state-of-the-art models in terms of two
commonly-used evaluation metrics for ranking.

    ", Information Retrieval,recurrent neural networks,Position-enhanced and Time-aware Graph Convolutional Network for Sequential Recommendations
"  Recurrent Neural Networks (RNNs) represent the de facto standard machine
learning tool for sequence modelling, owing to their expressive power and
memory. However, when dealing with large dimensional data, the corresponding
exponential increase in the number of parameters imposes a computational
bottleneck. The necessity to equip RNNs with the ability to deal with the curse
of dimensionality, such as through the parameter compression ability inherent
to tensors, has led to the development of the Tensor-Train RNN (TT-RNN).
Despite achieving promising results in many applications, the full potential of
the TT-RNN is yet to be explored in the context of interpretable financial
modelling, a notoriously challenging task characterized by multi-modal data
with low signal-to-noise ratio. To address this issue, we investigate the
potential of TT-RNN in the task of financial forecasting of currencies. We
show, through the analysis of TT-factors, that the physical meaning underlying
tensor decomposition, enables the TT-RNN model to aid the interpretability of
results, thus mitigating the notorious ""black-box"" issue associated with neural
networks. Furthermore, simulation results highlight the regularization power of
TT decomposition, demonstrating the superior performance of TT-RNN over its
uncompressed RNN counterpart and other tensor forecasting methods.

    ", Machine Learning,recurrent neural networks,Tensor-Train Recurrent Neural Networks for Interpretable Multi-Way Financial Forecasting
"  This paper describes our contribution to SemEval-2020 Task 7: Assessing Humor
in Edited News Headlines. Here we present a method based on a deep neural
network. In recent years, quite some attention has been devoted to humor
production and perception. Our team KdeHumor employs recurrent neural network
models including Bi-Directional LSTMs (BiLSTMs). Moreover, we utilize the
state-of-the-art pre-trained sentence embedding techniques. We analyze the
performance of our method and demonstrate the contribution of each component of
our architecture.

    ", Computation and Language,recurrent neural networks,kdehumor at semeval-2020 task 7: a neural network model for detecting funniness in dataset humicroedit
"  We use deep imaging from the Dark Energy Camera to explore the peripheral
regions of nine globular clusters in the outer halo of the Milky Way. Apart
from Whiting 1 and NGC 7492, which are projected against the Sagittarius
stream, we see no evidence for adjacent stellar populations to indicate any of
these clusters is associated with coherent tidal debris from a destroyed host
dwarf. We also find no evidence for tidal tails around any of the clusters in
our sample; however, both NGC 1904 and 6981 appear to possess outer envelopes.
Motivated by a slew of recent Gaia-based discoveries, we compile a sample of
clusters with robust detections of extra-tidal structure, and search for
correlations with orbital properties. While we observe that clusters with tidal
tails are typically on moderately or very eccentric orbits that are highly
inclined to the Galactic plane and often retrograde, these are neither
necessary nor sufficient conditions for the formation of extra-tidal structure.
That many objects with tidal tails appear to be accreted leads us to speculate
that this lack of consistency may stem from the inhomogeneous dynamical history
of the Milky Way globular cluster system. Finally, we note that clusters with
prominent stellar envelopes detected in ground-based imaging (such as NGC 1851
and 7089) are now all known from Gaia to possess long tidal tails --
experimental confirmation that the presence of an extended envelope is
indicative of tidal erosion.

    ", Astrophysics of Galaxies,clustering,A search for stellar structures around nine outer halo globular clusters in the Milky Way
"  The outer light (stellar halos) of massive galaxies has recently emerged as a
possible low scatter tracer of dark matter halo mass. To test the robustness of
outer light measurements across different data sets, we compare the surface
brightness profiles of massive galaxies using four independent data sets: the
Hyper Suprime-Cam survey (HSC), the Dark Energy Camera Legacy Survey (DECaLS),
the Sloan Digital Sky Survey (SDSS), and the Dragonfly Wide Field Survey
(Dragonfly). We use customized pipelines for HSC and DECaLS to achieve better
sky background subtraction. For galaxies at $z<0.05$, Dragonfly has the best
control of systematics, reaching surface brightness levels of $\mu_r \sim 30$
mag/arcsec$^{2}$. At $0.19<z<0.50$, HSC can reliably recover surface brightness
profiles to $\mu_{r} \sim 28.5$ mag/arcsec$^{2}$ reaching $R=100 - 150$ kpc.
DECaLS surface brightness profiles show good agreement with HSC but are noisier
at large radii. The median profiles of galaxy ensembles in both HSC and DECaLS
reach $R > 200$ kpc without significant bias. At $0.19<z<0.50$, DECaLS and HSC
measurements of the stellar mass contained within 100 kpc agree within 0.05
dex. Finally, we use weak gravitational lensing to show that measurements of
outer light with DECaLS at $0.19<z<0.50$ show a similar promise as HSC as a low
scatter proxy of halo mass. The tests and results from this paper represent an
important step forward for accurate measurements of the outer light of massive
galaxies and demonstrate that outer light measurements from DECam imaging will
be a promising method for finding galaxy clusters for DES and DESI.

    ", Astrophysics of Galaxies,clustering,"Reaching for the Edge I: Probing the Outskirts of Massive Galaxies with HSC, DECaLS, SDSS, and Dragonfly"
"  In the mining industry, many reports are generated in the project management
process. These past documents are a great resource of knowledge for future
success. However, it would be a tedious and challenging task to retrieve the
necessary information if the documents are unorganized and unstructured.
Document clustering is a powerful approach to cope with the problem, and many
methods have been introduced in past studies. Nonetheless, there is no silver
bullet that can perform the best for any types of documents. Thus, exploratory
studies are required to apply the clustering methods for new datasets. In this
study, we will investigate multiple topic modelling (TM) methods. The
objectives are finding the appropriate approach for the mining project reports
using the dataset of the Geological Survey of Queensland, Department of
Resources, Queensland Government, and understanding the contents to get the
idea of how to organise them. Three TM methods, Latent Dirichlet Allocation
(LDA), Nonnegative Matrix Factorization (NMF), and Nonnegative Tensor
Factorization (NTF) are compared statistically and qualitatively. After the
evaluation, we conclude that the LDA performs the best for the dataset;
however, the possibility remains that the other methods could be adopted with
some improvements.

    ", Information Retrieval,clustering,Investigation of Topic Modelling Methods for Understanding the Reports of the Mining Projects in Queensland
"  Using the Parker Solar Probe FIELDS bandpass filter data and SWEAP electron
data from Encounters 1 through 9, we show statistical properties of narrowband
whistlers from ~16 Rs to ~130 Rs, and compare wave occurrence to electron
properties including beta, temperature anisotropy and heat flux. Whistlers are
very rarely observed inside ~28 Rs (~0.13 au). Outside 28 Rs, they occur within
a narrow range of parallel electron beta from ~1 to 10, and with a beta-heat
flux occurrence consistent with the whistler heat flux fan instability. Because
electron distributions inside ~30 Rs display signatures of the ambipolar
electric field, the lack of whistlers suggests that the modification of the
electron distribution function associated with the ambipolar electric field or
changes in other plasma properties must result in lower instability limits for
the other modes (including solitary waves, ion acoustic waves) that are
observed close to the Sun. The lack of narrowband whistler-mode waves close to
the Sun and in regions of either low (<.1) or high (>10) beta is also
significant for the understanding and modeling of the evolution of
flare-accelerated electrons, and the regulation of heat flux in astrophysical
settings including other stellar winds, the interstellar medium, accretion
disks, and the intra-galaxy cluster medium

    ", Solar and Stellar Astrophysics,clustering,Parker Solar Probe evidence for the absence of whistlers close to the Sun to scatter strahl and regulate heat flux
"  In the field of deep learning, various architectures have been developed.
However, most studies are limited to specific tasks or datasets due to their
fixed layer structure. This paper does not express the structure delivering
information as a network model but as a data structure called an association
tree(AT). And we propose two artificial association networks(AANs) designed to
solve the problems of existing networks by analyzing the structure of human
neural networks. Defining the starting and ending points of the path in a
single graph is difficult, and a tree cannot express the relationship among
sibling nodes. On the contrary, an AT can express leaf and root nodes as the
starting and ending points of the path and the relationship among sibling
nodes. Instead of using fixed sequence layers, we create an AT for each data
and train AANs according to the tree's structure. AANs are data-driven learning
in which the number of convolutions varies according to the depth of the tree.
Moreover, AANs can simultaneously learn various types of datasets through the
recursive learning. Depth-first convolution (DFC) encodes the interaction
result from leaf nodes to the root node in a bottom-up approach, and
depth-first deconvolution (DFD) decodes the interaction result from the root
node to the leaf nodes in a top-down approach. We conducted three experiments.
The first experiment verified whether it could be processed by combining AANs
and feature extraction networks. In the second, we compared the performance of
networks that separately learned image, sound, and tree, graph structure
datasets with the performance simultaneously learned by connecting these
networks. In the third, we verified whether the output of AANs can embed all
data in the AT. As a result, AATs learned without significant performance
degradation.

    ", Artificial Intelligence,convolutional neural networks,Artificial Association Neural Networks
"  In this work, we propose a Graph Convolutional Neural Networks (GCN) based
scheduling algorithm for adhoc networks. In particular, we consider a
generalized interference model called the $k$-tolerant conflict graph model and
design an efficient approximation for the well-known Max-Weight scheduling
algorithm. A notable feature of this work is that the proposed method do not
require labelled data set (NP-hard to compute) for training the neural network.
Instead, we design a loss function that utilises the existing greedy approaches
and trains a GCN that improves the performance of greedy approaches. Our
extensive numerical experiments illustrate that using our GCN approach, we can
significantly ($4$-$20$ percent) improve the performance of the conventional
greedy approach.

    ", Systems and Control,convolutional neural networks,Graph Neural Network based scheduling : Improved throughput under a generalized interference model
"  Damage assessment after natural disasters is needed to distribute aid and
forces to recovery from damage dealt optimally. This process involves acquiring
satellite imagery for the region of interest, localization of buildings, and
classification of the amount of damage caused by nature or urban factors to
buildings. In case of natural disasters, this means processing many square
kilometers of the area to judge whether a particular building had suffered from
the damaging factors.
", Computer Vision and Pattern Recognition,convolutional neural networks,Fully convolutional Siamese neural networks for buildings damage assessment from satellite images
"  Recently, the stability of graph filters has been studied as one of the key
theoretical properties driving the highly successful graph convolutional neural
networks (GCNs). The stability of a graph filter characterizes the effect of
topology perturbation on the output of a graph filter, a fundamental building
block for GCNs. Many existing results have focused on the regime of small
perturbation with a small number of edge rewires. However, the number of edge
rewires can be large in many applications. To study the latter case, this work
departs from the previous analysis and proves a bound on the stability of graph
filter relying on the filter's frequency response. Assuming the graph filter is
low pass, we show that the stability of the filter depends on perturbation to
the community structure. As an application, we show that for stochastic block
model graphs, the graph filter distance converges to zero when the number of
nodes approaches infinity. Numerical simulations validate our findings.

    ", Signal Processing,convolutional neural networks,On the Stability of Low Pass Graph Filter With a Large Number of Edge Rewires
"  Many nonequilibrium systems, such as biochemical reactions and socioeconomic
interactions, can be described by reaction-diffusion equations that demonstrate
a wide variety of complex spatiotemporal patterns. The diversity of the
morphology of these patterns makes it difficult to classify them quantitatively
and they are often described visually. Hence, searching through a large
parameter space for patterns is a tedious manual task. We discuss how
convolutional neural networks can be used to scan the parameter space,
investigate existing patterns in more detail, and aid in finding new groups of
patterns. As an example, we consider the Gray-Scott model for which training
data is easy to obtain. Due to the popularity of machine learning in many
scientific fields, well maintained open source toolkits are available that make
it easy to implement the methods we discuss in advanced undergraduate and
graduate computational physics projects.

    ", Computational Physics,convolutional neural networks,Exploring complex pattern formation with convolutional neural networks
"  Equivariance has emerged as a desirable property of representations of
objects subject to identity-preserving transformations that constitute a group,
such as translations and rotations. However, the expressivity of a
representation constrained by group equivariance is still not fully understood.
We address this gap by providing a generalization of Cover's Function Counting
Theorem that quantifies the number of linearly separable and group-invariant
binary dichotomies that can be assigned to equivariant representations of
objects. We find that the fraction of separable dichotomies is determined by
the dimension of the space that is fixed by the group action. We show how this
relation extends to operations such as convolutions, element-wise
nonlinearities, and global and local pooling. While other operations do not
change the fraction of separable dichotomies, local pooling decreases the
fraction, despite being a highly nonlinear operation. Finally, we test our
theory on intermediate representations of randomly initialized and fully
trained convolutional neural networks and find perfect agreement.

    ", Machine Learning,convolutional neural networks,Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?
"  We address the issue of safety in reinforcement learning. We pose the problem
in a discounted infinite-horizon constrained Markov decision process framework.
Existing results have shown that gradient-based methods are able to achieve an
$\mathcal{O}(1/\sqrt{T})$ global convergence rate both for the optimality gap
and the constraint violation. We exhibit a natural policy gradient-based
algorithm that has a faster convergence rate $\mathcal{O}(\log(T)/T)$ for both
the optimality gap and the constraint violation. When Slater's condition is
satisfied and known a priori, zero constraint violation can be further
guaranteed for a sufficiently large $T$ while maintaining the same convergence
rate.

    ", Machine Learning,reinforcement learning,Fast Global Convergence of Policy Optimization for Constrained MDPs
"  Multi-agent reinforcement learning tasks put a high demand on the volume of
training samples. Different from its single-agent counterpart, distributed
value-based multi-agent reinforcement learning faces the unique challenges of
demanding data transfer, inter-process communication management, and high
requirement of exploration. We propose a containerized learning framework to
solve these problems. We pack several environment instances, a local learner
and buffer, and a carefully designed multi-queue manager which avoids blocking
into a container. Local policies of each container are encouraged to be as
diverse as possible, and only trajectories with highest priority are sent to a
global learner. In this way, we achieve a scalable, time-efficient, and diverse
distributed MARL learning framework with high system throughput. To own
knowledge, our method is the first to solve the challenging Google Research
Football full game $5\_v\_5$. On the StarCraft II micromanagement benchmark,
our method gets $4$-$18\times$ better results compared to state-of-the-art
non-distributed MARL algorithms.

    ", Machine Learning,reinforcement learning,Containerized Distributed Value-Based Multi-Agent Reinforcement Learning
"  Collaborating with humans requires rapidly adapting to their individual
strengths, weaknesses, and preferences. Unfortunately, most standard
multi-agent reinforcement learning techniques, such as self-play (SP) or
population play (PP), produce agents that overfit to their training partners
and do not generalize well to humans. Alternatively, researchers can collect
human data, train a human model using behavioral cloning, and then use that
model to train ""human-aware"" agents (""behavioral cloning play"", or BCP). While
such an approach can improve the generalization of agents to new human
co-players, it involves the onerous and expensive step of collecting large
amounts of human data first. Here, we study the problem of how to train agents
that collaborate well with human partners without using human data. We argue
that the crux of the problem is to produce a diverse set of training partners.
Drawing inspiration from successful multi-agent approaches in competitive
domains, we find that a surprisingly simple approach is highly effective. We
train our agent partner as the best response to a population of self-play
agents and their past checkpoints taken throughout training, a method we call
Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative
cooking simulator that has recently been proposed as a challenge problem for
coordination with humans. We find that FCP agents score significantly higher
than SP, PP, and BCP when paired with novel agent and human partners.
Furthermore, humans also report a strong subjective preference to partnering
with FCP agents over all baselines.

    ", Machine Learning,reinforcement learning,Collaborating with Humans without Human Data
"  Reliable AI agents should be mindful of the limits of their knowledge and
consult humans when sensing that they do not have sufficient knowledge to make
sound decisions. We formulate a hierarchical reinforcement learning framework
for learning to decide when to request additional information from humans and
what type of information would be helpful to request. Our framework extends
partially-observed Markov decision processes (POMDPs) by allowing an agent to
interact with an assistant to leverage their knowledge in accomplishing tasks.
Results on a simulated human-assisted navigation problem demonstrate the
effectiveness of our framework: aided with an interaction policy learned by our
method, a navigation policy achieves up to a 7x improvement in task success
rate compared to performing tasks only by itself. The interaction policy is
also efficient: on average, only a quarter of all actions taken during a task
execution are requests for information. We analyze benefits and challenges of
learning with a hierarchical policy structure and suggest directions for future
work.

    ", Machine Learning,reinforcement learning,Learning When and What to Ask: a Hierarchical Reinforcement Learning Framework
"  Plants are dynamic systems that are integral to our existence and survival.
Plants face environment changes and adapt over time to their surrounding
conditions. We argue that plant responses to an environmental stimulus are a
good example of a real-world problem that can be approached within a
reinforcement learning (RL)framework. With the objective of controlling a plant
by moving the light source, we propose GrowSpace, as a new RL benchmark. The
back-end of the simulator is implemented using the Space Colonisation
Algorithm, a plant growing model based on competition for space. Compared to
video game RL environments, this simulator addresses a real-world problem and
serves as a test bed to visualize plant growth and movement in a faster way
than physical experiments. GrowSpace is composed of a suite of challenges that
tackle several problems such as control, multi-stage learning,fairness and
multi-objective learning. We provide agent baselines alongside case studies to
demonstrate the difficulty of the proposed benchmark.

    ", Machine Learning,reinforcement learning,GrowSpace: Learning How to Shape Plants
"  State abstraction enables sample-efficient learning and better task transfer
in complex reinforcement learning environments. Recently, we proposed RePReL
(Kokel et al. 2021), a hierarchical framework that leverages a relational
planner to provide useful state abstractions for learning. We present a brief
overview of this framework and the use of a dynamic probabilistic logic model
to design these state abstractions. Our experiments show that RePReL not only
achieves better performance and efficient learning on the task at hand but also
demonstrates better generalization to unseen tasks.

    ", Artificial Intelligence,reinforcement learning,Dynamic probabilistic logic models for effective abstractions in RL
"  We report two essential improvements in readability assessment: 1. three
novel features in advanced semantics and 2. the timely evidence that
traditional ML models (e.g. Random Forest, using handcrafted features) can
combine with transformers (e.g. RoBERTa) to augment model performance. First,
we explore suitable transformers and traditional ML models. Then, we extract
255 handcrafted linguistic features using self-developed extraction software.
Finally, we assemble those to create several hybrid models, achieving
state-of-the-art (SOTA) accuracy on popular datasets in readability assessment.
The use of handcrafted features help model performance on smaller datasets.
Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification
accuracy of 99%, a 20.3% increase from the previous SOTA.

    ", Computation and Language,natural language processing,Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features
"  We propose a novel task of jointly repairing program codes and generating
commit messages. Code repair and commit message generation are two essential
and related tasks for software development. However, existing work usually
performs the two tasks independently. We construct a multilingual triple
dataset including buggy code, fixed code, and commit messages for this novel
task. We provide the cascaded models as baseline, which are enhanced with
different training approaches, including the teacher-student method, the
multi-task method, and the back-translation method. To deal with the error
propagation problem of the cascaded method, the joint model is proposed that
can both repair the code and generate the commit message in a unified
framework. Experimental results show that the enhanced cascaded model with
teacher-student method and multitask-learning method achieves the best score on
different metrics of automated code repair, and the joint model behaves better
than the cascaded model on commit message generation.

    ", Computation and Language,natural language processing,Jointly Learning to Repair Code and Generate Commit Message
"  Topic Modeling refers to the problem of discovering the main topics that have
occurred in corpora of textual data, with solutions finding crucial
applications in numerous fields. In this work, inspired by the recent
advancements in the Natural Language Processing domain, we introduce FAME, an
open-source framework enabling an efficient mechanism of extracting and
incorporating textual features and utilizing them in discovering topics and
clustering text documents that are semantically similar in a corpus. These
features range from traditional approaches (e.g., frequency-based) to the most
recent auto-encoding embeddings from transformer-based language models such as
BERT model family. To demonstrate the effectiveness of this library, we
conducted experiments on the well-known News-Group dataset. The library is
available online.

    ", Computation and Language,natural language processing,A Framework for Neural Topic Modeling of Text Corpora
"  With the development of deep learning (DL), natural language processing (NLP)
makes it possible for us to analyze and understand a large amount of language
texts. Accordingly, we can achieve a semantic communication in terms of joint
semantic source and channel coding over a noisy channel with the help of NLP.
However, the existing method to realize this goal is to use a fixed transformer
of NLP while ignoring the difference of semantic information contained in each
sentence. To solve this problem, we propose a new semantic communication system
based on Universal Transformer. Compared with the traditional transformer, an
adaptive circulation mechanism is introduced in the Universal Transformer.
Through the introduction of the circulation mechanism, the new semantic
communication system can be more flexible to transmit sentences with different
semantic information, and achieve better end-to-end performance under various
channel conditions.

    ", Computation and Language,natural language processing,Semantic Communication with Adaptive Universal Transformer
"  Document alignment techniques based on multilingual sentence representations
have recently shown state of the art results. However, these techniques rely on
unsupervised distance measurement techniques, which cannot be fined-tuned to
the task at hand. In this paper, instead of these unsupervised distance
measurement techniques, we employ Metric Learning to derive task-specific
distance measurements. These measurements are supervised, meaning that the
distance measurement metric is trained using a parallel dataset. Using a
dataset belonging to English, Sinhala, and Tamil, which belong to three
different language families, we show that these task-specific supervised
distance learning metrics outperform their unsupervised counterparts, for
document alignment.

    ", Computation and Language,natural language processing,Metric Learning in Multilingual Sentence Similarity Measurement for Document Alignment
"  Activation functions play a pivotal role in determining the training dynamics
and neural network performance. The widely adopted activation function ReLU
despite being simple and effective has few disadvantages including the Dying
ReLU problem. In order to tackle such problems, we propose a novel activation
function called Serf which is self-regularized and nonmonotonic in nature. Like
Mish, Serf also belongs to the Swish family of functions. Based on several
experiments on computer vision (image classification and object detection) and
natural language processing (machine translation, sentiment classification and
multimodal entailment) tasks with different state-of-the-art architectures, it
is observed that Serf vastly outperforms ReLU (baseline) and other activation
functions including both Swish and Mish, with a markedly bigger margin on
deeper architectures. Ablation studies further demonstrate that Serf based
architectures perform better than those of Swish and Mish in varying scenarios,
validating the effectiveness and compatibility of Serf with varying depth,
complexity, optimizers, learning rates, batch sizes, initializers and dropout
rates. Finally, we investigate the mathematical relation between Swish and
Serf, thereby showing the impact of preconditioner function ingrained in the
first derivative of Serf which provides a regularization effect making
gradients smoother and optimization faster.

    ", Machine Learning,natural language processing,SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function
"  Pretrained language models based on the Transformer architecture have
achieved state-of-the-art results in various natural language processing tasks
such as part-of-speech tagging, named entity recognition, and question
answering. However, no such monolingual model for the Uzbek language is
publicly available. In this paper, we introduce UzBERT, a pretrained Uzbek
language model based on the BERT architecture. Our model greatly outperforms
multilingual BERT on masked language model accuracy. We make the model publicly
available under the MIT open-source license.

    ", Computation and Language,natural language processing,UzBERT: pretraining a BERT model for Uzbek
"  Most applications of deep learning techniques in medical imaging are
supervised and require a large number of labeled data which is expensive and
requires many hours of careful annotation by experts. In this paper, we propose
an unsupervised, physics driven domain specific transporter framework with an
attention mechanism to identify relevant key points with applications in
ultrasound imaging. The proposed framework identifies key points that provide a
concise geometric representation highlighting regions with high structural
variation in ultrasound videos. We incorporate physics driven domain specific
information as a feature probability map and use the radon transform to
highlight features in specific orientations. The proposed framework has been
trained on130 Lung ultrasound (LUS) videos and 113 Wrist ultrasound (WUS)
videos and validated on 100 Lung ultrasound (LUS) videos and 58 Wrist
ultrasound (WUS) videos acquired from multiple centers across the globe. Images
from both datasets were independently assessed by experts to identify
clinically relevant features such as A-lines, B-lines and pleura from LUS and
radial metaphysis, radial epiphysis and carpal bones from WUS videos. The key
points detected from both datasets showed high sensitivity (LUS = 99\% , WUS =
74\%) in detecting the image landmarks identified by experts. Also, on
employing for classification of the given lung image into normal and abnormal
classes, the proposed approach, even with no prior training, achieved an
average accuracy of 97\% and an average F1-score of 95\% respectively on the
task of co-classification with 3 fold cross-validation. With the purely
unsupervised nature of the proposed approach, we expect the key point detection
approach to increase the applicability of ultrasound in various examination
performed in emergency and point of care.

    ", Image and Video Processing,unsupervised learning,Physics Driven Domain Specific Transporter Framework with Attention Mechanism for Ultrasound Imaging
"  Recently, vehicle re-identification methods based on deep learning constitute
remarkable achievement. However, this achievement requires large-scale and
well-annotated datasets. In constructing the dataset, assigning globally
available identities (Ids) to vehicles captured from a great number of cameras
is labour-intensive, because it needs to consider their subtle appearance
differences or viewpoint variations. In this paper, we propose
camera-tracklet-aware contrastive learning (CTACL) using the multi-camera
tracklet information without vehicle identity labels. The proposed CTACL
divides an unlabelled domain, i.e., entire vehicle images, into multiple
camera-level subdomains and conducts contrastive learning within and beyond the
subdomains. The positive and negative samples for contrastive learning are
defined using tracklet Ids of each camera. Additionally, the domain adaptation
across camera networks is introduced to improve the generalisation performance
of learnt representations and alleviate the performance degradation resulted
from the domain gap between the subdomains. We demonstrate the effectiveness of
our approach on video-based and image-based vehicle Re-ID datasets.
Experimental results show that the proposed method outperforms the recent
state-of-the-art unsupervised vehicle Re-ID methods. The source code for this
paper is publicly available on
`", Computer Vision and Pattern Recognition,unsupervised learning,Camera-Tracklet-Aware Contrastive Learning for Unsupervised Vehicle Re-Identification
"  Purpose. Brain Magnetic Resonance Images (MRIs) are essential for the
diagnosis of neurological diseases. Recently, deep learning methods for
unsupervised anomaly detection (UAD) have been proposed for the analysis of
brain MRI. These methods rely on healthy brain MRIs and eliminate the
requirement of pixel-wise annotated data compared to supervised deep learning.
While a wide range of methods for UAD have been proposed, these methods are
mostly 2D and only learn from MRI slices, disregarding that brain lesions are
inherently 3D and the spatial context of MRI volumes remains unexploited.
", Image and Video Processing,unsupervised learning,3-Dimensional Deep Learning with Spatial Erasing for Unsupervised Anomaly Segmentation in Brain MRI
"  Recently, $k$NN-MT has shown the promising capability of directly
incorporating the pre-trained neural machine translation (NMT) model with
domain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve
domain adaptation without retraining. Despite being conceptually attractive, it
heavily relies on high-quality in-domain parallel corpora, limiting its
capability on unsupervised domain adaptation, where in-domain parallel corpora
are scarce or nonexistent. In this paper, we propose a novel framework that
directly uses in-domain monolingual sentences in the target language to
construct an effective datastore for $k$-nearest-neighbor retrieval. To this
end, we first introduce an autoencoder task based on the target language, and
then insert lightweight adapters into the original NMT model to map the
token-level representation of this task to the ideal representation of
translation task. Experiments on multi-domain datasets demonstrate that our
proposed approach significantly improves the translation accuracy with
target-side monolingual data, while achieving comparable performance with
back-translation.

    ", Computation and Language,unsupervised learning,Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation
"  Unsupervised representation learning has achieved outstanding performances
using centralized data available on the Internet. However, the increasing
awareness of privacy protection limits sharing of decentralized unlabeled image
data that grows explosively in multiple parties (e.g., mobile phones and
cameras). As such, a natural problem is how to leverage these data to learn
visual representations for downstream tasks while preserving data privacy. To
address this problem, we propose a novel federated unsupervised learning
framework, FedU. In this framework, each party trains models from unlabeled
data independently using contrastive learning with an online network and a
target network. Then, a central server aggregates trained models and updates
clients' models with the aggregated model. It preserves data privacy as each
party only has access to its raw data. Decentralized data among multiple
parties are normally non-independent and identically distributed (non-IID),
leading to performance degradation. To tackle this challenge, we propose two
simple but effective methods: 1) We design the communication protocol to upload
only the encoders of online networks for server aggregation and update them
with the aggregated encoder; 2) We introduce a new module to dynamically decide
how to update predictors based on the divergence caused by non-IID. The
predictor is the other component of the online network. Extensive experiments
and ablations demonstrate the effectiveness and significance of FedU. It
outperforms training with only one party by over 5% and other methods by over
14% in linear and semi-supervised evaluation on non-IID data.

    "," Distributed, Parallel, and Cluster Computing",unsupervised learning,Collaborative Unsupervised Visual Representation Learning from Decentralized Data
"  Although observations of high-redshift quasars demonstrate that many
supermassive black holes (BHs) reached large masses within one billion years
after the Big Bang, the origin of the first BHs is still a mystery. A promising
way to constrain the origin of the first BHs is to explore the average
properties of $z\gtrsim6$ BHs. However, typical BHs remain hidden from X-ray
surveys, which is due to their relatively faint nature and the limited
sensitivity of X-ray telescopes. Gravitational lensing provides an attractive
way to study this unique galaxy population as it magnifies the faint light from
these high-redshift galaxies. Here, we study the X-ray emission originating
from 155 gravitationally-lensed $z\gtrsim6$ galaxies that were detected in the
RELICS survey. We utilize Chandra X-ray observations to search for AGN in the
individual galaxies and in the stacked galaxy samples. We identify two
potential AGN in the high-redshift galaxies, but due to the $\sim$$2''$ offset
between the X-ray source and the galaxy, we speculate that the X-ray sources
are not likely to be associated with the galaxies. We stack the signal from all
galaxies and do not find a statistically significant detection. We split our
sample based on stellar mass, star-formation rate, and lensing magnification
and stack these sub-samples. We obtain a $2.2\sigma$ detection for massive
galaxies with an X-ray luminosity of $(3.7\pm1.6)\times10^{42} \ \rm{erg \
s^{-1}}$, which corresponds to a $(3.0\pm1.3)\times10^5 \ \rm{M_{\odot}}$ BH
accreting at its Eddington rate. Other stacks remain undetected and we place
upper limits on the AGN emission. These limits imply that the bulk of BHs at
$z\gtrsim6$ either accrete at a few percent of their Eddington rate and/or are
$1-2$ orders of magnitude less massive than expected based on the stellar mass
of their host galaxy.

    ", Astrophysics of Galaxies,clustering,Exploring Gravitationally-Lensed $z\gtrsim6$ X-ray AGN Behind the RELICS clusters
"  We present the first results of a deep $Chandra$ observation of the galaxy
cluster RBS 797, whose previous X-ray studies revealed two pronounced X-ray
cavities in the east-west (E-W) direction. Follow-up VLA radio observations of
the central active galactic nucleus (AGN) uncovered different jet and lobe
orientations, with radio lobes filling the E-W cavities and perpendicular jets
showing emission in the north-south (N-S) direction over the same scale
($\approx$30 kpc). With the new $\sim$427 ks total exposure, we report the
detection of two additional, symmetric X-ray cavities in the N-S direction at
nearly the same radial distance as the E-W ones. The newly discovered N-S
cavities are associated with the radio emission detected at 1.4 GHz and 4.8 GHz
in archival VLA data, making RBS 797 the first galaxy cluster found to have
four equidistant, centrally-symmetric, radio-filled cavities. We derive the
dynamical and radiative ages of the four cavities from X-ray and radio data,
respectively, finding that the two outbursts are approximately coeval, with an
age difference of $\lessapprox$10 Myr between the E-W and N-S cavities. We
discuss two scenarios for the origin of the two perpendicular, equidistant
cavity systems: either the presence of a binary AGN which is excavating coeval
pairs of cavities in perpendicular directions, or a fast ($<$10 Myr) jet
reorientation event which produced subsequent, misaligned outbursts.

    ", Astrophysics of Galaxies,clustering,The deepest $Chandra$ view of RBS 797: evidence for two pairs of equidistant X-ray cavities
"  Line intensity mapping (LIM) is an emerging technique with a unique potential
to probe a wide range of scales and redshifts. Realizing the full potential of
LIM, however, relies on accurate modeling of the signal. We introduce an
extended halo model for the power spectrum of intensity fluctuations of CO
rotational lines and [CII] fine transition line in real space, modeling
nonlinearities in matter fluctuations and biasing relation between the line
intensity fluctuations and the underlying dark matter distribution. We also
compute the stochastic contributions beyond the Poisson approximation using the
halo model framework. To establish the accuracy of the model, we create the
first cosmological-scale simulations of CO and [CII] intensity maps,
\textsf{MithraLIMSims}, at redshifts $0.5 \leq z\leq6$, using halo catalogs
from Hidden-Valley simulations, and painting halos according to
mass-redshift-luminosity relations for each line. We show that at $z=1$ on
scales $k_{\rm max} \lesssim 0.8 \ {\rm Mpc}^{-1}h$, the model predictions of
clustering power (with only two free parameters) are in agreement with the
measured power spectrum at better than 5\%. At higher redshift of $z=4.5$, this
remarkable agreement extends to smaller scale of $ k_{\rm max} \lesssim 2 \
{\rm Mpc}^{-1}h$. Furthermore, we show that on large scales, the stochastic
contributions to CO and CII power spectra are non-Poissonian, with amplitudes
reproduced reasonably well by the halo model prescription. Lastly, we assess
the performance of the theoretical model of the baryon acoustic oscillations
(BAO) and show that hypothetical LIM surveys probing CO lines at $z=1$, that
can be deployed within this decade, will be able to make a high significance
measurement of the BAO. On a longer time scale, a space-based mission probing
[CII] line can uniquely measure the BAO on a wide range of redshifts at an
unprecedented precision.

    ", Cosmology and Nongalactic Astrophysics,clustering,Precision Tests of CO and [CII] Power Spectra Models against Simulated Intensity Maps
"  In recent years, road safety has attracted significant attention from
researchers and practitioners in the intelligent transport systems domain. As
one of the most common and vulnerable groups of road users, pedestrians cause
great concerns due to their unpredictable behavior and movement, as subtle
misunderstandings in vehicle-pedestrian interaction can easily lead to risky
situations or collisions. Existing methods use either predefined
collision-based models or human-labeling approaches to estimate the
pedestrians' risks. These approaches are usually limited by their poor
generalization ability and lack of consideration of interactions between the
ego vehicle and a pedestrian. This work tackles the listed problems by
proposing a Pedestrian Risk Level Prediction system. The system consists of
three modules. Firstly, vehicle-perspective pedestrian data are collected.
Since the data contains information regarding the movement of both the ego
vehicle and pedestrian, it can simplify the prediction of spatiotemporal
features in an interaction-aware fashion. Using the long short-term memory
model, the pedestrian trajectory prediction module predicts their
spatiotemporal features in the subsequent five frames. As the predicted
trajectory follows certain interaction and risk patterns, a hybrid clustering
and classification method is adopted to explore the risk patterns in the
spatiotemporal features and train a risk level classifier using the learned
patterns. Upon predicting the spatiotemporal features of pedestrians and
identifying the corresponding risk level, the risk patterns between the ego
vehicle and pedestrians are determined. Experimental results verified the
capability of the PRLP system to predict the risk level of pedestrians, thus
supporting the collision risk assessment of intelligent vehicles and providing
safety warnings to both vehicles and pedestrians.

    ", Robotics,clustering,Prediction of Pedestrian Spatiotemporal Risk Levels for Intelligent Vehicles: A Data-driven Approach
"  This perspective piece came about through the Generative Adversarial
Collaboration (GAC) series of workshops organized by the Computational
Cognitive Neuroscience (CCN) conference in 2020. We brought together a number
of experts from the field of theoretical neuroscience to debate emerging issues
in our understanding of how learning is implemented in biological recurrent
neural networks. Here, we will give a brief review of the common assumptions
about biological learning and the corresponding findings from experimental
neuroscience and contrast them with the efficiency of gradient-based learning
in recurrent neural networks commonly used in artificial intelligence. We will
then outline the key issues discussed in the workshop: synaptic plasticity,
neural circuits, theory-experiment divide, and objective functions. Finally, we
conclude with recommendations for both theoretical and experimental
neuroscientists when designing new studies that could help to bring clarity to
these issues.

    ", Neurons and Cognition,recurrent neural networks,CCN GAC Workshop: Issues with learning in biological recurrent neural networks
"  Reproducibility of a deep-learning fully convolutional neural network is
evaluated by training several times the same network on identical conditions
(database, hyperparameters, hardware) with non-deterministic Graphics
Processings Unit (GPU) operations. The propagation of two-dimensional acoustic
waves, typical of time-space evolving physical systems, is studied on both
recursive and non-recursive tasks. Significant changes in models properties
(weights, featured fields) are observed. When tested on various propagation
benchmarks, these models systematically returned estimations with a high level
of deviation, especially for the recurrent analysis which strongly amplifies
variability due to the non-determinism. Trainings performed with double
floating-point precision provide slightly better estimations and a significant
reduction of the variability of both the network parameters and its testing
error range.

    ", Machine Learning,recurrent neural networks,On the reproducibility of fully convolutional neural networks for modeling time-space evolving physical systems
"  This paper describes an automatic drum transcription (ADT) method that
directly estimates a tatum-level drum score from a music signal, in contrast to
most conventional ADT methods that estimate the frame-level onset probabilities
of drums. To estimate a tatum-level score, we propose a deep transcription
model that consists of a frame-level encoder for extracting the latent features
from a music signal and a tatum-level decoder for estimating a drum score from
the latent features pooled at the tatum level. To capture the global repetitive
structure of drum scores, which is difficult to learn with a recurrent neural
network (RNN), we introduce a self-attention mechanism with tatum-synchronous
positional encoding into the decoder. To mitigate the difficulty of training
the self-attention-based model from an insufficient amount of paired data and
improve the musical naturalness of the estimated scores, we propose a
regularized training method that uses a global structure-aware masked language
(score) model with a self-attention mechanism pretrained from an extensive
collection of drum scores. Experimental results showed that the proposed
regularized model outperformed the conventional RNN-based model in terms of the
tatum-level error rate and the frame-level F-measure, even when only a limited
amount of paired data was available so that the non-regularized model
underperformed the RNN-based model.

    ", Sound,recurrent neural networks,Global Structure-Aware Drum Transcription Based on Self-Attention Mechanisms
"  A critical challenge for any intelligent system is to infer structure from
continuous data streams. Theories of event-predictive cognition suggest that
the brain segments sensorimotor information into compact event encodings, which
are used to anticipate and interpret environmental dynamics. Here, we introduce
a SUrprise-GAted Recurrent neural network (SUGAR) using a novel form of
counterfactual regularization. We test the model on a hierarchical sequence
prediction task, where sequences are generated by alternating hidden graph
structures. Our model learns to both compress the temporal dynamics of the task
into latent event-predictive encodings and anticipate event transitions at the
right moments, given noisy hidden signals about them. The addition of the
counterfactual regularization term ensures fluid transitions from one latent
code to the next, whereby the resulting latent codes exhibit compositional
properties. The implemented mechanisms offer a host of useful applications in
other domains, including hierarchical reasoning, planning, and decision making.

    ", Machine Learning,recurrent neural networks,Latent Event-Predictive Encodings through Counterfactual Regularization
"  Sequential information contains short- to long-range dependencies; however,
learning long-timescale information has been a challenge for recurrent neural
networks. Despite improvements in long short-term memory networks (LSTMs), the
forgetting mechanism results in the exponential decay of information, limiting
their capacity to capture long-timescale information. Here, we propose a power
law forget gate, which instead learns to forget information along a slower
power law decay function. Specifically, the new gate learns to control the
power law decay factor, p, allowing the network to adjust the information decay
rate according to task demands. Our experiments show that an LSTM with power
law forget gates (pLSTM) can effectively capture long-range dependencies beyond
hundreds of elements on image classification, language modeling, and
categorization tasks, improving performance over the vanilla LSTM. We also
inspected the revised forget gate by varying the initialization of p, setting p
to a fixed value, and ablating cells in the pLSTM network. The results show
that the information decay can be controlled by the learnable decay factor p,
which allows pLSTM to achieve its superior performance. Altogether, we found
that LSTM with the proposed forget gate can learn long-term dependencies,
outperforming other recurrent networks in multiple domains; such gating
mechanism can be integrated into other architectures for improving the learning
of long timescale information in recurrent neural networks.

    ", Machine Learning,recurrent neural networks,Slower is Better: Revisiting the Forgetting Mechanism in LSTM for Slower Information Decay
"  This paper proposes a method for recognizing online handwritten mathematical
expressions (OnHME) by building a symbol relation tree (SRT) directly from a
sequence of strokes. A bidirectional recurrent neural network learns from
multiple derived paths of SRT to predict both symbols and spatial relations
between symbols using global context. The recognition system has two parts: a
temporal classifier and a tree connector. The temporal classifier produces an
SRT by recognizing an OnHME pattern. The tree connector splits the SRT into
several sub-SRTs. The final SRT is formed by looking up the best combination
among those sub-SRTs. Besides, we adopt a tree sorting method to deal with
various stroke orders. Recognition experiments indicate that the proposed OnHME
recognition system is competitive to other methods. The recognition system
achieves 44.12% and 41.76% expression recognition rates on the Competition on
Recognition of Online Handwritten Mathematical Expressions (CROHME) 2014 and
2016 testing sets.

    ", Computer Vision and Pattern Recognition,recurrent neural networks,Learning symbol relation tree for online mathematical expression recognition
"  Sensor drift is a long-existing unpredictable problem that deteriorates the
performance of gaseous substance recognition, calling for an antidrift domain
adaptation algorithm. However, the prerequisite for traditional methods to
achieve fine results is to have data from both nondrift distributions (source
domain) and drift distributions (target domain) for domain alignment, which is
usually unrealistic and unachievable in real-life scenarios. To compensate for
this, in this paper, deep learning based on a target-domain-free domain
adaptation convolutional neural network (TDACNN) is proposed. The main concept
is that CNNs extract not only the domain-specific features of samples but also
the domain-invariant features underlying both the source and target domains.
Making full use of these various levels of embedding features can lead to
comprehensive utilization of different levels of characteristics, thus
achieving drift compensation by the extracted intermediate features between two
domains. In the TDACNN, a flexible multibranch backbone with a multiclassifier
structure is proposed under the guidance of bionics, which utilizes multiple
embedding features comprehensively without involving target domain data during
training. A classifier ensemble method based on maximum mean discrepancy (MMD)
is proposed to evaluate all the classifiers jointly based on the credibility of
the pseudolabel. To optimize network training, an additive angular margin
softmax loss with parameter dynamic adjustment is utilized. Experiments on two
drift datasets under different settings demonstrate the superiority of TDACNN
compared with several state-of-the-art methods.

    ", Quantitative Methods,convolutional neural networks,TDACNN: Target-domain-free Domain Adaptation Convolutional Neural Network for Drift Compensation in Gas Sensors
"  Recent advancements in Graph Neural Networks have led to state-of-the-art
performance on representation learning of graphs for node classification.
However, the majority of existing works process directed graphs by
symmetrization, which may cause loss of directional information. In this paper,
we propose the magnetic Laplacian that preserves edge directionality by
encoding it into complex phase as a deformation of the combinatorial Laplacian.
In addition, we design an Auto-Regressive Moving-Average (ARMA) filter that is
capable of learning global features from graphs. To reduce time complexity,
Taylor expansion is applied to approximate the filter. We derive complex-valued
operations in graph neural network and devise a simplified Magnetic Graph
Convolution network, namely sMGC. Our experiment results demonstrate that sMGC
is a fast, powerful, and widely applicable GNN.

    ", Machine Learning,convolutional neural networks,sMGC: A Complex-Valued Graph Convolutional Network via Magnetic Laplacian for Directed Graphs
"  We present a simple, yet effective, approach for self-supervised 3D human
pose estimation. Unlike the prior work, we explore the temporal information
next to the multi-view self-supervision. During training, we rely on
triangulating 2D body pose estimates of a multiple-view camera system. A
temporal convolutional neural network is trained with the generated 3D
ground-truth and the geometric multi-view consistency loss, imposing
geometrical constraints on the predicted 3D body skeleton. During inference,
our model receives a sequence of 2D body pose estimates from a single-view to
predict the 3D body pose for each of them. An extensive evaluation shows that
our method achieves state-of-the-art performance in the Human3.6M and
MPI-INF-3DHP benchmarks. Our code and models are publicly available at
\url{", Computer Vision and Pattern Recognition,convolutional neural networks,Learning Temporal 3D Human Pose Estimation with Pseudo-Labels
"  This paper investigates unsupervised learning of Full-Waveform Inversion
(FWI), which has been widely used in geophysics to estimate subsurface velocity
maps from seismic data. This problem is mathematically formulated by a second
order partial differential equation (PDE), but is hard to solve. Moreover,
acquiring velocity map is extremely expensive, making it impractical to scale
up a supervised approach to train the mapping from seismic data to velocity
maps with convolutional neural networks (CNN). We address these difficulties by
integrating PDE and CNN in a loop, thus shifting the paradigm to unsupervised
learning that only requires seismic data. In particular, we use finite
difference to approximate the forward modeling of PDE as a differentiable
operator (from velocity map to seismic data) and model its inversion by CNN
(from seismic data to velocity map). Hence, we transform the supervised
inversion task into an unsupervised seismic data reconstruction task. We also
introduce a new large-scale dataset OpenFWI, to establish a more challenging
benchmark for the community. Experiment results show that our model (using
seismic data alone) yields comparable accuracy to the supervised counterpart
(using both seismic data and velocity map). Furthermore, it outperforms the
supervised model when involving more seismic data.

    ", Machine Learning,convolutional neural networks,Unsupervised Learning of Full-Waveform Inversion: Connecting CNN and Partial Differential Equation in a Loop
"  This paper presents the first large-scale multi-species dataset of acoustic
recordings of mosquitoes tracked continuously in free flight. We present 20
hours of audio recordings that we have expertly labelled and tagged precisely
in time. Significantly, 18 hours of recordings contain annotations from 36
different species. Mosquitoes are well-known carriers of diseases such as
malaria, dengue and yellow fever. Collecting this dataset is motivated by the
need to assist applications which utilise mosquito acoustics to conduct surveys
to help predict outbreaks and inform intervention policy. The task of detecting
mosquitoes from the sound of their wingbeats is challenging due to the
difficulty in collecting recordings from realistic scenarios. To address this,
as part of the HumBug project, we conducted global experiments to record
mosquitoes ranging from those bred in culture cages to mosquitoes captured in
the wild. Consequently, the audio recordings vary in signal-to-noise ratio and
contain a broad range of indoor and outdoor background environments from
Tanzania, Thailand, Kenya, the USA and the UK. In this paper we describe in
detail how we collected, labelled and curated the data. The data is provided
from a PostgreSQL database, which contains important metadata such as the
capture method, age, feeding status and gender of the mosquitoes. Additionally,
we provide code to extract features and train Bayesian convolutional neural
networks for two key tasks: the identification of mosquitoes from their
corresponding background environments, and the classification of detected
mosquitoes into species. Our extensive dataset is both challenging to machine
learning researchers focusing on acoustic identification, and critical to
entomologists, geo-spatial modellers and other domain experts to understand
mosquito behaviour, model their distribution, and manage the threat they pose
to humans.

    ", Sound,convolutional neural networks,HumBugDB: A Large-scale Acoustic Mosquito Dataset
"  Q-learning is a popular Reinforcement Learning (RL) algorithm which is widely
used in practice with function approximation (Mnih et al., 2015). In contrast,
existing theoretical results are pessimistic about Q-learning. For example,
(Baird, 1995) shows that Q-learning does not converge even with linear function
approximation for linear MDPs. Furthermore, even for tabular MDPs with
synchronous updates, Q-learning was shown to have sub-optimal sample complexity
(Li et al., 2021;Azar et al., 2013). The goal of this work is to bridge the gap
between practical success of Q-learning and the relatively pessimistic
theoretical results. The starting point of our work is the observation that in
practice, Q-learning is used with two important modifications: (i) training
with two networks, called online network and target network simultaneously
(online target learning, or OTL) , and (ii) experience replay (ER) (Mnih et
al., 2015). While they have been observed to play a significant role in the
practical success of Q-learning, a thorough theoretical understanding of how
these two modifications improve the convergence behavior of Q-learning has been
missing in literature. By carefully combining Q-learning with OTL and reverse
experience replay (RER) (a form of experience replay), we present novel methods
Q-Rex and Q-RexDaRe (Q-Rex + data reuse). We show that Q-Rex efficiently finds
the optimal policy for linear MDPs (or more generally for MDPs with zero
inherent Bellman error with linear approximation (ZIBEL)) and provide
non-asymptotic bounds on sample complexity -- the first such result for a
Q-learning method for this class of MDPs under standard assumptions.
Furthermore, we demonstrate that Q-RexDaRe in fact achieves near optimal sample
complexity in the tabular setting, improving upon the existing results for
vanilla Q-learning.

    ", Machine Learning,reinforcement learning,Online Target Q-learning with Reverse Experience Replay: Efficiently finding the Optimal Policy for Linear MDPs
"  Text-based games (TBG) have emerged as promising environments for driving
research in grounded language understanding and studying problems like
generalization and sample efficiency. Several deep reinforcement learning (RL)
methods with varying architectures and learning schemes have been proposed for
TBGs. However, these methods fail to generalize efficiently, especially under
distributional shifts. In a departure from deep RL approaches, in this paper,
we propose a general method inspired by case-based reasoning to train agents
and generalize out of the training distribution. The case-based reasoner
collects instances of positive experiences from the agent's interaction with
the world in the past and later reuses the collected experiences to act
efficiently. The method can be applied in conjunction with any existing
on-policy neural agent in the literature for TBGs. Our experiments show that
the proposed approach consistently improves existing methods, obtains good
out-of-distribution generalization, and achieves new state-of-the-art results
on widely used environments.

    ", Computation and Language,reinforcement learning,Case-based Reasoning for Better Generalization in Text-Adventure Games
"  Modern communications are usually designed to pursue a higher bit-level
precision and fewer bits required to transmit a message. This article rethinks
these two major features and introduces the concept and advantage of semantics
that characterizes a new kind of semantics-aware communication mechanism,
incorporating both the semantic encoding and the semantic communication
problem. Within the unified framework, we analyze the underlying defects of
existing semantics-aware techniques and establish a confidence-based
distillation mechanism for the joint semantics-noise coding (JSNC) problem, and
a reinforcement learning (RL)-powered semantic communication paradigm that
endows a system the ability to convey the semantics instead of pursuing the bit
level accuracy. On top of these technical contributions, this work provides a
new insight to understand how the semantics are processed and represented in a
semantics-aware coding and communication system, and verifies the significant
benefits of doing so.

    ", Signal Processing,reinforcement learning,Rethinking Modern Communication from Semantic Coding to Semantic Communication
"  In multi-agent reinforcement learning (MARL), it is challenging for a
collection of agents to learn complex temporally extended tasks. The
difficulties lie in computational complexity and how to learn the high-level
ideas behind reward functions. We study the graph-based Markov Decision Process
(MDP) where the dynamics of neighboring agents are coupled. We use a reward
machine (RM) to encode each agent's task and expose reward function internal
structures. RM has the capacity to describe high-level knowledge and encode
non-Markovian reward functions. We propose a decentralized learning algorithm
to tackle computational complexity, called decentralized graph-based
reinforcement learning using reward machines (DGRM), that equips each agent
with a localized policy, allowing agents to make decisions independently, based
on the information available to the agents. DGRM uses the actor-critic
structure, and we introduce the tabular Q-function for discrete state problems.
We show that the dependency of Q-function on other agents decreases
exponentially as the distance between them increases. Furthermore, the
complexity of DGRM is related to the local information size of the largest
$\kappa$-hop neighborhood, and DGRM can find an
$O(\rho^{\kappa+1})$-approximation of a stationary point of the objective
function. To further improve efficiency, we also propose the deep DGRM
algorithm, using deep neural networks to approximate the Q-function and policy
function to solve large-scale or continuous state problems. The effectiveness
of the proposed DGRM algorithm is evaluated by two case studies, UAV package
delivery and COVID-19 pandemic mitigation. Experimental results show that local
information is sufficient for DGRM and agents can accomplish complex tasks with
the help of RM. DGRM improves the global accumulated reward by 119% compared to
the baseline in the case of COVID-19 pandemic mitigation.

    ", Multiagent Systems,reinforcement learning,Decentralized Graph-Based Multi-Agent Reinforcement Learning Using Reward Machines
"  In human pedagogy, teachers and students can interact adaptively to maximize
communication efficiency. The teacher adjusts her teaching method for different
students, and the student, after getting familiar with the teacher's
instruction mechanism, can infer the teacher's intention to learn faster.
Recently, the benefits of integrating this cooperative pedagogy into machine
concept learning in discrete spaces have been proved by multiple works.
However, how cooperative pedagogy can facilitate machine parameter learning
hasn't been thoroughly studied. In this paper, we propose a gradient
optimization based teacher-aware learner who can incorporate teacher's
cooperative intention into the likelihood function and learn provably faster
compared with the naive learning algorithms used in previous machine teaching
works. We give theoretical proof that the iterative teacher-aware learning
(ITAL) process leads to local and global improvements. We then validate our
algorithms with extensive experiments on various tasks including regression,
classification, and inverse reinforcement learning using synthetic and real
data. We also show the advantage of modeling teacher-awareness when agents are
learning from human teachers.

    ", Machine Learning,reinforcement learning,Iterative Teacher-Aware Learning
"  Sarcasm is the use of words usually used to either mock or annoy someone, or
for humorous purposes. Sarcasm is largely used in social networks and
microblogging websites, where people mock or censure in a way that makes it
difficult even for humans to tell if what is said is what is meant. Failure to
identify sarcastic utterances in Natural Language Processing applications such
as sentiment analysis and opinion mining will confuse classification algorithms
and generate false results. Several studies on sarcasm detection have utilized
different learning algorithms. However, most of these learning models have
always focused on the contents of expression only, leaving the contextual
information in isolation. As a result, they failed to capture the contextual
information in the sarcastic expression. Moreover, some datasets used in
several studies have an unbalanced dataset which impacting the model result. In
this paper, we propose a contextual model for sarcasm identification in twitter
using RoBERTa, and augmenting the dataset by applying Global Vector
representation (GloVe) for the construction of word embedding and context
learning to generate more data and balancing the dataset. The effectiveness of
this technique is tested with various datasets and data augmentation settings.
In particular, we achieve performance gain by 3.2% in the iSarcasm dataset when
using data augmentation to increase 20% of data labeled as sarcastic, resulting
F-score of 40.4% compared to 37.2% without data augmentation.

    ", Computation and Language,natural language processing,Sarcasm Detection in Twitter -- Performance Impact while using Data Augmentation: Word Embeddings
"  Facial action unit (FAU) intensities are popular descriptors for the analysis
of facial behavior. However, FAUs are sparsely represented when only a few are
activated at a time. In this study, we explore the possibility of representing
the dynamics of facial expressions by adopting algorithms used for word
representation in natural language processing. Specifically, we perform
clustering on a large dataset of temporal facial expressions with 5.3M frames
before applying the Global Vector representation (GloVe) algorithm to learn the
embeddings of the facial clusters. We evaluate the usefulness of our learned
representations on two downstream tasks: schizophrenia symptom estimation and
depression severity regression. These experimental results show the potential
effectiveness of our approach for improving the assessment of mental health
symptoms over baseline models that use FAU intensities alone.

    ", Computer Vision and Pattern Recognition,natural language processing,Modeling Dynamics of Facial Behavior for Mental Health Assessment
"  The Bidirectional Encoder Representations from Transformers (BERT) model has
been radically improving the performance of many Natural Language Processing
(NLP) tasks such as Text Classification and Named Entity Recognition (NER)
applications. However, it is challenging to scale BERT for low-latency and
high-throughput industrial use cases due to its enormous size. We successfully
optimize a Query-Title Relevance (QTR) classifier for deployment via a compact
model, which we name BERT Bidirectional Long Short-Term Memory (BertBiLSTM).
The model is capable of inferring an input in at most 0.2ms on CPU. BertBiLSTM
exceeds the off-the-shelf BERT model's performance in terms of accuracy and
efficiency for the aforementioned real-world production task. We achieve this
result in two phases. First, we create a pre-trained model, called eBERT, which
is the original BERT architecture trained with our unique item title corpus. We
then fine-tune eBERT for the QTR task. Second, we train the BertBiLSTM model to
mimic the eBERT model's performance through a process called Knowledge
Distillation (KD) and show the effect of data augmentation to achieve the
resembling goal. Experimental results show that the proposed model outperforms
other compact and production-ready models.

    ", Computation and Language,natural language processing,Deploying a BERT-based Query-Title Relevance Classifier in a Production System: a View from the Trenches
"  In the field of inorganic materials science, there is a growing demand to
extract knowledge such as physical properties and synthesis processes of
materials by machine-reading a large number of papers. This is because
materials researchers refer to many papers in order to come up with promising
terms of experiments for material synthesis. However, there are only a few
systems that can extract material names and their properties. This study
proposes a large-scale natural language processing (NLP) pipeline for
extracting material names and properties from materials science literature to
enable the search and retrieval of results in materials science. Therefore, we
propose a label definition for extracting material names and properties and
accordingly build a corpus containing 836 annotated paragraphs extracted from
301 papers for training a named entity recognition (NER) model. Experimental
results demonstrate the utility of this NER model; it achieves successful
extraction with a micro-F1 score of 78.1%. To demonstrate the efficacy of our
approach, we present a thorough evaluation on a real-world automatically
annotated corpus by applying our trained NER model to 12,895 materials science
papers. We analyze the trend in materials science by visualizing the outputs of
the NLP pipeline. For example, the country-by-year analysis indicates that in
recent years, the number of papers on ""MoS2,"" a material used in perovskite
solar cells, has been increasing rapidly in China but decreasing in the United
States. Further, according to the conditions-by-year analysis, the processing
temperature of the catalyst material ""PEDOT:PSS"" is shifting below 200 degree,
and the number of reports with a processing time exceeding 5 h is increasing
slightly.

    ", Computation and Language,natural language processing,Analyzing Research Trends in Inorganic Materials Literature Using NLP
"  Recognizing causal elements and causal relations in text is one of the
challenging issues in natural language processing; specifically, in low
resource languages such as Persian. In this research we prepare a causality
human annotated corpus for the Persian language which consists of 4446
sentences and 5128 causal relations and three labels of cause, effect and
causal mark -- if possibl -- are specified for each relation. We have used this
corpus to train a system for detecting causal elements boundaries. Also, we
present a causality detection benchmark for three machine learning methods and
two deep learning systems based on this corpus. Performance evaluations
indicate that our best total result is obtained through CRF classifier which
has F-measure of 0.76 and the best accuracy obtained through Bi-LSTM-CRF deep
learning method with Accuracy equal to %91.4.

    ", Computation and Language,natural language processing,Persian Causality Corpus (PerCause) and the Causality Detection Benchmark
"  Person re-identification (ReID) aims to re-identify a person from
non-overlapping camera views. Since person ReID data contains sensitive
personal information, researchers have adopted federated learning, an emerging
distributed training method, to mitigate the privacy leakage risks. However,
existing studies rely on data labels that are laborious and time-consuming to
obtain. We present FedUReID, a federated unsupervised person ReID system to
learn person ReID models without any labels while preserving privacy. FedUReID
enables in-situ model training on edges with unlabeled data. A cloud server
aggregates models from edges instead of centralizing raw data to preserve data
privacy. Moreover, to tackle the problem that edges vary in data volumes and
distributions, we personalize training in edges with joint optimization of
cloud and edge. Specifically, we propose personalized epoch to reassign
computation throughout training, personalized clustering to iteratively predict
suitable labels for unlabeled data, and personalized update to adapt the server
aggregated model to each edge. Extensive experiments on eight person ReID
datasets demonstrate that FedUReID not only achieves higher accuracy but also
reduces computation cost by 29%. Our FedUReID system with the joint
optimization will shed light on implementing federated learning to more
multimedia tasks without data labels.

    "," Distributed, Parallel, and Cluster Computing",unsupervised learning,Joint Optimization in Edge-Cloud Continuum for Federated Unsupervised Person Re-identification
"  Continual Learning (CL) investigates how to train Deep Networks on a stream
of tasks without incurring catastrophic forgetting. CL settings proposed in the
literature assume that every incoming example is paired with ground-truth
annotations. However, this clashes with many real-world applications: gathering
labeled data, which is in itself tedious and expensive, becomes indeed
infeasible when data flow as a stream and must be consumed in real-time. This
work explores Weakly Supervised Continual Learning (WSCL): here, only a small
fraction of labeled input examples are shown to the learner. We assess how
current CL methods (e.g.: EWC, LwF, iCaRL, ER, GDumb, DER) perform in this
novel and challenging scenario, in which overfitting entangles forgetting.
Subsequently, we design two novel WSCL methods which exploit metric learning
and consistency regularization to leverage unsupervised data while learning. In
doing so, we show that not only our proposals exhibit higher flexibility when
supervised information is scarce, but also that less than 25% labels can be
enough to reach or even outperform SOTA methods trained under full supervision.

    ", Machine Learning,unsupervised learning,Weakly Supervised Continual Learning
"  Disentangled visual representations have largely been studied with generative
models such as Variational AutoEncoders (VAEs). While prior work has focused on
generative methods for disentangled representation learning, these approaches
do not scale to large datasets due to current limitations of generative models.
Instead, we explore regularization methods with contrastive learning, which
could result in disentangled representations that are powerful enough for large
scale datasets and downstream applications. However, we find that unsupervised
disentanglement is difficult to achieve due to optimization and initialization
sensitivity, with trade-offs in task performance. We evaluate disentanglement
with downstream tasks, analyze the benefits and disadvantages of each
regularization used, and discuss future directions.

    ", Computer Vision and Pattern Recognition,unsupervised learning,Unsupervised Disentanglement without Autoencoding: Pitfalls and Future Directions
"  This paper investigates the problem of reconstructing hyperspectral (HS)
images from single RGB images captured by commercial cameras, \textbf{without}
using paired HS and RGB images during training. To tackle this challenge, we
propose a new lightweight and end-to-end learning-based framework.
Specifically, on the basis of the intrinsic imaging degradation model of RGB
images from HS images, we progressively spread the differences between input
RGB images and re-projected RGB images from recovered HS images via effective
unsupervised camera spectral response function estimation. To enable the
learning without paired ground-truth HS images as supervision, we adopt the
adversarial learning manner and boost it with a simple yet effective
$\mathcal{L}_1$ gradient clipping scheme. Besides, we embed the semantic
information of input RGB images to locally regularize the unsupervised
learning, which is expected to promote pixels with identical semantics to have
consistent spectral signatures. In addition to conducting quantitative
experiments over two widely-used datasets for HS image reconstruction from
synthetic RGB images, we also evaluate our method by applying recovered HS
images from real RGB images to HS-based visual tracking. Extensive results show
that our method significantly outperforms state-of-the-art unsupervised methods
and even exceeds the latest supervised method under some settings. The source
code is public available at
", Computer Vision and Pattern Recognition,unsupervised learning,Semantic-embedded Unsupervised Spectral Reconstruction from Single RGB Images in the Wild
"  Most publicly available datasets for image classification are with single
labels, while images are inherently multi-labeled in our daily life. Such an
annotation gap makes many pre-trained single-label classification models fail
in practical scenarios. This annotation issue is more concerned for aerial
images: Aerial data collected from sensors naturally cover a relatively large
land area with multiple labels, while annotated aerial datasets, which are
publicly available (e.g., UCM, AID), are single-labeled. As manually annotating
multi-label aerial images would be time/labor-consuming, we propose a novel
self-correction integrated domain adaptation (SCIDA) method for automatic
multi-label learning. SCIDA is weakly supervised, i.e., automatically learning
the multi-label image classification model from using massive, publicly
available single-label images. To achieve this goal, we propose a novel
Label-Wise self-Correction (LWC) module to better explore underlying label
correlations. This module also makes the unsupervised domain adaptation (UDA)
from single- to multi-label data possible. For model training, the proposed
model only uses single-label information yet requires no prior knowledge of
multi-labeled data; and it predicts labels for multi-label aerial images. In
our experiments, trained with single-labeled MAI-AID-s and MAI-UCM-s datasets,
the proposed model is tested directly on our collected Multi-scene Aerial Image
(MAI) dataset.

    ", Computer Vision and Pattern Recognition,unsupervised learning,SCIDA: Self-Correction Integrated Domain Adaptation from Single- to Multi-label Aerial Images
"  We establish a connection between knot theory and cluster algebras via
representation theory. To every knot diagram (or link diagram), we associate a
cluster algebra by constructing a quiver with potential. The rank of the
cluster algebra is $2n$, where $n$ is the number of crossing points in the knot
diagram. We then construct $2n$ indecomposable modules $T(i)$ over the Jacobian
algebra of the quiver with potential. For each $T(i)$, we show that the
submodule lattice is isomorphic to the corresponding lattice of Kauffman
states. We then give a realization of the Alexander polynomial of the knot as a
specialization of the $F$-polynomial of $T(i)$, for every $i$. Furthermore, we
conjecture that the collection of the $T(i)$ forms a cluster in the cluster
algebra whose quiver is isomorphic to the opposite of the initial quiver, and
that the resulting cluster automorphism is of order two.

    ", Representation Theory,clustering,Knot theory and cluster algebras
"  The $\beta$-exponential inflation is driven by a class of primordial
potentials, derived in the framework of braneworld scenarios, that generalizes
the well-known power law inflation. In this paper we update previous
constraints on the minimal coupled $\beta$-exponential model [1] and extend the
results also deriving the equations for the non-minimal coupled scenario. The
predictions of both models are tested in light of the latest temperature and
polarization maps of the Cosmic Microwave Background and clustering data. We
also compare the predictions of these models with the standard $\Lambda$CDM
model using the deviance information criterion (DIC), and find that the
observational data show a moderate preference for the non-minimally coupled
$\beta$-exponential inflationary model.

    ", Cosmology and Nongalactic Astrophysics,clustering,Constraining non-minimally coupled $Œ≤$-exponential inflation with CMB data
"  Minimizing a sum of simple submodular functions of limited support is a
special case of general submodular function minimization that has seen numerous
applications in machine learning. We develop fast techniques for instances
where components in the sum are cardinality-based, meaning they depend only on
the size of the input set. This variant is one of the most widely applied in
practice, encompassing, e.g., common energy functions arising in image
segmentation and recent generalized hypergraph cut functions. We develop the
first approximation algorithms for this problem, where the approximations can
be quickly computed via reduction to a sparse graph cut problem, with graph
sparsity controlled by the desired approximation factor. Our method relies on a
new connection between sparse graph reduction techniques and piecewise linear
approximations to concave functions. Our sparse reduction technique leads to
significant improvements in theoretical runtimes, as well as substantial
practical gains in problems ranging from benchmark image segmentation tasks to
hypergraph clustering problems.

    ", Machine Learning,clustering,Approximate Decomposable Submodular Function Minimization for Cardinality-Based Components
"  There is a known tension between cosmological parameter constraints obtained
from the primary CMB and those from galaxy cluster samples. One possible
explanation could be certain types of groups or clusters of galaxy have been
missed in the past. We aim to search for galaxy groups and clusters with
particularly extended surface brightness distributions, by creating a new X-ray
selected catalog of extended galaxy clusters from the ROSAT All-Sky Survey,
using a dedicated algorithm optimized for extended sources. Through extensive
simulations, the detection efficiency and sample purity are investigated.
Previous cluster catalogs in X-ray and other wave-bands, as well as
spectroscopic and photometric redshifts of galaxies are used for the cluster
identification. We report a catalog of galaxy clusters at high galactic
latitude based on the ROSAT All-sky Survey, named as RXGCC, which includes 944
groups and clusters. Out of it, 641 clusters have been identified through ICM
emission previously (Bronze), 154 known optical and infrared clusters are
detected as X-ray clusters for the first time (Silver), and 149 identified as
clusters for the first time (Gold). Based on 200 simulations, the contamination
ratio of the detections which were identified as clusters by ICM emission, and
the detections which were identified as optical and infrared clusters in
previous work is 0.008 and 0.100, respectively. Compared with Bronze sample,
the Gold+Silver sample is less luminous, less massive, and has a flatter
surface brightness profile. Specifically, the median flux in [0.1-2.4]keV band
for Gold+Silver and Bronze sample is 2.496e-12 erg/s/cm^2 and 4.955e-12
erg/s/cm^2, respectively. The median slope of cluster profile is 0.76 and 0.83
for Gold+Silver and Bronze sample, respectively. This whole sample is available
at ", Cosmology and Nongalactic Astrophysics,clustering,An X-ray selected catalog of extended galaxy clusters from the ROSAT All-Sky Survey (RXGCC)
"  CONCERTO (CarbON CII line in post-rEionisation and ReionisaTiOn) is a large
field-of-view (FoV) spectro-imager that has been installed on the Cassegrain
Cabin of Atacama Pathfinder EXperiment (APEX) telescope in April 2021. CONCERTO
hosts 2 focal planes and a total number of 4000 Kinetic Inductance Detectors
(KID), with an instantaneous FoV of 18.6 arcminutes in the range of 130-310
GHz. The spectral resolution can be easily tuned down to 1 GHz depending on the
scientific target. The scientific program of CONCERTO has many objectives, with
two main programs focused on mapping the fluctuations of the [CII] line
intensity in the reionisation and post-reionisation epoch (4.5<z<8.5), and on
studying galaxy clusters via the thermal and kinetic Sunyaev-Zel'dovich (SZ)
effect. CONCERTO will also measure the dust and molecular gas contents of local
and intermediate-redshift galaxies, it will study the Galactic star-forming
clouds and finally it will observe the CO intensity fluctuations arising from
0.3<z<2 galaxies. The design of the instrument, installation at APEX and
current status of the commissioning phase and science verification will be
presented. Also we describe the deployment and first on-sky tests performed
between April and June 2021.

    ", Instrumentation and Methods for Astrophysics,clustering,CONCERTO at APEX: Installation and first phase of on-sky commissioning
"  Recently, surrogate models based on deep learning have attracted much
attention for engineering analysis and optimization. As the construction of
data pairs in most engineering problems is time-consuming, data acquisition is
becoming the predictive capability bottleneck of most deep surrogate models,
which also exists in surrogate for thermal analysis and design. To address this
issue, this paper develops a physics-informed convolutional neural network
(CNN) for the thermal simulation surrogate. The network can learn a mapping
from heat source layout to the steady-state temperature field without labeled
data, which equals solving an entire family of partial difference equations
(PDEs). To realize the physics-guided training without labeled data, we employ
the heat conduction equation and finite difference method to construct the loss
function. Since the solution is sensitive to boundary conditions, we properly
impose hard constraints by padding in the Dirichlet and Neumann boundary
conditions. In addition, the neural network architecture is well-designed to
improve the prediction precision of the problem at hand, and pixel-level online
hard example mining is introduced to overcome the imbalance of optimization
difficulty in the computation domain. The experiments demonstrate that the
proposed method can provide comparable predictions with numerical method and
data-driven deep learning models. We also conduct various ablation studies to
investigate the effectiveness of the network component and training methods
proposed in this paper.

    ", Machine Learning,convolutional neural networks,Physics-informed Convolutional Neural Networks for Temperature Field Prediction of Heat Source Layout without Labeled Data
"  In this paper, we propose a new approach, hereafter referred as AdaInject,
for the gradient descent optimizers by injecting the curvature information with
adaptive momentum. Specifically, the curvature information is used as a weight
to inject the second order moment in the update rule. The curvature information
is captured through the short-term parameter history. The AdaInject approach
boosts the parameter update by exploiting the curvature information. The
proposed approach is generic in nature and can be integrated with any existing
adaptive momentum stochastic gradient descent optimizers. The effectiveness of
the AdaInject optimizer is tested using a theoretical analysis as well as
through toy examples. We also show the convergence property of the proposed
injection based optimizer. Further, we depict the efficacy of the AdaInject
approach through extensive experiments in conjunction with the state-of-the-art
optimizers, i.e., AdamInject, diffGradInject, RadamInject, and AdaBeliefInject
on four benchmark datasets. Different CNN models are used in the experiments. A
highest improvement in the top-1 classification error rate of $16.54\%$ is
observed using diffGradInject optimizer with ResNeXt29 model over the CIFAR10
dataset. Overall, we observe very promising performance improvement of existing
optimizers with the proposed AdaInject approach.

    ", Machine Learning,convolutional neural networks,Curvature Injected Adaptive Momentum Optimizer for Convolutional Neural Networks
"  Radiation therapy (RT) is widely employed in the clinic for the treatment of
head and neck (HaN) cancers. An essential step of RT planning is the accurate
segmentation of various organs-at-risks (OARs) in HaN CT images. Nevertheless,
segmenting OARs manually is time-consuming, tedious, and error-prone
considering that typical HaN CT images contain tens to hundreds of slices.
Automated segmentation algorithms are urgently required. Recently,
convolutional neural networks (CNNs) have been extensively investigated on this
task. Particularly, 3D CNNs are frequently adopted to process 3D HaN CT images.
There are two issues with na√Øve 3D CNNs. First, the depth resolution of 3D CT
images is usually several times lower than the in-plane resolution. Direct
employment of 3D CNNs without distinguishing this difference can lead to the
extraction of distorted image features and influence the final segmentation
performance. Second, a severe class imbalance problem exists, and large organs
can be orders of times larger than small organs. It is difficult to
simultaneously achieve accurate segmentation for all the organs. To address
these issues, we propose a novel hybrid CNN that fuses 2D and 3D convolutions
to combat the different spatial resolutions and extract effective edge and
semantic features from 3D HaN CT images. To accommodate large and small organs,
our final model, named OrganNet2.5D, consists of only two instead of the
classic four downsampling operations, and hybrid dilated convolutions are
introduced to maintain the respective field. Experiments on the MICCAI 2015
challenge dataset demonstrate that OrganNet2.5D achieves promising performance
compared to state-of-the-art methods.

    ", Image and Video Processing,convolutional neural networks,A Novel Hybrid Convolutional Neural Network for Accurate Organ Segmentation in 3D Head and Neck CT Images
"  Convolutional Neural Networks (CNNs) are commonly designed for closed set
arrangements, where test instances only belong to some ""Known Known"" (KK)
classes used in training. As such, they predict a class label for a test sample
based on the distribution of the KK classes. However, when used under the Open
Set Recognition (OSR) setup (where an input may belong to an ""Unknown Unknown""
or UU class), such a network will always classify a test instance as one of the
KK classes even if it is from a UU class. As a solution, recently, data
augmentation based on Generative Adversarial Networks(GAN) has been used. In
this work, we propose a novel approach for mining a ""Known UnknownTrainer"" or
KUT set and design a deep OSR Network (OSRNet) to harness this dataset. The
goal isto teach OSRNet the essence of the UUs through KUT set, which is
effectively a collection of mined ""hard Known Unknown negatives"". Once trained,
OSRNet can detect the UUs while maintaining high classification accuracy on
KKs. We evaluate OSRNet on six benchmark datasets and demonstrate it
outperforms contemporary OSR methods.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,A novel network training approach for open set image recognition
"  Forecasting the trajectories of neighbor vehicles is a crucial step for
decision making and motion planning of autonomous vehicles. This paper proposes
a graph-based spatial-temporal convolutional network (GSTCN) to predict future
trajectory distributions of all neighbor vehicles using past trajectories. This
network tackles the spatial interactions using a graph convolutional network
(GCN), and captures the temporal features with a convolutional neural network
(CNN). The spatial-temporal features are encoded and decoded by a gated
recurrent unit (GRU) network to generate future trajectory distributions.
Besides, we propose a weighted adjacency matrix to describe the intensities of
mutual influence between vehicles, and the ablation study demonstrates the
effectiveness of our proposed scheme. Our network is evaluated on two
real-world freeway trajectory datasets: I-80 and US-101 in the Next Generation
Simulation (NGSIM).Comparisons in three aspects, including prediction errors,
model sizes, and inference speeds, show that our network can achieve
state-of-the-art performance.

    ", Machine Learning,convolutional neural networks,Graph-Based Spatial-Temporal Convolutional Network for Vehicle Trajectory Prediction in Autonomous Driving
"  Echo State Networks (ESNs) are a special type of recurrent neural networks
(RNNs), in which the input and recurrent connections are traditionally
generated randomly, and only the output weights are trained. Despite the recent
success of ESNs in various tasks of audio, image and radar recognition, we
postulate that a purely random initialization is not the ideal way of
initializing ESNs. The aim of this work is to propose an unsupervised
initialization of the input connections using the K-Means algorithm on the
training data. We show that for a large variety of datasets this initialization
performs equivalently or superior than a randomly initialized ESN whilst
needing significantly less reservoir neurons. Furthermore, we discuss that this
approach provides the opportunity to estimate a suitable size of the reservoir
based on prior knowledge about the data.

    ", Machine Learning,recurrent neural networks,Cluster-based Input Weight Initialization for Echo State Networks
"  We present in this paper an ultra-low power (ULP) Recurrent Neural Network
(RNN) based classifier for an always-on voice Wake-Up Sensor (WUS) with
performances suitable for real-world applications. The purpose of our sensor is
to bring down by at least a factor 100 the power consumption in background
noise of always-on speech processing algorithms such as Automatic Speech
Recognition, Keyword Spotting, Speaker Verification, etc. Unlike the other
published approaches, we designed our wake-up sensor to be robust to unseen
real-world noises for realistic levels of speech and noise by carefully
designing the dataset and the loss function. We also specifically trained it to
mark only the speech start rather than adopting a traditional Voice Activity
Detection (VAD) approach. We achieve less than 3% No Trigger Rate (NTR) for a
duty cycle less than 1% in challenging background noises pooled using a model
of an analogue front-end. We demonstrate the superiority of RNNs on this task
compared to the other tested approaches, with an estimated power consumption of
45 nW for the RNN itself in 65nm CMOS and a minimal memory footprint of 0.52
kB.

    ", Audio and Speech Processing,recurrent neural networks,An Ultra-low Power RNN Classifier for Always-On Voice Wake-Up Detection Robust to Real-World Scenarios
"  Recently Convolutional Neural Networks have been proposed for Sequence
Modelling tasks such as Image Caption Generation. However, unlike Recurrent
Neural Networks, the performance of Convolutional Neural Networks as Decoders
for Image Caption Generation has not been extensively studied. In this work, we
analyse various aspects of Convolutional Neural Network based Decoders such as
Network complexity and depth, use of Data Augmentation, Attention mechanism,
length of sentences used during training, etc on performance of the model. We
perform experiments using Flickr8k and Flickr30k image captioning datasets and
observe that unlike Recurrent Neural Network based Decoder, Convolutional
Decoder for Image Captioning does not generally benefit from increase in
network depth, in the form of stacked Convolutional Layers, and also the use of
Data Augmentation techniques. In addition, use of Attention mechanism also
provides limited performance gains with Convolutional Decoder. Furthermore, we
observe that Convolutional Decoders show performance comparable with Recurrent
Decoders only when trained using sentences of smaller length which contain up
to 15 words but they have limitations when trained using higher sentence
lengths which suggests that Convolutional Decoders may not be able to model
long-term dependencies efficiently. In addition, the Convolutional Decoder
usually performs poorly on CIDEr evaluation metric as compared to Recurrent
Decoder.

    ", Computer Vision and Pattern Recognition,recurrent neural networks,Analysis of Convolutional Decoder for Image Caption Generation
"  Fuzzy Cognitive Maps (FCMs) are considered a soft computing technique
combining elements of fuzzy logic and recurrent neural networks. They found
multiple application in such domains as modeling of system behavior, prediction
of time series, decision making and process control. Less attention, however,
has been turned towards using them in pattern classification. In this work we
propose an FCM based classifier with a fully connected map structure. In
contrast to methods that expect reaching a steady system state during
reasoning, we chose to execute a few FCM iterations (steps) before collecting
output labels. Weights were learned with a gradient algorithm and logloss or
cross-entropy were used as the cost function. Our primary goal was to verify,
whether such design would result in a descent general purpose classifier, with
performance comparable to off the shelf classical methods. As the preliminary
results were promising, we investigated the hypothesis that the performance of
$d$-step classifier can be attributed to a fact that in previous $d-1$ steps it
transforms the feature space by grouping observations belonging to a given
class, so that they became more compact and separable. To verify this
hypothesis we calculated three clustering scores for the transformed feature
space. We also evaluated performance of pipelines built from FCM-based data
transformer followed by a classification algorithm. The standard statistical
analyzes confirmed both the performance of FCM based classifier and its
capability to improve data. The supporting prototype software was implemented
in Python using TensorFlow library.

    ", Machine Learning,recurrent neural networks,Classification and Feature Transformation with Fuzzy Cognitive Maps
"  Human movement disorders or paralysis lead to the loss of control of muscle
activation and thus motor control. Functional Electrical Stimulation (FES) is
an established and safe technique for contracting muscles by stimulating the
skin above a muscle to induce its contraction. However, an open challenge
remains on how to restore motor abilities to human limbs through FES, as the
problem of controlling the stimulation is unclear. We are taking a robotics
perspective on this problem, by developing robot learning algorithms that
control the ultimate humanoid robot, the human body, through electrical muscle
stimulation. Human muscles are not trivial to control as actuators due to their
force production being non-stationary as a result of fatigue and other internal
state changes, in contrast to robot actuators which are well-understood and
stationary over broad operation ranges. We present our Deep Reinforcement
Learning approach to the control of human muscles with FES, using a recurrent
neural network for dynamic state representation, to overcome the unobserved
elements of the behaviour of human muscles under external stimulation. We
demonstrate our technique both in neuromuscular simulations but also
experimentally on a human. Our results show that our controller can learn to
manipulate human muscles, applying appropriate levels of stimulation to achieve
the given tasks while compensating for advancing muscle fatigue which arises
throughout the tasks. Additionally, our technique can learn quickly enough to
be implemented in real-world human-in-the-loop settings.

    ", Robotics,recurrent neural networks,I am Robot: Neuromuscular Reinforcement Learning to Actuate Human Limbs through Functional Electrical Stimulation
"  In this paper, we propose an architecture to solve a novel problem statement
that has stemmed more so in recent times with an increase in demand for virtual
content delivery due to the COVID-19 pandemic. All educational institutions,
workplaces, research centers, etc. are trying to bridge the gap of
communication during these socially distanced times with the use of online
content delivery. The trend now is to create presentations, and then
subsequently deliver the same using various virtual meeting platforms. The time
being spent in such creation of presentations and delivering is what we try to
reduce and eliminate through this paper which aims to use Machine Learning (ML)
algorithms and Natural Language Processing (NLP) modules to automate the
process of creating a slides-based presentation from a document, and then use
state-of-the-art voice cloning models to deliver the content in the desired
author's voice. We consider a structured document such as a research paper to
be the content that has to be presented. The research paper is first summarized
using BERT summarization techniques and condensed into bullet points that go
into the slides. Tacotron inspired architecture with Encoder, Synthesizer, and
a Generative Adversarial Network (GAN) based vocoder, is used to convey the
contents of the slides in the author's voice (or any customized voice). Almost
all learning has now been shifted to online mode, and professionals are now
working from the comfort of their homes. Due to the current situation, teachers
and professionals have shifted to presentations to help them in imparting
information. In this paper, we aim to reduce the considerable amount of time
that is taken in creating a presentation by automating this process and
subsequently delivering this presentation in a customized voice, using a
content delivery mechanism that can clone any voice using a short audio clip.

    ", Machine Learning,natural language processing,AI based Presentation Creator With Customized Audio Content Delivery
"  In large technology companies, the requirements for managing and organizing
technical documents created by engineers and managers in supporting relevant
decision making have increased dramatically in recent years, which has led to a
higher demand for more scalable, accurate, and automated document
classification. Prior studies have only focused on processing text for
classification, whereas technical documents often contain multimodal
information. This paper presents a novel multimodal deep learning architecture,
TechDoc, for technical document classification, which utilizes three types of
information, including natural language texts and descriptive images within
documents and the associations among the documents. The architecture
synthesizes the convolutional neural network, recurrent neural network, and
graph neural network through an integrated multimodal training process. We
applied the architecture to a large multimodal technical document database and
trained the model for classifying documents based on the hierarchical
International Patent Classification system. Our results show that TechDoc
presents a greater classification accuracy than the unimodal methods and other
state-of-the-art methods.

    ", Machine Learning,natural language processing,Deep Learning for Technical Document Classification
"  Forming and interpreting abstraction is a core process in human
communication. In particular, when giving and performing complex instructions
stated in natural language (NL), people may naturally evoke abstract constructs
such as objects, loops, conditions and functions to convey their intentions in
an efficient and precise way. Yet, interpreting and grounding abstraction
stated in NL has not been systematically studied in NLP/AI. To elicit
naturally-occurring abstractions in NL we develop the Hexagons referential
game, where players describe increasingly complex images on a two-dimensional
Hexagons board, and other players need to follow these instructions to recreate
the images. Using this game we collected the Hexagons dataset, which consists
of 164 images and over 3000 naturally-occurring instructions, rich with diverse
abstractions. Results of our baseline models on an instruction-to-execution
task derived from the Hexagons dataset confirm that higher-level abstractions
in NL are indeed more challenging for current systems to process. Thus, this
dataset exposes a new and challenging dimension for grounded semantic parsing,
and we propose it for the community as a future benchmark to explore more
sophisticated and high-level communication within NLP applications.

    ", Computation and Language,natural language processing,Draw Me a Flower: Grounding Formal Abstract Structures Stated in Informal Natural Language
"  Extracting structured clinical information from free-text radiology reports
can enable the use of radiology report information for a variety of critical
healthcare applications. In our work, we present RadGraph, a dataset of
entities and relations in full-text chest X-ray radiology reports based on a
novel information extraction schema we designed to structure radiology reports.
We release a development dataset, which contains board-certified radiologist
annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579
entities and 10,889 relations), and a test dataset, which contains two
independent sets of board-certified radiologist annotations for 100 radiology
reports split equally across the MIMIC-CXR and CheXpert datasets. Using these
datasets, we train and test a deep learning model, RadGraph Benchmark, that
achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR
and CheXpert test sets respectively. Additionally, we release an inference
dataset, which contains annotations automatically generated by RadGraph
Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4
million relations) and 500 CheXpert reports (13,783 entities and 9,908
relations) with mappings to associated chest radiographs. Our freely available
dataset can facilitate a wide range of research in medical natural language
processing, as well as computer vision and multi-modal learning when linked to
chest radiographs.

    ", Computation and Language,natural language processing,RadGraph: Extracting Clinical Entities and Relations from Radiology Reports
"  In offline reinforcement learning (offline RL), one of the main challenges is
to deal with the distributional shift between the learning policy and the given
dataset. To address this problem, recent offline RL methods attempt to
introduce conservatism bias to encourage learning in high-confidence areas.
Model-free approaches directly encode such bias into policy or value function
learning using conservative regularizations or special network structures, but
their constrained policy search limits the generalization beyond the offline
dataset. Model-based approaches learn forward dynamics models with conservatism
quantifications and then generate imaginary trajectories to extend the offline
datasets. However, due to limited samples in offline datasets, conservatism
quantifications often suffer from overgeneralization in out-of-support regions.
The unreliable conservative measures will mislead forward model-based
imaginations to undesired areas, leading to overaggressive behaviors. To
encourage more conservatism, we propose a novel model-based offline RL
framework, called Reverse Offline Model-based Imagination (ROMI). We learn a
reverse dynamics model in conjunction with a novel reverse policy, which can
generate rollouts leading to the target goal states within the offline dataset.
These reverse imaginations provide informed data augmentation for model-free
policy learning and enable conservative generalization beyond the offline
dataset. ROMI can effectively combine with off-the-shelf model-free algorithms
to enable model-based generalization with proper conservatism. Empirical
results show that our method can generate more conservative behaviors and
achieve state-of-the-art performance on offline RL benchmark tasks.

    ", Machine Learning,reinforcement learning,Offline Reinforcement Learning with Reverse Model-based Imagination
"  Analog circuit sizing takes a significant amount of manual effort in a
typical design cycle. With rapidly developing technology and tight schedules,
bringing automated solutions for sizing has attracted great attention. This
paper presents DNN-Opt, a Reinforcement Learning (RL) inspired Deep Neural
Network (DNN) based black-box optimization framework for analog circuit sizing.
The key contributions of this paper are a novel sample-efficient two-stage deep
learning optimization framework leveraging RL actor-critic algorithms, and a
recipe to extend it on large industrial circuits using critical device
identification. Our method shows 5--30x sample efficiency compared to other
black-box optimization methods both on small building blocks and on large
industrial circuits with better performance metrics. To the best of our
knowledge, this is the first application of DNN-based circuit sizing on
industrial scale circuits.

    ", Machine Learning,reinforcement learning,DNN-Opt: An RL Inspired Optimization for Analog Circuit Sizing using Deep Neural Networks
"  The ability to learn and execute optimal control policies safely is critical
to realization of complex autonomy, especially where task restarts are not
available and/or the systems are safety-critical. Safety requirements are often
expressed in terms of state and/or control constraints. Methods such as barrier
transformation and control barrier functions have been successfully used, in
conjunction with model-based reinforcement learning, for safe learning in
systems under state constraints, to learn the optimal control policy. However,
existing barrier-based safe learning methods rely on full state feedback. In
this paper, an output-feedback safe model-based reinforcement learning
technique is developed that utilizes a novel dynamic state estimator to
implement simultaneous learning and control for a class of safety-critical
systems with partially observable state.

    ", Systems and Control,reinforcement learning,Safety aware model-based reinforcement learning for optimal control of a class of output-feedback nonlinear systems
"  Entropy regularization is a popular method in reinforcement learning (RL).
Although it has many advantages, it alters the RL objective and makes the
converged policy deviate from the optimal policy of the original Markov
Decision Process. Though divergence regularization has been proposed to settle
this problem, it cannot be trivially applied to cooperative multi-agent
reinforcement learning (MARL). In this paper, we investigate divergence
regularization in cooperative MARL and propose a novel off-policy cooperative
MARL framework, divergence-regularized multi-agent actor-critic (DMAC).
Mathematically, we derive the update rule of DMAC which is naturally
off-policy, guarantees a monotonic policy improvement and is not biased by the
regularization. DMAC is a flexible framework and can be combined with many
existing MARL algorithms. We evaluate DMAC in a didactic stochastic game and
StarCraft Multi-Agent Challenge and empirically show that DMAC substantially
improves the performance of existing MARL algorithms.

    ", Machine Learning,reinforcement learning,Divergence-Regularized Multi-Agent Actor-Critic
"  The current research focus in Robot-Assisted Minimally Invasive Surgery
(RAMIS) is directed towards increasing the level of robot autonomy, to place
surgeons in a supervisory position. Although Learning from Demonstrations (LfD)
approaches are among the preferred ways for an autonomous surgical system to
learn expert gestures, they require a high number of demonstrations and show
poor generalization to the variable conditions of the surgical environment. In
this work, we propose an LfD methodology based on Generative Adversarial
Imitation Learning (GAIL) that is built on a Deep Reinforcement Learning (DRL)
setting. GAIL combines generative adversarial networks to learn the
distribution of expert trajectories with a DRL setting to ensure generalisation
of trajectories providing human-like behaviour. We consider automation of
tissue retraction, a common RAMIS task that involves soft tissues manipulation
to expose a region of interest. In our proposed methodology, a small set of
expert trajectories can be acquired through the da Vinci Research Kit (dVRK)
and used to train the proposed LfD method inside a simulated environment.
Results indicate that our methodology can accomplish the tissue retraction task
with human-like behaviour while being more sample-efficient than the baseline
DRL method. Towards the end, we show that the learnt policies can be
successfully transferred to the real robotic platform and deployed for soft
tissue retraction on a synthetic phantom.

    ", Robotics,reinforcement learning,Learning from Demonstrations for Autonomous Soft-tissue Retraction
"  We present results from a search for high-redshift radio galaxy (H$z$RG)
candidates using 1.28 GHz data in the Abell 2751 field drawn from the MeerKAT
Galaxy Cluster Legacy Survey (MGCLS). We use the H$z$RG criteria that a radio
source is undetected in all-sky optical and infrared catalogues, and has a very
steep radio spectrum. We cross-match the radio catalogue against
multi-wavelength galaxy catalogues from DECaLS and AllWISE. For those radio
sources with no multi-wavelength counterpart, we further implement a radio
spectral index criterium of $\alpha < -1$, using in-band spectral index
measurements from the wide-band MeerKAT data. Using a 5$\sigma$ signal-to-noise
cut on the radio flux densities, we find a total of 274 HzRG candidates: 179
ultra-steep spectrum sources, and 95 potential candidates which cannot be ruled
out as they have no spectral information available. The spectral index
assignments in this work are complete above a flux density of 0.3 mJy, at least
an order of magnitude lower than existing studies in this frequency range or
when extrapolating from lower frequency limits. Our faintest HzRG candidates
with and without an in-band spectral index measurement have a 1.28\,GHz flux
density of 57 $\pm$ 8 $\mu$Jy and 68 $\pm$ 13 $\mu$Jy, respectively. Although
our study is not complete down to these flux densities, our results indicate
that the sensitivity and bandwidth of the MGCLS data makes them a powerful
radio resource to search for H$z$RG candidates in the Southern sky, with 20 of
the MGCLS pointings having similar image quality as the Abell~2751 field and
full coverage in both DECaLS and AllWISE. Data at additional radio frequencies
will be needed for the faintest source populations, which could be provided in
the near future by the MeerKAT UHF band (580 -- 1015 MHz) at a similar
resolution ($\sim$ 8-10 arcsec).

    ", Astrophysics of Galaxies,clustering,Searching for high-z radio galaxies with the MGCLS
"  We investigate dynamics and bifurcations in a mathematical model that
captures electrochemical experiments on arrays of microelectrodes. In
isolation, each individual microelectrode is described by a one-dimensional
unit with a bistable current-potential response. When an array of such
electrodes is coupled by controlling the total electric current, the common
electric potential of all electrodes oscillates in some interval of the
current. These coupling-induced collective oscillations of bistable
one-dimensional units are captured by the model. Moreover, any equilibrium is
contained in a cluster subspace, where the electrodes take at most three
distinct states. We systematically analyze the dynamics and bifurcations of the
model equations: We consider the dynamics on cluster subspaces of successively
increasing dimension and analyze the bifurcations occurring therein. Most
importantly, the system exhibits an equivariant transcritical bifurcation of
limit cycles. From this bifurcation, several limit cycles branch, one of which
is stable for arbitrarily many bistable units.

    ", Adaptation and Self-Organizing Systems,clustering,Bifurcations of Clusters and Collective Oscillations in Networks of Bistable Units
"  Recent advancements in computational resources and Deep Learning
methodologies has significantly benefited development of intelligent
vision-based surveillance applications. Gait recognition in the presence of
occlusion is one of the challenging research topics in this area, and the
solutions proposed by researchers to date lack in robustness and also dependent
of several unrealistic constraints, which limits their practical applicability.
We improve the state-of-the-art by developing novel deep learning-based
algorithms to identify the occluded frames in an input sequence and next
reconstruct these occluded frames by exploiting the spatio-temporal information
present in the gait sequence. The multi-stage pipeline adopted in this work
consists of key pose mapping, occlusion detection and reconstruction, and
finally gait recognition. While the key pose mapping and occlusion detection
phases are done %using Constrained KMeans Clustering and via a graph sorting
algorithm, reconstruction of occluded frames is done by fusing the key
pose-specific information derived in the previous step along with the
spatio-temporal information contained in a gait sequence using a Bi-Directional
Long Short Time Memory. This occlusion reconstruction model has been trained
using synthetically occluded CASIA-B and OU-ISIR data, and the trained model is
termed as Bidirectional Gait Reconstruction Network BGait-R-Net. Our LSTM-based
model reconstructs occlusion and generates frames that are temporally
consistent with the periodic pattern of a gait cycle, while simultaneously
preserving the body structure.

    ", Computer Vision and Pattern Recognition,clustering,BGaitR-Net: Occluded Gait Sequence reconstructionwith temporally constrained model for gait recognition
"  Alloying is a successful strategy for tuning the phases and properties of
two-dimensional (2D) transition metal dichalcogenides (TMDCs). To accelerate
the synthesis of new TMDC alloys, we present a method for generating
temperature-composition equilibrium phase diagrams by combining
first-principles total energy calculations with thermodynamic solution models.
This method is applied to three representative 2D TMDC alloys: an isostructural
alloy, MoS2(1-x)Te2x, and two heterostructural alloys, Mo1-xWxTe2 and
WS2(1-x)Te2x. We show that the mixing enthalpy of the entire composition range
of these binary alloys can be reliably represented using a sub-regular solution
model fitted to the total energy of a small number of compositions that are
calculated using density-functional theory on special quasi-random structures.
The sub-regular solution model uses a cubic fit that captures three-body
effects that are important in these TMDC alloys having hexagonal structures. By
comparing both isostructural and heterostructural phase diagrams generated with
this method to those calculated with cluster expansion methods, we demonstrate
that this method can be used to rapidly design phase diagrams of TMDC alloys,
and related 2D materials.

    ", Materials Science,clustering,An Efficient Method for Generating Equilibrium Phase Diagrams of Isostructural and Heterostructural Two-Dimensional Alloys from First Principles
"  Most recent studies on detecting and localizing temporal anomalies have
mainly employed deep neural networks to learn the normal patterns of temporal
data in an unsupervised manner. Unlike them, the goal of our work is to fully
utilize instance-level (or weak) anomaly labels, which only indicate whether
any anomalous events occurred or not in each instance of temporal data. In this
paper, we present WETAS, a novel framework that effectively identifies
anomalous temporal segments (i.e., consecutive time points) in an input
instance. WETAS learns discriminative features from the instance-level labels
so that it infers the sequential order of normal and anomalous segments within
each instance, which can be used as a rough segmentation mask. Based on the
dynamic time warping (DTW) alignment between the input instance and its
segmentation mask, WETAS obtains the result of temporal segmentation, and
simultaneously, it further enhances itself by using the mask as additional
supervision. Our experiments show that WETAS considerably outperforms other
baselines in terms of the localization of temporal anomalies, and also it
provides more informative results than point-level detection methods.

    ", Computer Vision and Pattern Recognition,unsupervised learning,Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping
"  Event camera is an emerging imaging sensor for capturing dynamics of moving
objects as events, which motivates our work in estimating 3D human pose and
shape from the event signals. Events, on the other hand, have their unique
challenges: rather than capturing static body postures, the event signals are
best at capturing local motions. This leads us to propose a two-stage deep
learning approach, called EventHPE. The first-stage, FlowNet, is trained by
unsupervised learning to infer optical flow from events. Both events and
optical flow are closely related to human body dynamics, which are fed as input
to the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate
the discrepancy between image-based flow (optical flow) and shape-based flow
(vertices movement of human body shape), a novel flow coherence loss is
introduced by exploiting the fact that both flows are originated from the
identical human motion. An in-house event-based 3D human dataset is curated
that comes with 3D pose and shape annotations, which is by far the largest one
to our knowledge. Empirical evaluations on DHP19 dataset and our in-house
dataset demonstrate the effectiveness of our approach.

    ", Computer Vision and Pattern Recognition,unsupervised learning,EventHPE: Event-based 3D Human Pose and Shape Estimation
"  Cortical pyramidal neurons have a complex dendritic anatomy, whose function
is an active research field. In particular, the segregation between its soma
and the apical dendritic tree is believed to play an active role in processing
feed-forward sensory information and top-down or feedback signals. In this
work, we use a simple two-compartment model accounting for the nonlinear
interactions between basal and apical input streams and show that standard
unsupervised Hebbian learning rules in the basal compartment allow the neuron
to align the feed-forward basal input with the top-down target signal received
by the apical compartment. We show that this learning process, termed
coincidence detection, is robust against strong distractions in the basal input
space and demonstrate its effectiveness in a linear classification task.

    ", Neurons and Cognition,unsupervised learning,Nonlinear Dendritic Coincidence Detection for Supervised Learning
"  Gastrointestinal (GI) cancer precursors require frequent monitoring for risk
stratification of patients. Automated segmentation methods can help to assess
risk areas more accurately, and assist in therapeutic procedures or even
removal. In clinical practice, addition to the conventional white-light imaging
(WLI), complimentary modalities such as narrow-band imaging (NBI) and
fluorescence imaging are used. While, today most segmentation approaches are
supervised and only concentrated on a single modality dataset, this work
exploits to use a target-independent unsupervised domain adaptation (UDA)
technique that is capable to generalize to an unseen target modality. In this
context, we propose a novel UDA-based segmentation method that couples the
variational autoencoder and U-Net with a common EfficientNet-B4 backbone, and
uses a joint loss for latent-space optimization for target samples. We show
that our model can generalize to unseen target NBI (target) modality when
trained using only WLI (source) modality. Our experiments on both upper and
lower GI endoscopy data show the effectiveness of our approach compared to
naive supervised approach and state-of-the-art UDA segmentation methods.

    ", Image and Video Processing,unsupervised learning,EndoUDA: A modality independent segmentation approach for endoscopy imaging
"  In this paper we study test time decoding; an ubiquitous step in almost all
sequential text generation task spanning across a wide array of natural
language processing (NLP) problems. Our main contribution is to develop a
continuous relaxation framework for the combinatorial NP-hard decoding problem
and propose Disco - an efficient algorithm based on standard first order
gradient based. We provide tight analysis and show that our proposed algorithm
linearly converges to within $\epsilon$ neighborhood of the optima. Finally, we
perform preliminary experiments on the task of adversarial text generation and
show superior performance of Disco over several popular decoding approaches.

    ", Computation and Language,unsupervised learning,DISCO : efficient unsupervised decoding for discrete natural language problems via convex relaxation
"  Knowledge transfer from synthetic to real data has been widely studied to
mitigate data annotation constraints in various computer vision tasks such as
semantic segmentation. However, the study focused on 2D images and its
counterpart in 3D point clouds segmentation lags far behind due to the lack of
large-scale synthetic datasets and effective transfer methods. We address this
issue by collecting SynLiDAR, a large-scale synthetic LiDAR dataset that
contains point-wise annotated point clouds with accurate geometric shapes and
comprehensive semantic classes. SynLiDAR was collected from multiple virtual
environments with rich scenes and layouts which consists of over 19 billion
points of 32 semantic classes. In addition, we design PCT, a novel point cloud
translator that effectively mitigates the gap between synthetic and real point
clouds. Specifically, we decompose the synthetic-to-real gap into an appearance
component and a sparsity component and handle them separately which improves
the point cloud translation greatly. We conducted extensive experiments over
three transfer learning setups including data augmentation, semi-supervised
domain adaptation and unsupervised domain adaptation. Extensive experiments
show that SynLiDAR provides a high-quality data source for studying 3D transfer
and the proposed PCT achieves superior point cloud translation consistently
across the three setups. SynLiDAR project page:
\url{", Computer Vision and Pattern Recognition,unsupervised learning,Transfer Learning from Synthetic to Real LiDAR Point Cloud for Semantic Segmentation
"  The widespread diffusion of mobile phones is triggering an exponential growth
of mobile data traffic that is likely to cause, in the near future,
considerable traffic overload issues even in last-generation cellular networks.
Offloading part of the traffic to other networks is considered a very promising
approach and, in particular, in this paper, we consider offloading through
opportunistic networks of users' devices. However, the performance of this
solution strongly depends on the pattern of encounters between mobile nodes,
which should therefore be taken into account when designing offloading control
algorithms. In this paper, we propose an adaptive offloading solution based on
the Reinforcement Learning framework and we evaluate and compare the
performance of two well-known learning algorithms: Actor-Critic and Q-Learning.
More precisely, in our solution the controller of the dissemination process,
once trained, is able to select a proper number of content replicas to be
injected into the opportunistic network to guarantee the timely delivery of
contents to all interested users. We show that our system based on
Reinforcement Learning is able to automatically learn a very efficient strategy
to reduce the traffic on the cellular network, without relying on any
additional context information about the opportunistic network. Our solution
achieves a higher level of offloading with respect to other state-of-the-art
approaches, in a range of different mobility settings. Moreover, we show that a
more refined learning solution, based on the Actor-Critic algorithm, is
significantly more efficient than a simpler solution based on Q-learning.

    ", Networking and Internet Architecture,reinforcement learning,Cellular traffic offloading via Opportunistic Networking with Reinforcement Learning
"  We investigate the problem of designing optimal stealthy poisoning attacks on
the control channel of Markov decision processes (MDPs). This research is
motivated by the recent interest of the research community for adversarial and
poisoning attacks applied to MDPs, and reinforcement learning (RL) methods. The
policies resulting from these methods have been shown to be vulnerable to
attacks perturbing the observations of the decision-maker. In such an attack,
drawing inspiration from adversarial examples used in supervised learning, the
amplitude of the adversarial perturbation is limited according to some norm,
with the hope that this constraint will make the attack imperceptible. However,
such constraints do not grant any level of undetectability and do not take into
account the dynamic nature of the underlying Markov process. In this paper, we
propose a new attack formulation, based on information-theoretical quantities,
that considers the objective of minimizing the detectability of the attack as
well as the performance of the controlled process. We analyze the trade-off
between the efficiency of the attack and its detectability. We conclude with
examples and numerical simulations illustrating this trade-off.

    ", Systems and Control,reinforcement learning,Balancing detectability and performance of attacks on the control channel of Markov Decision Processes
"  Offensive language detection is an ever-growing natural language processing
(NLP) application. This growth is mainly because of the widespread usage of
social networks, which becomes a mainstream channel for people to communicate,
work, and enjoy entertainment content. Many incidents of sharing aggressive and
offensive content negatively impacted society to a great extend. We believe
contributing to improving and comparing different machine learning models to
fight such harmful contents is an important and challenging goal for this
thesis. We targeted the problem of offensive language detection for building
efficient automated models for offensive language detection. With the recent
advancements of NLP models, specifically, the Transformer model, which tackled
many shortcomings of the standard seq-to-seq techniques. The BERT model has
shown state-of-the-art results on many NLP tasks. Although the literature still
exploring the reasons for the BERT achievements in the NLP field. Other
efficient variants have been developed to improve upon the standard BERT, such
as RoBERTa and ALBERT. Moreover, due to the multilingual nature of text on
social media that could affect the model decision on a given tween, it is
becoming essential to examine multilingual models such as XLM-RoBERTa trained
on 100 languages and how did it compare to unilingual models. The RoBERTa based
model proved to be the most capable model and achieved the highest F1 score for
the tasks. Another critical aspect of a well-rounded offensive language
detection system is the speed at which a model can be trained and make
inferences. In that respect, we have considered the model run-time and
fine-tuned the very efficient implementation of FastText called BlazingText
that achieved good results, which is much faster than BERT-based models.

    ", Computation and Language,natural language processing,Neural Models for Offensive Language Detection
"  Scientific literature contain important information related to cutting-edge
innovations in diverse domains. Advances in natural language processing have
been driving the fast development in automated information extraction from
scientific literature. However, scientific literature is often available in
unstructured PDF format. While PDF is great for preserving basic visual
elements, such as characters, lines, shapes, etc., on a canvas for presentation
to humans, automatic processing of the PDF format by machines presents many
challenges. With over 2.5 trillion PDF documents in existence, these issues are
prevalent in many other important application domains as well.
", Information Retrieval,natural language processing,ICDAR 2021 Competition on Scientific Literature Parsing
"  High hospitalization rates due to the global spread of Covid-19 bring about a
need for improvements to classical triaging workflows. To this end,
convolutional neural networks (CNNs) can effectively differentiate critical
from non-critical images so that critical cases may be addressed quickly, so
long as there exists some representative image for the illness. Presented is a
conglomerate neural network system consisting of multiple VGG16 CNNs; the
system trains on weighted skin disease images re-labelled as critical or
non-critical, to then attach to input images a critical index between 0 and 10.
A critical index offers a more comprehensive rating system compared to binary
critical/non-critical labels. Results for batches of input images run through
the trained network are promising. A batch is shown being re-ordered by the
proposed architecture from most critical to least critical roughly accurately.

    ", Image and Video Processing,convolutional neural networks,Leveraging Multiple CNNs for Triaging Medical Workflow
"  Motivated by the problem of learning when the number of training samples is
small, this paper shows how to incorporate into support-vector machines (SVMs)
those properties that have made convolutional neural networks (CNNs)
successful. Particularly important is the ability to incorporate domain
knowledge of invariances, e.g., translational invariance of images. Kernels
based on the \textit{minimum} distance over a group of transformations, which
corresponds to defining similarity as the \textit{best} over the possible
transformations, are not generally positive definite. Perhaps it is for this
reason that they have neither previously been experimentally tested for their
performance nor studied theoretically. Instead, previous attempts have employed
kernels based on the \textit{average} distance over a group of transformations,
which are trivially positive definite, but which generally yield both poor
margins as well as poor performance, as we show. We address this lacuna and
show that positive definiteness indeed holds \textit{with high probability} for
kernels based on the minimum distance in the small training sample set regime
of interest, and that they do yield the best results in that regime. Another
important property of CNNs is their ability to incorporate local features at
multiple spatial scales, e.g., through max pooling. A third important property
is their ability to provide the benefits of composition through the
architecture of multiple layers. We show how these additional properties can
also be embedded into SVMs. We verify through experiments on widely available
image sets that the resulting SVMs do provide superior accuracy in comparison
to well-established deep neural network (DNN) benchmarks for small sample
sizes.

    ", Machine Learning,convolutional neural networks,Learning from Small Samples: Transformation-Invariant SVMs with Composition and Locality at Multiple Scales
"  Deep generative networks in recent years have reinforced the need for caution
while consuming various modalities of digital information. One avenue of
deepfake creation is aligned with injection and removal of tumors from medical
scans. Failure to detect medical deepfakes can lead to large setbacks on
hospital resources or even loss of life. This paper attempts to address the
detection of such attacks with a structured case study. We evaluate different
machine learning algorithms and pretrained convolutional neural networks on
distinguishing between tampered and untampered data. The findings of this work
show near perfect accuracy in detecting instances of tumor injections and
removals.

    ", Computer Vision and Pattern Recognition,convolutional neural networks,Machine Learning based Medical Image Deepfake Detection: A Comparative Study
"  Sleep stage classification is essential for sleep assessment and disease
diagnosis. Although previous attempts to classify sleep stages have achieved
high classification performance, several challenges remain open: 1) How to
effectively utilize time-varying spatial and temporal features from
multi-channel brain signals remains challenging. Prior works have not been able
to fully utilize the spatial topological information among brain regions. 2)
Due to the many differences found in individual biological signals, how to
overcome the differences of subjects and improve the generalization of deep
neural networks is important. 3) Most deep learning methods ignore the
interpretability of the model to the brain. To address the above challenges, we
propose a multi-view spatial-temporal graph convolutional networks (MSTGCN)
with domain generalization for sleep stage classification. Specifically, we
construct two brain view graphs for MSTGCN based on the functional connectivity
and physical distance proximity of the brain regions. The MSTGCN consists of
graph convolutions for extracting spatial features and temporal convolutions
for capturing the transition rules among sleep stages. In addition, attention
mechanism is employed for capturing the most relevant spatial-temporal
information for sleep stage classification. Finally, domain generalization and
MSTGCN are integrated into a unified framework to extract subject-invariant
sleep features. Experiments on two public datasets demonstrate that the
proposed model outperforms the state-of-the-art baselines.

    ", Signal Processing,convolutional neural networks,Multi-View Spatial-Temporal Graph Convolutional Networks with Domain Generalization for Sleep Stage Classification
"  Domains such as manufacturing and medicine crave for continuous monitoring
and analysis of their processes, especially in combination with time series as
produced by sensors. Time series data can be exploited to, for example, explain
and predict concept drifts during runtime. Generally, a certain data volume is
required in order to produce meaningful analysis results. However, reliable
data sets are often missing, for example, if event streams and times series
data are collected separately, in case of a new process, or if it is too
expensive to obtain a sufficient data volume. Additional challenges arise with
preparing time series data from multiple event sources, variations in data
collection frequency, and concept drift. This paper proposes the GENLOG
approach to generate reliable event and time series data that follows the
distribution of the underlying input data set. GENLOG employs data resampling
and enables the user to select different parts of the log data to orchestrate
the training of a recurrent neural network for stream generation. The generated
data is sampled back to its original sample rate and is embedded into the
originating log data file. Overall, GENLOG can boost small data sets and
consequently the application of online process mining.

    ", Machine Learning,recurrent neural networks,Generating Reliable Process Event Streams and Time Series Data based on Neural Networks
"  The design of recurrent neural networks (RNNs) to accurately process
sequential inputs with long-time dependencies is very challenging on account of
the exploding and vanishing gradient problem. To overcome this, we propose a
novel RNN architecture which is based on a structure preserving discretization
of a Hamiltonian system of second-order ordinary differential equations that
models networks of oscillators. The resulting RNN is fast, invertible (in
time), memory efficient and we derive rigorous bounds on the hidden state
gradients to prove the mitigation of the exploding and vanishing gradient
problem. A suite of experiments are presented to demonstrate that the proposed
RNN provides state of the art performance on a variety of learning tasks with
(very) long-time dependencies.

    ", Machine Learning,recurrent neural networks,UnICORNN: A recurrent model for learning very long time dependencies
"  Structural credit assignment for recurrent learning is challenging. An
algorithm called RTRL can compute gradients for recurrent networks online but
is computationally intractable for large networks. Alternatives, such as BPTT,
are not online. In this work, we propose a credit-assignment algorithm --
\algoname{} -- that approximates the gradients for recurrent learning in
real-time using $O(n)$ operations and memory per-step. Our method builds on the
idea that for modular recurrent networks, composed of columns with scalar
states, it is sufficient for a parameter to only track its influence on the
state of its column. We empirically show that as long as connections between
columns are sparse, our method approximates the true gradient well. In the
special case when there are no connections between columns, the $O(n)$ gradient
estimate is exact. We demonstrate the utility of the approach for both
recurrent state learning and meta-learning by comparing the estimated gradient
to the true gradient on a synthetic test-bed.

    ", Machine Learning,recurrent neural networks,Scalable Online Recurrent Learning Using Columnar Neural Networks
